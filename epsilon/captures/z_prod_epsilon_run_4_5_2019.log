
 (epsilon)$ python src/run.py [In PRDALL]

2019-02-01 15:47:59,195-INFO-The directory value is: datasets/lnkd/
2019-02-01 15:47:59,195-DEBUG-File found: datasets/lnkd/mar_18_data.json
2019-02-01 15:47:59,195-DEBUG-File found: datasets/lnkd/issues_aug_15.json
2019-02-01 15:47:59,195-DEBUG-File found: datasets/lnkd/issues_mar_16.json
2019-02-01 15:47:59,195-DEBUG-File found: datasets/lnkd/issues_sep_17.json
2019-02-01 15:47:59,195-DEBUG-File found: datasets/lnkd/aug_17_data.json
2019-02-01 15:47:59,195-DEBUG-File found: datasets/lnkd/nov_17_data.json
2019-02-01 15:47:59,195-DEBUG-File found: datasets/lnkd/may_16_data.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/sep_16_data.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/sep_17_data.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/jun_18_data.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/issues_jul_17.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/issues_oct_15.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/issues_jan_dec_14.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/may_18_data.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/issues_mar_17.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/issues_aug_17.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/issues_apr_17.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/issues_jun_16.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/issues_feb_15.json
2019-02-01 15:47:59,196-DEBUG-File found: datasets/lnkd/dec_16_data.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/dec_17_data.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/jan_17_data.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/issues_oct_16.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/issues_jun_15.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/aug_16_data.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/oct_17_data.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/issues_jul_16.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/issues_may_15.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/issues_feb_17.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/issues_apr_15.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/aug_18_data.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/issues_jan_17.json
2019-02-01 15:47:59,197-DEBUG-File found: datasets/lnkd/mar_17_data.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/feb_17_data.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/apr_18_data.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/issues_dec_16.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/issues_apr_16.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/issues_jul_15.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/issues_jan_15.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/oct_16_data.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/jul_18_data.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/jul_16_data.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/issues_may_17.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/issues_nov_16.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/nov_16_data.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/jun_17_data.json
2019-02-01 15:47:59,198-DEBUG-File found: datasets/lnkd/jan_18_data.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/issues_sep_15.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/issues_may_16.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/issues_mar_15.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/feb_18_data.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/may_17_data.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/issues_oct_17.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/issues_nov_17.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/jul_17_data.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/issues_aug_16.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/apr_17_data.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/jun_16_data.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/issues_dec_feb_16.json
2019-02-01 15:47:59,199-DEBUG-File found: datasets/lnkd/issues_sep_16.json
2019-02-01 15:47:59,200-DEBUG-File found: datasets/lnkd/sep_18_data.json
2019-02-01 15:47:59,200-DEBUG-File found: datasets/lnkd/issues_jun_17.json
2019-02-01 15:47:59,200-DEBUG-File found: datasets/lnkd/issues_nov_15.json
2019-02-01 15:47:59,200-DEBUG-Working with file batch: datasets/lnkd/mar_18_data.json
2019-02-01 15:47:59,429-DEBUG-Working with file batch: datasets/lnkd/issues_aug_15.json
2019-02-01 15:47:59,574-DEBUG-Working with file batch: datasets/lnkd/issues_mar_16.json
2019-02-01 15:47:59,713-DEBUG-Working with file batch: datasets/lnkd/issues_sep_17.json
2019-02-01 15:48:00,031-DEBUG-Working with file batch: datasets/lnkd/aug_17_data.json
2019-02-01 15:48:00,138-DEBUG-Working with file batch: datasets/lnkd/nov_17_data.json
2019-02-01 15:48:00,367-DEBUG-Working with file batch: datasets/lnkd/may_16_data.json
2019-02-01 15:48:00,388-DEBUG-Working with file batch: datasets/lnkd/sep_16_data.json
2019-02-01 15:48:00,428-DEBUG-Working with file batch: datasets/lnkd/sep_17_data.json
2019-02-01 15:48:00,545-DEBUG-Working with file batch: datasets/lnkd/jun_18_data.json
2019-02-01 15:48:00,729-DEBUG-Working with file batch: datasets/lnkd/issues_jul_17.json
2019-02-01 15:48:00,965-DEBUG-Working with file batch: datasets/lnkd/issues_oct_15.json
2019-02-01 15:48:01,123-DEBUG-Working with file batch: datasets/lnkd/issues_jan_dec_14.json
2019-02-01 15:48:01,266-DEBUG-Working with file batch: datasets/lnkd/may_18_data.json
2019-02-01 15:48:01,423-DEBUG-Working with file batch: datasets/lnkd/issues_mar_17.json
2019-02-01 15:48:01,766-DEBUG-Working with file batch: datasets/lnkd/issues_aug_17.json
2019-02-01 15:48:02,005-DEBUG-Working with file batch: datasets/lnkd/issues_apr_17.json
2019-02-01 15:48:02,169-DEBUG-Working with file batch: datasets/lnkd/issues_jun_16.json
2019-02-01 15:48:02,543-DEBUG-Working with file batch: datasets/lnkd/issues_feb_15.json
2019-02-01 15:48:02,693-DEBUG-Working with file batch: datasets/lnkd/dec_16_data.json
2019-02-01 15:48:02,720-DEBUG-Working with file batch: datasets/lnkd/dec_17_data.json
2019-02-01 15:48:02,778-DEBUG-Working with file batch: datasets/lnkd/jan_17_data.json
2019-02-01 15:48:02,810-DEBUG-Working with file batch: datasets/lnkd/issues_oct_16.json
2019-02-01 15:48:02,943-DEBUG-Working with file batch: datasets/lnkd/issues_jun_15.json
2019-02-01 15:48:03,090-DEBUG-Working with file batch: datasets/lnkd/aug_16_data.json
2019-02-01 15:48:03,136-DEBUG-Working with file batch: datasets/lnkd/oct_17_data.json
2019-02-01 15:48:03,253-DEBUG-Working with file batch: datasets/lnkd/issues_jul_16.json
2019-02-01 15:48:03,375-DEBUG-Working with file batch: datasets/lnkd/issues_may_15.json
2019-02-01 15:48:03,740-DEBUG-Working with file batch: datasets/lnkd/issues_feb_17.json
2019-02-01 15:48:03,911-DEBUG-Working with file batch: datasets/lnkd/issues_apr_15.json
2019-02-01 15:48:04,037-DEBUG-Working with file batch: datasets/lnkd/aug_18_data.json
2019-02-01 15:48:04,233-DEBUG-Working with file batch: datasets/lnkd/issues_jan_17.json
2019-02-01 15:48:04,364-DEBUG-Working with file batch: datasets/lnkd/mar_17_data.json
2019-02-01 15:48:04,393-DEBUG-Working with file batch: datasets/lnkd/feb_17_data.json
2019-02-01 15:48:04,418-DEBUG-Working with file batch: datasets/lnkd/apr_18_data.json
2019-02-01 15:48:04,558-DEBUG-Working with file batch: datasets/lnkd/issues_dec_16.json
2019-02-01 15:48:04,654-DEBUG-Working with file batch: datasets/lnkd/issues_apr_16.json
2019-02-01 15:48:05,156-DEBUG-Working with file batch: datasets/lnkd/issues_jul_15.json
2019-02-01 15:48:05,320-DEBUG-Working with file batch: datasets/lnkd/issues_jan_15.json
2019-02-01 15:48:05,418-DEBUG-Working with file batch: datasets/lnkd/oct_16_data.json
2019-02-01 15:48:05,451-DEBUG-Working with file batch: datasets/lnkd/jul_18_data.json
2019-02-01 15:48:05,590-DEBUG-Working with file batch: datasets/lnkd/jul_16_data.json
2019-02-01 15:48:05,622-DEBUG-Working with file batch: datasets/lnkd/issues_may_17.json
2019-02-01 15:48:05,804-DEBUG-Working with file batch: datasets/lnkd/issues_nov_16.json
2019-02-01 15:48:05,924-DEBUG-Working with file batch: datasets/lnkd/nov_16_data.json
2019-02-01 15:48:05,951-DEBUG-Working with file batch: datasets/lnkd/jun_17_data.json
2019-02-01 15:48:05,984-DEBUG-Working with file batch: datasets/lnkd/jan_18_data.json
2019-02-01 15:48:06,118-DEBUG-Working with file batch: datasets/lnkd/issues_sep_15.json
2019-02-01 15:48:06,260-DEBUG-Working with file batch: datasets/lnkd/issues_may_16.json
2019-02-01 15:48:06,412-DEBUG-Working with file batch: datasets/lnkd/issues_mar_15.json
2019-02-01 15:48:06,553-DEBUG-Working with file batch: datasets/lnkd/feb_18_data.json
2019-02-01 15:48:07,063-DEBUG-Working with file batch: datasets/lnkd/may_17_data.json
2019-02-01 15:48:07,103-DEBUG-Working with file batch: datasets/lnkd/issues_oct_17.json
2019-02-01 15:48:07,345-DEBUG-Working with file batch: datasets/lnkd/issues_nov_17.json
2019-02-01 15:48:07,570-DEBUG-Working with file batch: datasets/lnkd/jul_17_data.json
2019-02-01 15:48:07,617-DEBUG-Working with file batch: datasets/lnkd/issues_aug_16.json
2019-02-01 15:48:07,798-DEBUG-Working with file batch: datasets/lnkd/apr_17_data.json
2019-02-01 15:48:07,833-DEBUG-Working with file batch: datasets/lnkd/jun_16_data.json
2019-02-01 15:48:07,856-DEBUG-Working with file batch: datasets/lnkd/issues_dec_feb_16.json
2019-02-01 15:48:08,159-DEBUG-Working with file batch: datasets/lnkd/issues_sep_16.json
2019-02-01 15:48:08,316-DEBUG-Working with file batch: datasets/lnkd/sep_18_data.json
2019-02-01 15:48:08,406-DEBUG-Working with file batch: datasets/lnkd/issues_jun_17.json
2019-02-01 15:48:08,589-DEBUG-Working with file batch: datasets/lnkd/issues_nov_15.json
2019-02-01 15:48:09,154-DEBUG-Processing ticket #0
2019-02-01 15:48:11,530-DEBUG-Processing ticket #1000
2019-02-01 15:48:13,849-DEBUG-Processing ticket #2000
2019-02-01 15:48:15,804-DEBUG-Processing ticket #3000
2019-02-01 15:48:18,107-DEBUG-Processing ticket #4000
2019-02-01 15:48:19,741-DEBUG-Processing ticket #5000
2019-02-01 15:48:21,520-DEBUG-Processing ticket #6000
2019-02-01 15:48:23,536-DEBUG-Processing ticket #7000
2019-02-01 15:48:25,663-DEBUG-Processing ticket #8000
2019-02-01 15:48:27,506-DEBUG-Processing ticket #9000
2019-02-01 15:48:29,125-DEBUG-Processing ticket #10000
2019-02-01 15:48:31,052-DEBUG-Processing ticket #11000
2019-02-01 15:48:32,862-DEBUG-Processing ticket #12000
2019-02-01 15:48:34,679-DEBUG-Processing ticket #13000
2019-02-01 15:48:36,758-DEBUG-Processing ticket #14000
2019-02-01 15:48:39,621-DEBUG-Processing ticket #15000
2019-02-01 15:48:41,100-DEBUG-Processing ticket #16000
2019-02-01 15:48:43,465-DEBUG-Processing ticket #17000
2019-02-01 15:48:45,156-DEBUG-Processing ticket #18000
2019-02-01 15:48:46,805-DEBUG-Processing ticket #19000
2019-02-01 15:48:49,119-DEBUG-Processing ticket #20000
2019-02-01 15:48:50,961-DEBUG-Processing ticket #21000
2019-02-01 15:48:53,032-DEBUG-Processing ticket #22000
2019-02-01 15:48:54,618-DEBUG-Processing ticket #23000
2019-02-01 15:48:56,306-DEBUG-Processing ticket #24000
2019-02-01 15:48:59,557-DEBUG-Processing ticket #25000
2019-02-01 15:49:01,906-DEBUG-Processing ticket #26000
2019-02-01 15:49:03,928-DEBUG-Processing ticket #27000
2019-02-01 15:49:05,535-DEBUG-Processing ticket #28000
2019-02-01 15:49:07,204-DEBUG-Processing ticket #29000
2019-02-01 15:49:08,692-DEBUG-Processing ticket #30000
2019-02-01 15:49:10,363-DEBUG-Processing ticket #31000
2019-02-01 15:49:12,208-DEBUG-Processing ticket #32000
2019-02-01 15:49:14,881-DEBUG-Processing ticket #33000
2019-02-01 15:49:16,700-DEBUG-Processing ticket #34000
2019-02-01 15:49:18,803-DEBUG-Processing ticket #35000
2019-02-01 15:49:21,279-DEBUG-Processing ticket #36000
2019-02-01 15:49:23,523-DEBUG-Processing ticket #37000
2019-02-01 15:49:25,194-DEBUG-Processing ticket #38000
2019-02-01 15:49:26,480-DEBUG-Processing ticket #39000
2019-02-01 15:49:27,936-DEBUG-Processing ticket #40000
2019-02-01 15:49:29,487-DEBUG-Processing ticket #41000
2019-02-01 15:49:31,269-DEBUG-Processing ticket #42000
2019-02-01 15:49:33,104-DEBUG-Processing ticket #43000
2019-02-01 15:49:34,010-DEBUG-Completed processing tickets  ticket, building dictionary
2019-02-01 15:49:41,594-DEBUG-Completed building dictionary, vectorizing tickets
2019-02-01 15:50:22,741-DEBUG-Ticket count:39817  Duplicates:3666 or 8.430881033967298%
 ========  NNClassifier
2019-02-01 15:50:46.957989: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.
2 AVX
2019-02-01 15:50:46.989195: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threa
ds for best performance.
2019-02-01 15:51:14,717-INFO-#  Training loss at round 0: 7.252825
2019-02-01 16:30:57,509-INFO-#  Training loss at round 200: 4.82633
2019-02-01 17:06:20,270-INFO-#  Training loss at round 400: 4.827646
2019-02-01 17:44:19,711-INFO-#  Training loss at round 600: 4.827432
2019-02-01 18:22:08,239-INFO-#  Training loss at round 800: 4.828521
2019-02-01 19:00:54,106-INFO-#  Training loss at round 1000: 4.8296432
2019-02-01 19:40:08,355-INFO-#  Training loss at round 1200: 4.8310885
2019-02-01 20:19:57,012-INFO-#  Training loss at round 1400: 4.831867
2019-02-01 21:00:24,492-INFO-#  Training loss at round 1600: 4.83146
2019-02-01 21:39:17,465-INFO-#  Training loss at round 1800: 4.831396
2019-02-01 22:16:19,686-INFO-#  Training loss at round 1999: 4.832014
2019-02-01 22:16:20,241-DEBUG-findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/yaberraf/miniconda3
/envs/epsilon/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000
2019-02-01 22:16:20,298-DEBUG-findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('/home/yaberraf/miniconda3
/envs/epsilon/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000
2019-02-01 22:16:28,050-INFO-Accuracy: 0.0691370091416087
2019-02-01 22:18:11,010-INFO-#  Training loss at round 0: 7.2464857
2019-02-01 23:30:09,371-INFO-#  Training loss at round 200: 4.924005
2019-02-02 00:41:53,599-INFO-#  Training loss at round 400: 4.924287
2019-02-02 01:53:32,233-INFO-#  Training loss at round 600: 4.9257674
2019-02-02 03:05:07,680-INFO-#  Training loss at round 800: 4.9271464
2019-02-02 04:12:17,682-INFO-#  Training loss at round 1000: 4.9300947
2019-02-02 05:15:10,027-INFO-#  Training loss at round 1200: 4.928098
2019-02-02 06:17:41,069-INFO-#  Training loss at round 1400: 4.9300847
2019-02-02 07:20:25,157-INFO-#  Training loss at round 1600: 4.93013
2019-02-02 08:24:20,120-INFO-#  Training loss at round 1800: 4.928969
2019-02-02 09:28:26,230-INFO-#  Training loss at round 1999: 4.9291573
2019-02-02 09:28:41,381-INFO-Accuracy: 0.07589260047145403
2019-02-02 09:29:49,232-INFO-#  Training loss at round 0: 7.249919
2019-02-02 10:38:35,390-INFO-#  Training loss at round 200: 4.92408
2019-02-02 11:38:24,353-INFO-#  Training loss at round 400: 4.924293
2019-02-02 12:38:23,326-INFO-#  Training loss at round 600: 4.92549
2019-02-02 13:39:39,931-INFO-#  Training loss at round 800: 4.9254136
2019-02-02 14:42:09,303-INFO-#  Training loss at round 1000: 4.926418
2019-02-02 15:45:48,295-INFO-#  Training loss at round 1200: 4.926992
2019-02-02 16:49:21,424-INFO-#  Training loss at round 1400: 4.928883
2019-02-02 17:52:33,841-INFO-#  Training loss at round 1600: 4.928612
2019-02-02 18:56:28,834-INFO-#  Training loss at round 1800: 4.928578
2019-02-02 20:00:51,781-INFO-#  Training loss at round 1999: 4.9299474
2019-02-02 20:01:07,307-INFO-Accuracy: 0.0713505433220261
2019-02-02 20:02:00,920-INFO-#  Training loss at round 0: 7.248073
2019-02-02 21:07:11,253-INFO-#  Training loss at round 200: 4.9002285
2019-02-02 22:13:34,324-INFO-#  Training loss at round 400: 4.9036016
2019-02-02 23:20:42,960-INFO-#  Training loss at round 600: 4.9030256
2019-02-03 00:29:09,642-INFO-#  Training loss at round 800: 4.905528
2019-02-03 01:38:29,327-INFO-#  Training loss at round 1000: 4.904155
2019-02-03 02:48:53,844-INFO-#  Training loss at round 1200: 4.9056277
2019-02-03 04:00:25,756-INFO-#  Training loss at round 1400: 4.9045267
2019-02-03 05:12:05,771-INFO-#  Training loss at round 1600: 4.904598
2019-02-03 06:23:49,724-INFO-#  Training loss at round 1800: 4.905493
2019-02-03 07:35:10,574-INFO-#  Training loss at round 1999: 4.905705
2019-02-03 07:35:28,333-INFO-Accuracy: 0.06876131888349096
2019-02-03 07:36:28,290-INFO-#  Training loss at round 0: 7.247468
2019-02-03 08:48:05,320-INFO-#  Training loss at round 200: 4.8842106
2019-02-03 09:59:50,076-INFO-#  Training loss at round 400: 4.887023
2019-02-03 11:11:43,808-INFO-#  Training loss at round 600: 4.9148088
2019-02-03 12:23:45,119-INFO-#  Training loss at round 800: 4.890273
2019-02-03 13:36:03,624-INFO-#  Training loss at round 1000: 4.892268
2019-02-03 14:48:20,209-INFO-#  Training loss at round 1200: 4.88457
2019-02-03 16:00:54,898-INFO-#  Training loss at round 1400: 4.886827
2019-02-03 17:13:35,598-INFO-#  Training loss at round 1600: 4.886906
2019-02-03 18:26:23,472-INFO-#  Training loss at round 1800: 4.889507
2019-02-03 19:38:48,788-INFO-#  Training loss at round 1999: 4.8868284
2019-02-03 19:39:06,854-INFO-Accuracy: 0.07824762123781873
Score / NNSARSClassifier<PRDALL.3-8.tanh.0.5>              0.07267781861127971       0.07264893180287507       -2.888680840464064e-05
 ========  NNClassifier
2019-02-03 19:40:02,431-INFO-#  Training loss at round 0: 7.243356
2019-02-03 20:53:36,453-INFO-#  Training loss at round 200: 4.82555
2019-02-03 22:07:11,009-INFO-#  Training loss at round 400: 4.82554
2019-02-03 23:20:51,719-INFO-#  Training loss at round 600: 4.8255363
2019-02-04 00:34:23,414-INFO-#  Training loss at round 800: 4.8255334
2019-02-04 01:48:06,867-INFO-#  Training loss at round 1000: 4.8255286
2019-02-04 03:01:43,161-INFO-#  Training loss at round 1200: 4.82553
2019-02-04 04:15:14,963-INFO-#  Training loss at round 1400: 4.825333
2019-02-04 05:25:32,352-INFO-#  Training loss at round 1600: 4.8253317
2019-02-04 06:35:10,536-INFO-#  Training loss at round 1800: 4.825319
2019-02-04 07:44:37,666-INFO-#  Training loss at round 1999: 4.8253202
2019-02-04 07:44:54,954-INFO-Accuracy: 0.06919450353590526
2019-02-04 07:45:46,522-INFO-#  Training loss at round 0: 7.2426167
2019-02-04 08:53:37,401-INFO-#  Training loss at round 200: 4.9239206
2019-02-04 09:36:51,055-INFO-#  Training loss at round 400: 4.9231634
2019-02-04 10:12:55,747-INFO-#  Training loss at round 600: 4.921737
2019-02-04 10:50:07,509-INFO-#  Training loss at round 800: 4.5490246
2019-02-04 11:28:03,780-INFO-#  Training loss at round 1000: 4.4899797
2019-02-04 12:03:33,933-INFO-#  Training loss at round 1200: 4.1264486
2019-02-04 12:39:10,538-INFO-#  Training loss at round 1400: 3.8164635
2019-02-04 13:16:03,260-INFO-#  Training loss at round 1600: 3.6191645
2019-02-04 13:53:54,782-INFO-#  Training loss at round 1800: 3.517551
2019-02-04 14:32:05,541-INFO-#  Training loss at round 1999: 3.468483
XIO:  fatal IO error 25 (Inappropriate ioctl for device) on X server ":0"
      after 2532 requests (2532 known processed) with 118 events remaining.





















(epsilon)$ python src/run.py [In PRDALL]
2019-02-12 15:59:00,378-INFO-The directory value is: datasets/lnkd/
2019-02-12 15:59:00,378-DEBUG-File found: datasets/lnkd/mar_18_data.json
2019-02-12 15:59:00,378-DEBUG-File found: datasets/lnkd/issues_aug_15.json
2019-02-12 15:59:00,378-DEBUG-File found: datasets/lnkd/issues_mar_16.json
2019-02-12 15:59:00,378-DEBUG-File found: datasets/lnkd/issues_sep_17.json
2019-02-12 15:59:00,378-DEBUG-File found: datasets/lnkd/aug_17_data.json
2019-02-12 15:59:00,378-DEBUG-File found: datasets/lnkd/nov_17_data.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/may_16_data.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/sep_16_data.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/sep_17_data.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/jun_18_data.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/issues_jul_17.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/issues_oct_15.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/issues_jan_dec_14.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/may_18_data.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/issues_mar_17.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/issues_aug_17.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/issues_apr_17.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/issues_jun_16.json
2019-02-12 15:59:00,379-DEBUG-File found: datasets/lnkd/issues_feb_15.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/dec_16_data.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/dec_17_data.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/jan_17_data.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/issues_oct_16.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/issues_jun_15.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/aug_16_data.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/oct_17_data.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/issues_jul_16.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/issues_may_15.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/issues_feb_17.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/issues_apr_15.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/aug_18_data.json
2019-02-12 15:59:00,380-DEBUG-File found: datasets/lnkd/issues_jan_17.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/mar_17_data.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/feb_17_data.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/apr_18_data.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/issues_dec_16.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/issues_apr_16.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/issues_jul_15.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/issues_jan_15.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/oct_16_data.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/jul_18_data.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/jul_16_data.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/issues_may_17.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/issues_nov_16.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/nov_16_data.json
2019-02-12 15:59:00,381-DEBUG-File found: datasets/lnkd/jun_17_data.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/jan_18_data.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/issues_sep_15.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/issues_may_16.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/issues_mar_15.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/feb_18_data.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/may_17_data.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/issues_oct_17.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/issues_nov_17.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/jul_17_data.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/issues_aug_16.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/apr_17_data.json
2019-02-12 15:59:00,382-DEBUG-File found: datasets/lnkd/jun_16_data.json
2019-02-12 15:59:00,383-DEBUG-File found: datasets/lnkd/issues_dec_feb_16.json
2019-02-12 15:59:00,383-DEBUG-File found: datasets/lnkd/issues_sep_16.json
2019-02-12 15:59:00,383-DEBUG-File found: datasets/lnkd/sep_18_data.json
2019-02-12 15:59:00,383-DEBUG-File found: datasets/lnkd/issues_jun_17.json
2019-02-12 15:59:00,383-DEBUG-File found: datasets/lnkd/issues_nov_15.json
2019-02-12 15:59:00,383-DEBUG-Working with file batch: datasets/lnkd/mar_18_data.json
2019-02-12 15:59:00,610-DEBUG-Working with file batch: datasets/lnkd/issues_aug_15.json
2019-02-12 15:59:00,763-DEBUG-Working with file batch: datasets/lnkd/issues_mar_16.json
2019-02-12 15:59:00,905-DEBUG-Working with file batch: datasets/lnkd/issues_sep_17.json
2019-02-12 15:59:01,221-DEBUG-Working with file batch: datasets/lnkd/aug_17_data.json
2019-02-12 15:59:01,331-DEBUG-Working with file batch: datasets/lnkd/nov_17_data.json
2019-02-12 15:59:01,560-DEBUG-Working with file batch: datasets/lnkd/may_16_data.json
2019-02-12 15:59:01,581-DEBUG-Working with file batch: datasets/lnkd/sep_16_data.json
2019-02-12 15:59:01,621-DEBUG-Working with file batch: datasets/lnkd/sep_17_data.json
2019-02-12 15:59:01,738-DEBUG-Working with file batch: datasets/lnkd/jun_18_data.json
2019-02-12 15:59:01,924-DEBUG-Working with file batch: datasets/lnkd/issues_jul_17.json
2019-02-12 15:59:02,168-DEBUG-Working with file batch: datasets/lnkd/issues_oct_15.json
2019-02-12 15:59:02,329-DEBUG-Working with file batch: datasets/lnkd/issues_jan_dec_14.json
2019-02-12 15:59:02,472-DEBUG-Working with file batch: datasets/lnkd/may_18_data.json
2019-02-12 15:59:02,632-DEBUG-Working with file batch: datasets/lnkd/issues_mar_17.json
2019-02-12 15:59:02,975-DEBUG-Working with file batch: datasets/lnkd/issues_aug_17.json
2019-02-12 15:59:03,212-DEBUG-Working with file batch: datasets/lnkd/issues_apr_17.json
2019-02-12 15:59:03,380-DEBUG-Working with file batch: datasets/lnkd/issues_jun_16.json
2019-02-12 15:59:03,755-DEBUG-Working with file batch: datasets/lnkd/issues_feb_15.json
2019-02-12 15:59:03,905-DEBUG-Working with file batch: datasets/lnkd/dec_16_data.json
2019-02-12 15:59:03,935-DEBUG-Working with file batch: datasets/lnkd/dec_17_data.json
2019-02-12 15:59:04,004-DEBUG-Working with file batch: datasets/lnkd/jan_17_data.json
2019-02-12 15:59:04,035-DEBUG-Working with file batch: datasets/lnkd/issues_oct_16.json
2019-02-12 15:59:04,170-DEBUG-Working with file batch: datasets/lnkd/issues_jun_15.json
2019-02-12 15:59:04,320-DEBUG-Working with file batch: datasets/lnkd/aug_16_data.json
2019-02-12 15:59:04,367-DEBUG-Working with file batch: datasets/lnkd/oct_17_data.json
2019-02-12 15:59:04,485-DEBUG-Working with file batch: datasets/lnkd/issues_jul_16.json
2019-02-12 15:59:04,611-DEBUG-Working with file batch: datasets/lnkd/issues_may_15.json
2019-02-12 15:59:04,979-DEBUG-Working with file batch: datasets/lnkd/issues_feb_17.json
2019-02-12 15:59:05,149-DEBUG-Working with file batch: datasets/lnkd/issues_apr_15.json
2019-02-12 15:59:05,276-DEBUG-Working with file batch: datasets/lnkd/aug_18_data.json
2019-02-12 15:59:05,477-DEBUG-Working with file batch: datasets/lnkd/issues_jan_17.json
2019-02-12 15:59:05,629-DEBUG-Working with file batch: datasets/lnkd/mar_17_data.json
2019-02-12 15:59:05,660-DEBUG-Working with file batch: datasets/lnkd/feb_17_data.json
2019-02-12 15:59:05,686-DEBUG-Working with file batch: datasets/lnkd/apr_18_data.json
2019-02-12 15:59:05,827-DEBUG-Working with file batch: datasets/lnkd/issues_dec_16.json
2019-02-12 15:59:05,936-DEBUG-Working with file batch: datasets/lnkd/issues_apr_16.json
2019-02-12 15:59:06,462-DEBUG-Working with file batch: datasets/lnkd/issues_jul_15.json
2019-02-12 15:59:06,628-DEBUG-Working with file batch: datasets/lnkd/issues_jan_15.json
2019-02-12 15:59:06,748-DEBUG-Working with file batch: datasets/lnkd/oct_16_data.json
2019-02-12 15:59:06,782-DEBUG-Working with file batch: datasets/lnkd/jul_18_data.json
2019-02-12 15:59:06,924-DEBUG-Working with file batch: datasets/lnkd/jul_16_data.json
2019-02-12 15:59:06,956-DEBUG-Working with file batch: datasets/lnkd/issues_may_17.json
2019-02-12 15:59:07,128-DEBUG-Working with file batch: datasets/lnkd/issues_nov_16.json
2019-02-12 15:59:07,236-DEBUG-Working with file batch: datasets/lnkd/nov_16_data.json
2019-02-12 15:59:07,261-DEBUG-Working with file batch: datasets/lnkd/jun_17_data.json
2019-02-12 15:59:07,290-DEBUG-Working with file batch: datasets/lnkd/jan_18_data.json
2019-02-12 15:59:07,412-DEBUG-Working with file batch: datasets/lnkd/issues_sep_15.json
2019-02-12 15:59:07,539-DEBUG-Working with file batch: datasets/lnkd/issues_may_16.json
2019-02-12 15:59:07,681-DEBUG-Working with file batch: datasets/lnkd/issues_mar_15.json
2019-02-12 15:59:07,808-DEBUG-Working with file batch: datasets/lnkd/feb_18_data.json
2019-02-12 15:59:08,264-DEBUG-Working with file batch: datasets/lnkd/may_17_data.json
2019-02-12 15:59:08,301-DEBUG-Working with file batch: datasets/lnkd/issues_oct_17.json
2019-02-12 15:59:08,518-DEBUG-Working with file batch: datasets/lnkd/issues_nov_17.json
2019-02-12 15:59:08,722-DEBUG-Working with file batch: datasets/lnkd/jul_17_data.json
2019-02-12 15:59:08,766-DEBUG-Working with file batch: datasets/lnkd/issues_aug_16.json
2019-02-12 15:59:08,930-DEBUG-Working with file batch: datasets/lnkd/apr_17_data.json
2019-02-12 15:59:08,964-DEBUG-Working with file batch: datasets/lnkd/jun_16_data.json
2019-02-12 15:59:08,986-DEBUG-Working with file batch: datasets/lnkd/issues_dec_feb_16.json
2019-02-12 15:59:09,240-DEBUG-Working with file batch: datasets/lnkd/issues_sep_16.json
2019-02-12 15:59:09,385-DEBUG-Working with file batch: datasets/lnkd/sep_18_data.json
2019-02-12 15:59:09,465-DEBUG-Working with file batch: datasets/lnkd/issues_jun_17.json
2019-02-12 15:59:09,628-DEBUG-Working with file batch: datasets/lnkd/issues_nov_15.json
2019-02-12 15:59:10,122-DEBUG-Processing ticket #0
2019-02-12 15:59:12,134-DEBUG-Processing ticket #1000
2019-02-12 15:59:14,032-DEBUG-Processing ticket #2000
2019-02-12 15:59:15,728-DEBUG-Processing ticket #3000
2019-02-12 15:59:17,907-DEBUG-Processing ticket #4000
2019-02-12 15:59:19,448-DEBUG-Processing ticket #5000
2019-02-12 15:59:21,028-DEBUG-Processing ticket #6000
2019-02-12 15:59:22,890-DEBUG-Processing ticket #7000
2019-02-12 15:59:24,888-DEBUG-Processing ticket #8000
2019-02-12 15:59:26,667-DEBUG-Processing ticket #9000
2019-02-12 15:59:28,186-DEBUG-Processing ticket #10000
2019-02-12 15:59:30,063-DEBUG-Processing ticket #11000
2019-02-12 15:59:31,767-DEBUG-Processing ticket #12000
2019-02-12 15:59:33,488-DEBUG-Processing ticket #13000
2019-02-12 15:59:35,440-DEBUG-Processing ticket #14000
2019-02-12 15:59:38,102-DEBUG-Processing ticket #15000
2019-02-12 15:59:39,516-DEBUG-Processing ticket #16000
2019-02-12 15:59:41,807-DEBUG-Processing ticket #17000
2019-02-12 15:59:43,418-DEBUG-Processing ticket #18000
2019-02-12 15:59:45,011-DEBUG-Processing ticket #19000
2019-02-12 15:59:47,219-DEBUG-Processing ticket #20000
2019-02-12 15:59:48,985-DEBUG-Processing ticket #21000
2019-02-12 15:59:50,949-DEBUG-Processing ticket #22000
2019-02-12 15:59:52,462-DEBUG-Processing ticket #23000
2019-02-12 15:59:54,075-DEBUG-Processing ticket #24000
2019-02-12 15:59:57,203-DEBUG-Processing ticket #25000
2019-02-12 15:59:59,424-DEBUG-Processing ticket #26000
2019-02-12 16:00:01,351-DEBUG-Processing ticket #27000
2019-02-12 16:00:02,905-DEBUG-Processing ticket #28000
2019-02-12 16:00:04,525-DEBUG-Processing ticket #29000
2019-02-12 16:00:05,944-DEBUG-Processing ticket #30000
2019-02-12 16:00:07,571-DEBUG-Processing ticket #31000
2019-02-12 16:00:09,359-DEBUG-Processing ticket #32000
2019-02-12 16:00:11,938-DEBUG-Processing ticket #33000
2019-02-12 16:00:13,726-DEBUG-Processing ticket #34000
2019-02-12 16:00:15,765-DEBUG-Processing ticket #35000
2019-02-12 16:00:18,135-DEBUG-Processing ticket #36000
2019-02-12 16:00:20,292-DEBUG-Processing ticket #37000
2019-02-12 16:00:21,946-DEBUG-Processing ticket #38000
2019-02-12 16:00:23,205-DEBUG-Processing ticket #39000
2019-02-12 16:00:24,642-DEBUG-Processing ticket #40000
2019-02-12 16:00:26,140-DEBUG-Processing ticket #41000
2019-02-12 16:00:27,880-DEBUG-Processing ticket #42000
2019-02-12 16:00:29,655-DEBUG-Processing ticket #43000
2019-02-12 16:00:30,535-DEBUG-Completed processing tickets  ticket, building dictionary
2019-02-12 16:00:37,830-DEBUG-Completed building dictionary, vectorizing tickets
2019-02-12 16:01:09,879-DEBUG-Ticket count:39817  Duplicates:3666 or 8.430881033967298%
 ========  NNClassifier
2019-02-12 16:01:35.201535: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.
2 AVX
2019-02-12 16:01:35.220872: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threa
ds for best performance.
2019-02-12 16:01:56,293-INFO-#  Training loss at round 0: 7.24847
2019-02-12 17:04:06,669-INFO-#  Training loss at round 200: 4.8260026
2019-02-12 17:58:17,218-INFO-#  Training loss at round 400: 4.831021
2019-02-12 18:47:11,243-INFO-#  Training loss at round 600: 4.8285007
2019-02-12 19:35:37,721-INFO-#  Training loss at round 800: 4.830666
2019-02-12 20:25:24,769-INFO-#  Training loss at round 1000: 4.889171
2019-02-12 21:15:43,572-INFO-#  Training loss at round 1200: 4.8322396
2019-02-12 22:06:38,665-INFO-#  Training loss at round 1400: 4.829549
2019-02-12 22:58:38,697-INFO-#  Training loss at round 1600: 4.8326383
2019-02-12 23:50:21,941-INFO-#  Training loss at round 1800: 4.832116
2019-02-13 00:42:20,360-INFO-#  Training loss at round 1999: 4.8313766
2019-02-13 00:42:20,920-DEBUG-findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/home/yaberraf/miniconda3
/envs/epsilon/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000
2019-02-13 00:42:20,983-DEBUG-findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('/home/yaberraf/miniconda3
/envs/epsilon/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000
2019-02-13 00:42:33,457-INFO-Accuracy: 0.06916575633875698
2019-02-13 00:44:23,023-INFO-#  Training loss at round 0: 7.2436347
2019-02-13 02:08:48,253-INFO-#  Training loss at round 200: 4.9239526
2019-02-13 03:32:30,375-INFO-#  Training loss at round 400: 4.927271
2019-02-13 04:46:10,351-INFO-#  Training loss at round 600: 4.9279523
2019-02-13 05:52:15,841-INFO-#  Training loss at round 800: 4.929431
2019-02-13 06:59:21,898-INFO-#  Training loss at round 1000: 4.9296403
2019-02-13 08:07:38,277-INFO-#  Training loss at round 1200: 21.746384
2019-02-13 09:16:10,294-INFO-#  Training loss at round 1400: 5.1703844
2019-02-13 10:24:58,126-INFO-#  Training loss at round 1600: 4.9425035
2019-02-13 11:27:36,710-INFO-#  Training loss at round 1800: 4.9250827
2019-02-13 12:25:12,684-INFO-#  Training loss at round 1999: 4.9248996
2019-02-13 12:25:26,649-INFO-Accuracy: 0.07586385327430575
2019-02-13 12:26:42,524-INFO-#  Training loss at round 0: 7.2445
2019-02-13 13:22:56,336-INFO-#  Training loss at round 200: 4.9231954
2019-02-13 13:59:52,347-INFO-#  Training loss at round 400: 4.92339
2019-02-13 14:35:26,996-INFO-#  Training loss at round 600: 4.924308
2019-02-13 15:10:54,533-INFO-#  Training loss at round 800: 4.9272723
2019-02-13 15:46:41,970-INFO-#  Training loss at round 1000: 4.9283495
2019-02-13 16:23:13,643-INFO-#  Training loss at round 1200: 4.929562
2019-02-13 16:59:41,123-INFO-#  Training loss at round 1400: 4.927168
2019-02-13 17:35:31,735-INFO-#  Training loss at round 1600: 4.929607
2019-02-13 18:14:19,542-INFO-#  Training loss at round 1800: 4.9280744
2019-02-13 18:54:32,567-INFO-#  Training loss at round 1999: 4.9293504
2019-02-13 18:54:40,780-INFO-Accuracy: 0.07146553211061922
2019-02-13 18:55:55,548-INFO-#  Training loss at round 0: 7.243718
2019-02-13 19:44:19,124-INFO-#  Training loss at round 200: 4.898072
2019-02-13 20:31:08,222-INFO-#  Training loss at round 400: 4.9010825
2019-02-13 21:17:41,731-INFO-#  Training loss at round 600: 4.9007072
2019-02-13 22:04:18,665-INFO-#  Training loss at round 800: 4.9017797
2019-02-13 22:50:37,767-INFO-#  Training loss at round 1000: 4.9022365
2019-02-13 23:34:19,381-INFO-#  Training loss at round 1200: 4.9029326
2019-02-14 00:18:41,747-INFO-#  Training loss at round 1400: 4.903846
2019-02-14 01:03:18,082-INFO-#  Training loss at round 1600: 4.9043746
2019-02-14 01:45:44,097-INFO-#  Training loss at round 1800: 4.9017406
2019-02-14 02:26:59,523-INFO-#  Training loss at round 1999: 4.9050303
2019-02-14 02:27:08,014-INFO-Accuracy: 0.06881881162503234
2019-02-14 02:27:44,106-INFO-#  Training loss at round 0: 7.244237
2019-02-14 03:09:34,158-INFO-#  Training loss at round 200: 4.8815656
2019-02-14 03:51:07,059-INFO-#  Training loss at round 400: 4.882498
2019-02-14 04:32:33,185-INFO-#  Training loss at round 600: 4.8832984
2019-02-14 05:10:49,089-INFO-#  Training loss at round 800: 4.8845534
2019-02-14 05:47:18,874-INFO-#  Training loss at round 1000: 4.885435
2019-02-14 06:24:14,735-INFO-#  Training loss at round 1200: 4.8860917
2019-02-14 07:01:20,852-INFO-#  Training loss at round 1400: 4.889248
2019-02-14 07:38:38,890-INFO-#  Training loss at round 1600: 4.888311
2019-02-14 08:16:09,007-INFO-#  Training loss at round 1800: 4.886894
2019-02-14 08:53:39,069-INFO-#  Training loss at round 1999: 4.887754
2019-02-14 08:53:46,427-INFO-Accuracy: 0.07833386035013079
Score / NNSARSClassifier<PRDALL.3-8.tanh.0.5>              0.07272956273976902       0.07264893180287507       -8.063093689394951e-05
 ========  NNClassifier
2019-02-14 08:54:26,789-INFO-#  Training loss at round 0: 7.246595
2019-02-14 09:33:15,555-INFO-#  Training loss at round 200: 4.825134
2019-02-14 10:12:04,545-INFO-#  Training loss at round 400: 4.8243947
2019-02-14 10:51:12,345-INFO-#  Training loss at round 600: 4.5237637
2019-02-14 11:27:59,101-INFO-#  Training loss at round 800: 4.3709292
2019-02-14 12:04:14,717-INFO-#  Training loss at round 1000: 4.3385997
2019-02-14 12:38:30,865-INFO-#  Training loss at round 1200: 4.3701324
2019-02-14 13:13:47,246-INFO-#  Training loss at round 1400: 4.0312076
2019-02-14 13:49:40,055-INFO-#  Training loss at round 1600: 4.000768
2019-02-14 14:26:41,299-INFO-#  Training loss at round 1800: 3.900934
2019-02-14 15:07:16,374-INFO-#  Training loss at round 1999: 3.8779848
2019-02-14 15:07:24,909-INFO-Accuracy: 0.17432300350715804
2019-02-14 15:10:25,269-INFO-#  Training loss at round 0: 7.249162
2019-02-14 15:56:19,591-INFO-#  Training loss at round 200: 4.647014
2019-02-14 16:42:48,747-INFO-#  Training loss at round 400: 4.063596
2019-02-14 17:30:07,904-INFO-#  Training loss at round 600: 3.754903
2019-02-14 18:15:52,742-INFO-#  Training loss at round 800: 3.5031998
2019-02-14 18:58:12,659-INFO-#  Training loss at round 1000: 3.425445
2019-02-14 19:34:20,959-INFO-#  Training loss at round 1200: 3.408491
2019-02-14 20:08:17,957-INFO-#  Training loss at round 1400: 3.3875532
2019-02-14 20:42:06,281-INFO-#  Training loss at round 1600: 3.3603191
2019-02-14 21:16:20,712-INFO-#  Training loss at round 1800: 3.2534788
2019-02-14 21:50:11,265-INFO-#  Training loss at round 1999: 3.2184913
2019-02-14 21:50:18,704-INFO-Accuracy: 0.27525441269476225
2019-02-14 21:52:26,627-INFO-#  Training loss at round 0: 7.2427177
2019-02-14 22:32:55,112-INFO-#  Training loss at round 200: 4.923208
2019-02-14 23:06:41,730-INFO-#  Training loss at round 400: 4.4793386
2019-02-14 23:40:29,038-INFO-#  Training loss at round 600: 4.4621334
2019-02-15 00:14:16,202-INFO-#  Training loss at round 800: 4.1227927
2019-02-15 00:48:24,675-INFO-#  Training loss at round 1000: 3.8108048
2019-02-15 01:22:21,523-INFO-#  Training loss at round 1200: 3.7112057
2019-02-15 01:56:07,558-INFO-#  Training loss at round 1400: 3.664383
2019-02-15 02:29:53,892-INFO-#  Training loss at round 1600: 3.6793413
2019-02-15 03:03:39,975-INFO-#  Training loss at round 1800: 3.6719856
2019-02-15 03:37:17,891-INFO-#  Training loss at round 1999: 3.662594
2019-02-15 03:37:24,744-INFO-Accuracy: 0.20470879089288793
2019-02-15 03:38:57,173-INFO-#  Training loss at round 0: 7.2484784
2019-02-15 04:27:50,759-INFO-#  Training loss at round 200: 4.896791
2019-02-15 05:14:19,114-INFO-#  Training loss at round 400: 4.8967423
2019-02-15 06:00:55,476-INFO-#  Training loss at round 600: 4.8967123
2019-02-15 06:47:32,187-INFO-#  Training loss at round 800: 4.896563
2019-02-15 07:34:12,057-INFO-#  Training loss at round 1000: 4.8847637
2019-02-15 08:20:59,760-INFO-#  Training loss at round 1200: 4.7476387
2019-02-15 09:08:05,490-INFO-#  Training loss at round 1400: 4.1410413
2019-02-15 09:55:47,966-INFO-#  Training loss at round 1600: 3.985744
2019-02-15 10:43:52,268-INFO-#  Training loss at round 1800: 3.9686835
2019-02-15 11:31:05,935-INFO-#  Training loss at round 1999: 3.9525213
2019-02-15 11:31:15,256-INFO-Accuracy: 0.1308534797481818
2019-02-15 11:31:46,134-INFO-#  Training loss at round 0: 7.2444005
2019-02-15 12:12:16,520-INFO-#  Training loss at round 200: 4.881326
2019-02-15 12:51:34,208-INFO-#  Training loss at round 400: 4.881272
2019-02-15 13:30:57,342-INFO-#  Training loss at round 600: 4.879714
2019-02-15 14:10:22,570-INFO-#  Training loss at round 800: 4.514427
2019-02-15 14:50:13,605-INFO-#  Training loss at round 1000: 4.339162
2019-02-15 15:29:48,374-INFO-#  Training loss at round 1200: 4.1918206
2019-02-15 16:09:32,452-INFO-#  Training loss at round 1400: 4.005603
2019-02-15 16:49:58,183-INFO-#  Training loss at round 1600: 3.9843256
2019-02-15 17:30:52,687-INFO-#  Training loss at round 1800: 3.942509
2019-02-15 18:11:42,722-INFO-#  Training loss at round 1999: 3.9268894
2019-02-15 18:11:51,000-INFO-Accuracy: 0.17492166613964988
Score / NNSARSClassifier<PRDALL.3-8.tanh.0.05>             0.19201227059652798       0.12692276487392842       -0.06508950572259956
 ========  NNClassifier
2019-02-15 18:12:20,200-INFO-#  Training loss at round 0: 7.240938
2019-02-15 18:56:12,447-INFO-#  Training loss at round 200: 4.3349366
2019-02-15 19:39:04,691-INFO-#  Training loss at round 400: 3.087046
2019-02-15 20:22:02,549-INFO-#  Training loss at round 600: 2.6218288
2019-02-15 21:05:00,125-INFO-#  Training loss at round 800: 2.414873
2019-02-15 21:48:52,691-INFO-#  Training loss at round 1000: 2.24388
2019-02-15 22:32:13,457-INFO-#  Training loss at round 1200: 2.0617096
2019-02-15 23:16:01,076-INFO-#  Training loss at round 1400: 1.965968
2019-02-16 00:00:22,705-INFO-#  Training loss at round 1600: 1.8337101
2019-02-16 00:44:51,946-INFO-#  Training loss at round 1800: 1.7596902
2019-02-16 01:29:11,897-INFO-#  Training loss at round 1999: 1.6566623
2019-02-16 01:29:21,403-INFO-Accuracy: 0.5923072500431208
2019-02-16 01:29:51,459-INFO-#  Training loss at round 0: 7.24797
2019-02-16 02:14:28,753-INFO-#  Training loss at round 200: 4.5159044
2019-02-16 02:59:24,002-INFO-#  Training loss at round 400: 2.9962392
2019-02-16 03:44:08,785-INFO-#  Training loss at round 600: 2.4098747
2019-02-16 04:28:35,404-INFO-#  Training loss at round 800: 2.0618393
2019-02-16 05:13:14,988-INFO-#  Training loss at round 1000: 1.8606709
2019-02-16 05:58:31,299-INFO-#  Training loss at round 1200: 1.7036602
2019-02-16 06:44:12,851-INFO-#  Training loss at round 1400: 1.586441
2019-02-16 07:29:55,717-INFO-#  Training loss at round 1600: 1.4860886
2019-02-16 08:15:49,780-INFO-#  Training loss at round 1800: 1.4089301
2019-02-16 09:01:27,380-INFO-#  Training loss at round 1999: 1.3464377
2019-02-16 09:01:37,243-INFO-Accuracy: 0.6833496234117173
2019-02-16 09:02:08,223-INFO-#  Training loss at round 0: 7.2396746
2019-02-16 09:47:59,553-INFO-#  Training loss at round 200: 3.5265548
2019-02-16 10:33:52,869-INFO-#  Training loss at round 400: 2.7345974
2019-02-16 11:19:50,876-INFO-#  Training loss at round 600: 2.5741227
2019-02-16 12:06:02,013-INFO-#  Training loss at round 800: 2.235777
2019-02-16 12:52:22,578-INFO-#  Training loss at round 1000: 2.0424159
2019-02-16 13:38:42,724-INFO-#  Training loss at round 1200: 1.867229
2019-02-16 14:25:05,020-INFO-#  Training loss at round 1400: 1.7212008
2019-02-16 15:11:23,745-INFO-#  Training loss at round 1600: 1.6075995
2019-02-16 15:57:43,483-INFO-#  Training loss at round 1800: 1.5038186
2019-02-16 16:43:49,191-INFO-#  Training loss at round 1999: 1.439689
2019-02-16 16:43:59,202-INFO-Accuracy: 0.663255332605071
2019-02-16 16:44:32,420-INFO-#  Training loss at round 0: 7.2440343
2019-02-16 17:30:57,684-INFO-#  Training loss at round 200: 4.46524
2019-02-16 18:17:39,138-INFO-#  Training loss at round 400: 3.315779
2019-02-16 19:04:27,030-INFO-#  Training loss at round 600: 2.6967888
2019-02-16 19:51:20,026-INFO-#  Training loss at round 800: 2.2837143
2019-02-16 20:38:08,833-INFO-#  Training loss at round 1000: 1.98724
2019-02-16 21:25:38,749-INFO-#  Training loss at round 1200: 1.8258286
2019-02-16 22:12:41,960-INFO-#  Training loss at round 1400: 1.6972046
2019-02-16 22:59:51,205-INFO-#  Training loss at round 1600: 1.5618649
2019-02-16 23:46:58,317-INFO-#  Training loss at round 1800: 1.477832
2019-02-17 00:33:50,476-INFO-#  Training loss at round 1999: 1.3899848
2019-02-17 00:34:00,497-INFO-Accuracy: 0.674591082875787
2019-02-17 00:34:32,383-INFO-#  Training loss at round 0: 7.2500896
2019-02-17 01:21:40,814-INFO-#  Training loss at round 200: 4.884843
2019-02-17 02:08:52,558-INFO-#  Training loss at round 400: 4.511703
2019-02-17 02:56:06,726-INFO-#  Training loss at round 600: 3.3228781
2019-02-17 03:43:18,654-INFO-#  Training loss at round 800: 2.8046346
2019-02-17 04:30:26,550-INFO-#  Training loss at round 1000: 2.5061975
2019-02-17 05:17:33,598-INFO-#  Training loss at round 1200: 2.3204486
2019-02-17 06:04:42,880-INFO-#  Training loss at round 1400: 2.1357896
2019-02-17 06:52:04,775-INFO-#  Training loss at round 1600: 1.9162526
2019-02-17 07:39:30,177-INFO-#  Training loss at round 1800: 1.7885035
2019-02-17 08:27:00,785-INFO-#  Training loss at round 1999: 1.6861584
2019-02-17 08:27:11,000-INFO-Accuracy: 0.5832638629373041
Score / NNSARSClassifier<PRDALL.3-8.tanh.0.005>            0.6393534303746           0.10413231613773277       -0.5352211142368672
 ========  NNClassifier
2019-02-17 08:27:43,547-INFO-#  Training loss at round 0: 7.2436237
2019-02-17 09:15:29,904-INFO-#  Training loss at round 200: 4.8263927
2019-02-17 10:03:24,964-INFO-#  Training loss at round 400: 4.8296337
2019-02-17 10:51:32,392-INFO-#  Training loss at round 600: 4.830619
2019-02-17 11:39:39,444-INFO-#  Training loss at round 800: 4.832506
2019-02-17 12:27:45,792-INFO-#  Training loss at round 1000: 4.8331323
2019-02-17 13:15:50,603-INFO-#  Training loss at round 1200: 4.8322945
2019-02-17 14:03:58,111-INFO-#  Training loss at round 1400: 4.8332477
2019-02-17 14:52:12,904-INFO-#  Training loss at round 1600: 4.831268
2019-02-17 15:40:29,276-INFO-#  Training loss at round 1800: 4.8302135
2019-02-17 16:28:34,038-INFO-#  Training loss at round 1999: 4.8314466
2019-02-17 16:28:44,641-INFO-Accuracy: 0.06077157477145978
2019-02-17 16:29:28,050-INFO-#  Training loss at round 0: 7.244884
2019-02-17 17:17:48,229-INFO-#  Training loss at round 200: 4.924008
2019-02-17 18:06:08,754-INFO-#  Training loss at round 400: 4.9259186
2019-02-17 18:54:34,225-INFO-#  Training loss at round 600: 4.926677
2019-02-17 19:43:05,483-INFO-#  Training loss at round 800: 4.928995
2019-02-17 20:32:00,193-INFO-#  Training loss at round 1000: 4.9278555
2019-02-17 21:21:17,796-INFO-#  Training loss at round 1200: 4.9294777
2019-02-17 22:10:36,900-INFO-#  Training loss at round 1400: 4.929347
2019-02-17 22:59:53,581-INFO-#  Training loss at round 1600: 4.9287686
2019-02-17 23:49:12,817-INFO-#  Training loss at round 1800: 4.928832
2019-02-18 00:38:16,713-INFO-#  Training loss at round 1999: 4.9312506
2019-02-18 00:38:27,609-INFO-Accuracy: 0.07589260047145403
2019-02-18 00:39:00,933-INFO-#  Training loss at round 0: 7.244495
2019-02-18 01:28:15,177-INFO-#  Training loss at round 200: 4.923965
2019-02-18 02:17:28,765-INFO-#  Training loss at round 400: 4.9250803
2019-02-18 03:06:43,995-INFO-#  Training loss at round 600: 4.9267106
2019-02-18 03:56:06,163-INFO-#  Training loss at round 800: 4.9270134
2019-02-18 04:45:30,937-INFO-#  Training loss at round 1000: 4.9269905
2019-02-18 05:34:45,492-INFO-#  Training loss at round 1200: 4.930798
2019-02-18 06:24:02,361-INFO-#  Training loss at round 1400: 4.930973
2019-02-18 07:13:22,237-INFO-#  Training loss at round 1600: 4.9294477
2019-02-18 08:02:48,210-INFO-#  Training loss at round 1800: 4.9304814
2019-02-18 08:52:35,016-INFO-#  Training loss at round 1999: 4.931605
2019-02-18 08:52:46,380-INFO-Accuracy: 0.07132179612487782
2019-02-18 08:53:20,963-INFO-#  Training loss at round 0: 7.2477016
2019-02-18 09:42:57,508-INFO-#  Training loss at round 200: 4.8985844
2019-02-18 10:32:40,940-INFO-#  Training loss at round 400: 4.90213
2019-02-18 11:23:03,296-INFO-#  Training loss at round 600: 4.9023747
2019-02-18 12:12:49,877-INFO-#  Training loss at round 800: 31.515825
2019-02-18 13:02:38,662-INFO-#  Training loss at round 1000: 4.899295
2019-02-18 13:52:31,786-INFO-#  Training loss at round 1200: 4.898035
2019-02-18 14:42:22,907-INFO-#  Training loss at round 1400: 4.898035
2019-02-18 15:32:48,310-INFO-#  Training loss at round 1600: 4.898035
2019-02-18 16:22:48,596-INFO-#  Training loss at round 1800: 4.8980346
2019-02-18 17:12:34,582-INFO-#  Training loss at round 1999: 4.8980365
2019-02-18 17:12:45,604-INFO-Accuracy: 0.06873257251272027
2019-02-18 17:13:19,320-INFO-#  Training loss at round 0: 7.244291
2019-02-18 18:03:22,834-INFO-#  Training loss at round 200: 4.8817396
2019-02-18 18:53:22,288-INFO-#  Training loss at round 400: 4.8847194
2019-02-18 19:43:22,431-INFO-#  Training loss at round 600: 4.8861413
2019-02-18 20:33:28,566-INFO-#  Training loss at round 800: 4.886679
2019-02-18 21:24:00,636-INFO-#  Training loss at round 1000: 4.8851047
2019-02-18 22:14:29,718-INFO-#  Training loss at round 1200: 4.8900175
2019-02-18 23:04:53,427-INFO-#  Training loss at round 1400: 4.884984
2019-02-18 23:55:24,501-INFO-#  Training loss at round 1600: 4.8870554
2019-02-19 00:45:54,252-INFO-#  Training loss at round 1800: 4.8885164
2019-02-19 01:36:03,587-INFO-#  Training loss at round 1999: 4.8877344
2019-02-19 01:36:14,653-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.5-8.tanh.0.5>              0.07098173447535785       0.05551658731627048       -0.015465147159087374
 ========  NNClassifier
2019-02-19 01:36:48,605-INFO-#  Training loss at round 0: 7.244425
2019-02-19 02:27:16,936-INFO-#  Training loss at round 200: 4.8257747
2019-02-19 03:17:48,854-INFO-#  Training loss at round 400: 4.825752
2019-02-19 04:08:16,917-INFO-#  Training loss at round 600: 4.8257465
2019-02-19 04:58:58,956-INFO-#  Training loss at round 800: 4.825744
2019-02-19 05:49:35,076-INFO-#  Training loss at round 1000: 4.825719
2019-02-19 06:40:10,619-INFO-#  Training loss at round 1200: 4.760774
2019-02-19 07:30:49,822-INFO-#  Training loss at round 1400: 4.7380977
2019-02-19 08:21:28,179-INFO-#  Training loss at round 1600: 4.7373033
2019-02-19 09:12:17,900-INFO-#  Training loss at round 1800: 4.7369685
2019-02-19 10:03:01,260-INFO-#  Training loss at round 1999: 4.736857
2019-02-19 10:03:12,637-INFO-Accuracy: 0.08713275455643074
2019-02-19 10:03:46,913-INFO-#  Training loss at round 0: 7.2457376
2019-02-19 10:54:30,071-INFO-#  Training loss at round 200: 4.9239674
2019-02-19 11:45:42,836-INFO-#  Training loss at round 400: 4.922042
2019-02-19 12:26:10,077-INFO-#  Training loss at round 600: 4.9217668
2019-02-19 13:06:34,153-INFO-#  Training loss at round 800: 4.9216747
2019-02-19 13:46:49,561-INFO-#  Training loss at round 1000: 4.921622
2019-02-19 14:26:57,207-INFO-#  Training loss at round 1200: 4.9215784
2019-02-19 15:07:24,207-INFO-#  Training loss at round 1400: 4.9201174
2019-02-19 15:47:21,027-INFO-#  Training loss at round 1600: 4.9205694
2019-02-19 16:27:42,099-INFO-#  Training loss at round 1800: 4.9204044
2019-02-19 17:08:05,799-INFO-#  Training loss at round 1999: 4.9202986
2019-02-19 17:08:13,920-INFO-Accuracy: 0.07618007244293681
2019-02-19 17:08:51,218-INFO-#  Training loss at round 0: 7.246557
2019-02-19 17:49:08,263-INFO-#  Training loss at round 200: 4.924007
2019-02-19 18:28:54,175-INFO-#  Training loss at round 400: 4.923998
2019-02-19 19:09:20,929-INFO-#  Training loss at round 600: 4.9239974
2019-02-19 19:48:28,007-INFO-#  Training loss at round 800: 4.9239874
2019-02-19 20:27:33,196-INFO-#  Training loss at round 1000: 4.92399
2019-02-19 21:07:00,209-INFO-#  Training loss at round 1200: 4.923971
2019-02-19 21:46:38,533-INFO-#  Training loss at round 1400: 4.9275208
2019-02-19 22:26:20,256-INFO-#  Training loss at round 1600: 4.923986
2019-02-19 23:06:19,253-INFO-#  Training loss at round 1800: 4.9239845
2019-02-19 23:45:08,754-INFO-#  Training loss at round 1999: 4.923984
2019-02-19 23:45:16,993-INFO-Accuracy: 0.07132179612487782
2019-02-19 23:45:44,467-INFO-#  Training loss at round 0: 7.242555
2019-02-20 00:24:54,486-INFO-#  Training loss at round 200: 4.8983364
2019-02-20 01:04:16,635-INFO-#  Training loss at round 400: 4.897864
2019-02-20 01:43:38,795-INFO-#  Training loss at round 600: 4.897851
2019-02-20 02:23:12,628-INFO-#  Training loss at round 800: 4.89784
2019-02-20 03:02:45,395-INFO-#  Training loss at round 1000: 4.8978333
2019-02-20 03:42:29,755-INFO-#  Training loss at round 1200: 4.8978314
2019-02-20 04:22:47,811-INFO-#  Training loss at round 1400: 4.897834
2019-02-20 05:03:13,851-INFO-#  Training loss at round 1600: 4.8978252
2019-02-20 05:43:39,846-INFO-#  Training loss at round 1800: 4.8978276
2019-02-20 06:23:56,985-INFO-#  Training loss at round 1999: 4.8978276
2019-02-20 06:24:05,038-INFO-Accuracy: 0.06876131888349096
2019-02-20 06:24:33,253-INFO-#  Training loss at round 0: 7.2522407
2019-02-20 07:05:05,553-INFO-#  Training loss at round 200: 4.881518
2019-02-20 07:45:41,842-INFO-#  Training loss at round 400: 4.8814845
2019-02-20 08:26:17,492-INFO-#  Training loss at round 600: 4.881495
2019-02-20 09:07:12,325-INFO-#  Training loss at round 800: 4.881476
2019-02-20 09:48:42,799-INFO-#  Training loss at round 1000: 4.88148
2019-02-20 10:30:23,563-INFO-#  Training loss at round 1200: 4.8814726
2019-02-20 11:12:27,649-INFO-#  Training loss at round 1400: 4.881466
2019-02-20 11:54:55,854-INFO-#  Training loss at round 1600: 4.881466
2019-02-20 12:36:59,620-INFO-#  Training loss at round 1800: 4.881451
2019-02-20 13:18:51,545-INFO-#  Training loss at round 1999: 4.881455
2019-02-20 13:19:00,282-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.5-8.tanh.0.05>             0.07631721410080275       0.07425868229154932       -0.0020585318092534305
 ========  NNClassifier
2019-02-20 13:19:29,373-INFO-#  Training loss at round 0: 7.2520933
2019-02-20 14:01:50,781-INFO-#  Training loss at round 200: 4.8279953
2019-02-20 14:44:31,195-INFO-#  Training loss at round 400: 3.8660004
2019-02-20 15:27:15,345-INFO-#  Training loss at round 600: 3.424257
2019-02-20 16:09:20,909-INFO-#  Training loss at round 800: 3.036136
2019-02-20 16:52:03,383-INFO-#  Training loss at round 1000: 2.9030547
2019-02-20 17:33:37,170-INFO-#  Training loss at round 1200: 2.757111
2019-02-20 18:15:33,832-INFO-#  Training loss at round 1400: 2.671663
2019-02-20 18:58:41,763-INFO-#  Training loss at round 1600: 2.5618093
2019-02-20 19:42:05,809-INFO-#  Training loss at round 1800: 2.5267415
2019-02-20 20:21:22,849-INFO-#  Training loss at round 1999: 2.4683304
2019-02-20 20:21:30,714-INFO-Accuracy: 0.3462024952567125
2019-02-20 20:22:14,797-INFO-#  Training loss at round 0: 7.248049
2019-02-20 21:02:17,039-INFO-#  Training loss at round 200: 4.9270105
2019-02-20 21:42:58,213-INFO-#  Training loss at round 400: 4.923571
2019-02-20 22:24:14,640-INFO-#  Training loss at round 600: 4.9197936
2019-02-20 23:07:19,526-INFO-#  Training loss at round 800: 4.258671
2019-02-20 23:50:00,728-INFO-#  Training loss at round 1000: 3.7947423
2019-02-21 00:32:46,245-INFO-#  Training loss at round 1200: 3.3023052
2019-02-21 01:15:46,822-INFO-#  Training loss at round 1400: 3.1419148
2019-02-21 01:58:49,171-INFO-#  Training loss at round 1600: 3.0463629
2019-02-21 02:41:51,324-INFO-#  Training loss at round 1800: 2.9912388
2019-02-21 03:24:41,291-INFO-#  Training loss at round 1999: 2.8954337
2019-02-21 03:24:50,255-INFO-Accuracy: 0.24696717070085666
2019-02-21 03:25:28,272-INFO-#  Training loss at round 0: 7.245434
2019-02-21 04:08:30,621-INFO-#  Training loss at round 200: 4.9265504
2019-02-21 04:50:11,210-INFO-#  Training loss at round 400: 4.9230742
2019-02-21 05:31:38,114-INFO-#  Training loss at round 600: 4.9202366
2019-02-21 06:13:05,323-INFO-#  Training loss at round 800: 4.071985
2019-02-21 06:54:40,455-INFO-#  Training loss at round 1000: 3.3161573
2019-02-21 07:36:15,328-INFO-#  Training loss at round 1200: 3.070148
2019-02-21 08:17:53,692-INFO-#  Training loss at round 1400: 2.9406958
2019-02-21 08:59:28,474-INFO-#  Training loss at round 1600: 2.8095474
2019-02-21 09:41:12,259-INFO-#  Training loss at round 1800: 2.7197204
2019-02-21 10:22:51,243-INFO-#  Training loss at round 1999: 2.6749597
2019-02-21 10:23:00,314-INFO-Accuracy: 0.27856034036681426
2019-02-21 10:23:31,831-INFO-#  Training loss at round 0: 7.248791
2019-02-21 11:05:57,339-INFO-#  Training loss at round 200: 4.900884
2019-02-21 11:48:38,599-INFO-#  Training loss at round 400: 4.897564
2019-02-21 12:31:00,848-INFO-#  Training loss at round 600: 4.8949423
2019-02-21 13:13:46,614-INFO-#  Training loss at round 800: 3.9816136
2019-02-21 13:55:59,287-INFO-#  Training loss at round 1000: 3.4828722
2019-02-21 14:39:08,268-INFO-#  Training loss at round 1200: 3.2228007
2019-02-21 15:22:10,683-INFO-#  Training loss at round 1400: 3.006531
2019-02-21 16:04:26,999-INFO-#  Training loss at round 1600: 2.8385785
2019-02-21 16:47:01,381-INFO-#  Training loss at round 1800: 2.713711
2019-02-21 17:29:56,494-INFO-#  Training loss at round 1999: 2.6058028
2019-02-21 17:30:05,307-INFO-Accuracy: 0.3409319573403858
2019-02-21 17:30:54,747-INFO-#  Training loss at round 0: 7.2426686
2019-02-21 18:17:17,933-INFO-#  Training loss at round 200: 4.8850994
2019-02-21 19:03:06,707-INFO-#  Training loss at round 400: 4.881755
2019-02-21 19:47:45,362-INFO-#  Training loss at round 600: 4.877818
2019-02-21 20:30:38,411-INFO-#  Training loss at round 800: 3.8878596
2019-02-21 21:13:41,820-INFO-#  Training loss at round 1000: 3.3027601
2019-02-21 21:56:44,646-INFO-#  Training loss at round 1200: 3.0752513
2019-02-21 22:39:29,595-INFO-#  Training loss at round 1400: 2.8996444
2019-02-21 23:22:14,012-INFO-#  Training loss at round 1600: 2.843365
2019-02-22 00:04:52,477-INFO-#  Training loss at round 1800: 2.7636619
2019-02-22 00:47:38,148-INFO-#  Training loss at round 1999: 2.7090707
2019-02-22 00:47:47,025-INFO-Accuracy: 0.2669675453473999
Score / NNSARSClassifier<PRDALL.5-8.tanh.0.005>            0.2959259018024338        0.06262229814360375       -0.23330360365883002
 ========  NNClassifier
2019-02-22 00:48:30,398-INFO-#  Training loss at round 0: 7.2469535
2019-02-22 01:18:25,731-INFO-#  Training loss at round 200: 4.8259006
2019-02-22 01:48:59,662-INFO-#  Training loss at round 400: 18.451004
2019-02-22 02:19:52,977-INFO-#  Training loss at round 600: 4.8569193
2019-02-22 02:50:50,844-INFO-#  Training loss at round 800: 4.8254585
2019-02-22 03:22:11,179-INFO-#  Training loss at round 1000: 4.825451
2019-02-22 03:54:41,585-INFO-#  Training loss at round 1200: 4.8254437
2019-02-22 04:27:36,993-INFO-#  Training loss at round 1400: 4.825434
2019-02-22 05:00:32,028-INFO-#  Training loss at round 1600: 4.82545
2019-02-22 05:33:25,970-INFO-#  Training loss at round 1800: 4.8254347
2019-02-22 06:06:09,889-INFO-#  Training loss at round 1999: 4.8254404
2019-02-22 06:06:17,603-INFO-Accuracy: 0.06916575633875698
2019-02-22 06:06:42,951-INFO-#  Training loss at round 0: 7.2476883
2019-02-22 06:39:55,853-INFO-#  Training loss at round 200: 4.9226127
2019-02-22 07:13:16,307-INFO-#  Training loss at round 400: 4.923764
2019-02-22 07:47:05,666-INFO-#  Training loss at round 600: 4.925909
2019-02-22 08:20:58,606-INFO-#  Training loss at round 800: 4.926339
2019-02-22 08:54:48,205-INFO-#  Training loss at round 1000: 4.9296536
2019-02-22 09:28:42,496-INFO-#  Training loss at round 1200: 4.935874
2019-02-22 10:02:42,583-INFO-#  Training loss at round 1400: 4.93736
2019-02-22 10:36:47,052-INFO-#  Training loss at round 1600: 4.939822
2019-02-22 11:10:52,769-INFO-#  Training loss at round 1800: 4.9423523
2019-02-22 11:44:52,907-INFO-#  Training loss at round 1999: 4.944
2019-02-22 11:45:00,873-INFO-Accuracy: 0.049962628643707235
2019-02-22 11:45:26,605-INFO-#  Training loss at round 0: 7.243639
2019-02-22 12:19:37,707-INFO-#  Training loss at round 200: 4.9239597
2019-02-22 12:54:04,011-INFO-#  Training loss at round 400: 4.9261546
2019-02-22 13:28:49,466-INFO-#  Training loss at round 600: 4.9305606
2019-02-22 14:03:56,726-INFO-#  Training loss at round 800: 4.934211
2019-02-22 14:39:28,925-INFO-#  Training loss at round 1000: 4.935332
2019-02-22 15:15:07,531-INFO-#  Training loss at round 1200: 4.936177
2019-02-22 15:50:40,112-INFO-#  Training loss at round 1400: 4.9387217
2019-02-22 16:26:19,386-INFO-#  Training loss at round 1600: 4.940121
2019-02-22 17:02:06,229-INFO-#  Training loss at round 1800: 4.941747
2019-02-22 17:37:44,603-INFO-#  Training loss at round 1999: 4.944023
2019-02-22 17:37:53,271-INFO-Accuracy: 0.04720289771747255
2019-02-22 17:38:19,829-INFO-#  Training loss at round 0: 7.2536416
2019-02-22 18:14:08,698-INFO-#  Training loss at round 200: 4.898394
2019-02-22 18:50:00,942-INFO-#  Training loss at round 400: 4.9019465
2019-02-22 19:25:52,736-INFO-#  Training loss at round 600: 52.57194
2019-02-22 20:01:42,339-INFO-#  Training loss at round 800: 5.4261165
2019-02-22 20:37:32,613-INFO-#  Training loss at round 1000: 4.9040537
2019-02-22 21:13:46,282-INFO-#  Training loss at round 1200: 4.900197
2019-02-22 21:49:56,052-INFO-#  Training loss at round 1400: 4.898647
2019-02-22 22:25:39,107-INFO-#  Training loss at round 1600: 4.897916
2019-02-22 23:01:25,048-INFO-#  Training loss at round 1800: 4.897464
2019-02-22 23:37:00,084-INFO-#  Training loss at round 1999: 4.897402
2019-02-22 23:37:08,574-INFO-Accuracy: 0.06884755799580303
2019-02-22 23:37:34,812-INFO-#  Training loss at round 0: 7.247279
2019-02-23 00:13:20,673-INFO-#  Training loss at round 200: 4.8812385
2019-02-23 00:49:05,008-INFO-#  Training loss at round 400: 4.883446
2019-02-23 01:24:50,962-INFO-#  Training loss at round 600: 4.885971
2019-02-23 02:00:56,088-INFO-#  Training loss at round 800: 4.9040794
2019-02-23 02:37:10,999-INFO-#  Training loss at round 1000: 17.295826
2019-02-23 03:13:45,941-INFO-#  Training loss at round 1200: 4.990475
2019-02-23 03:50:18,070-INFO-#  Training loss at round 1400: 4.9123206
2019-02-23 04:26:50,912-INFO-#  Training loss at round 1600: 4.8914294
2019-02-23 05:03:51,082-INFO-#  Training loss at round 1800: 4.8864493
2019-02-23 05:40:11,190-INFO-#  Training loss at round 1999: 4.8847613
2019-02-23 05:40:19,989-INFO-Accuracy: 0.07821887486704804
Score / NNSARSClassifier<PRDALL.3-16.tanh.0.5>             0.06267954311255756       0.062300535804254854      -0.0003790073083027104
 ========  NNClassifier
2019-02-23 05:40:47,181-INFO-#  Training loss at round 0: 7.2530947
2019-02-23 06:17:21,858-INFO-#  Training loss at round 200: 4.381333
2019-02-23 06:54:23,409-INFO-#  Training loss at round 400: 4.2019067
2019-02-23 07:30:49,899-INFO-#  Training loss at round 600: 3.6500542
2019-02-23 08:07:20,897-INFO-#  Training loss at round 800: 3.5073419
2019-02-23 08:43:53,594-INFO-#  Training loss at round 1000: 3.3261943
2019-02-23 09:20:16,041-INFO-#  Training loss at round 1200: 2.9652953
2019-02-23 09:56:36,983-INFO-#  Training loss at round 1400: 2.8009477
2019-02-23 10:32:57,502-INFO-#  Training loss at round 1600: 2.7813509
2019-02-23 11:09:21,995-INFO-#  Training loss at round 1800: 2.7434628
2019-02-23 11:45:37,839-INFO-#  Training loss at round 1999: 2.7125523
2019-02-23 11:45:46,679-INFO-Accuracy: 0.3380382912666015
2019-02-23 11:46:13,271-INFO-#  Training loss at round 0: 7.2466793
2019-02-23 12:22:38,852-INFO-#  Training loss at round 200: 3.786684
2019-02-23 12:59:05,675-INFO-#  Training loss at round 400: 3.2999978
2019-02-23 13:35:36,178-INFO-#  Training loss at round 600: 3.15603
2019-02-23 14:12:23,445-INFO-#  Training loss at round 800: 2.9467194
2019-02-23 14:49:30,325-INFO-#  Training loss at round 1000: 2.86377
2019-02-23 15:26:44,113-INFO-#  Training loss at round 1200: 2.8659813
2019-02-23 16:04:00,873-INFO-#  Training loss at round 1400: 2.8272936
2019-02-23 16:41:17,222-INFO-#  Training loss at round 1600: 2.810387
2019-02-23 17:18:44,204-INFO-#  Training loss at round 1800: 2.789081
2019-02-23 17:56:06,759-INFO-#  Training loss at round 1999: 2.7838812
2019-02-23 17:56:15,989-INFO-Accuracy: 0.37175875352153165
2019-02-23 17:56:43,349-INFO-#  Training loss at round 0: 7.2601194
2019-02-23 18:34:12,150-INFO-#  Training loss at round 200: 4.8823943
2019-02-23 19:11:40,102-INFO-#  Training loss at round 400: 4.488924
2019-02-23 19:49:08,611-INFO-#  Training loss at round 600: 4.1115947
2019-02-23 20:26:34,926-INFO-#  Training loss at round 800: 3.8642433
2019-02-23 21:04:02,781-INFO-#  Training loss at round 1000: 3.683279
2019-02-23 21:41:35,726-INFO-#  Training loss at round 1200: 3.5173945
2019-02-23 22:19:04,497-INFO-#  Training loss at round 1400: 3.4055002
2019-02-23 22:56:30,480-INFO-#  Training loss at round 1600: 3.3786883
2019-02-23 23:33:55,581-INFO-#  Training loss at round 1800: 3.3267522
2019-02-24 00:11:09,248-INFO-#  Training loss at round 1999: 3.2731874
2019-02-24 00:11:18,194-INFO-Accuracy: 0.26740642787328234
2019-02-24 00:11:45,655-INFO-#  Training loss at round 0: 7.2505403
2019-02-24 00:49:13,639-INFO-#  Training loss at round 200: 4.179625
2019-02-24 01:26:44,441-INFO-#  Training loss at round 400: 3.1346772
2019-02-24 02:04:18,826-INFO-#  Training loss at round 600: 3.0048728
2019-02-24 02:42:03,156-INFO-#  Training loss at round 800: 2.8651226
2019-02-24 03:19:58,743-INFO-#  Training loss at round 1000: 2.8118129
2019-02-24 03:57:58,989-INFO-#  Training loss at round 1200: 2.830615
2019-02-24 04:35:51,845-INFO-#  Training loss at round 1400: 2.7372828
2019-02-24 05:13:56,596-INFO-#  Training loss at round 1600: 2.7244182
2019-02-24 05:52:00,648-INFO-#  Training loss at round 1800: 2.738202
2019-02-24 06:29:50,055-INFO-#  Training loss at round 1999: 2.7603967
2019-02-24 06:29:59,565-INFO-Accuracy: 0.3558800701411447
2019-02-24 06:30:27,230-INFO-#  Training loss at round 0: 7.24237
2019-02-24 07:08:23,392-INFO-#  Training loss at round 200: 4.881671
2019-02-24 07:46:13,610-INFO-#  Training loss at round 400: 4.4808407
2019-02-24 08:24:01,669-INFO-#  Training loss at round 600: 3.9027133
2019-02-24 09:01:52,341-INFO-#  Training loss at round 800: 3.5979395
2019-02-24 09:39:42,178-INFO-#  Training loss at round 1000: 3.324227
2019-02-24 10:17:29,679-INFO-#  Training loss at round 1200: 3.204479
2019-02-24 10:55:18,285-INFO-#  Training loss at round 1400: 3.144848
2019-02-24 11:33:03,127-INFO-#  Training loss at round 1600: 3.1298013
2019-02-24 12:10:46,641-INFO-#  Training loss at round 1800: 3.104537
2019-02-24 12:48:19,270-INFO-#  Training loss at round 1999: 3.084671
2019-02-24 12:48:28,528-INFO-Accuracy: 0.27156696467071034
Score / NNSARSClassifier<PRDALL.3-16.tanh.0.05>            0.3209301014946541        0.1787355641568289        -0.14219453733782522
 ========  NNClassifier
2019-02-24 12:48:56,435-INFO-#  Training loss at round 0: 7.25152
2019-02-24 13:26:45,343-INFO-#  Training loss at round 200: 3.9300418
2019-02-24 14:04:49,548-INFO-#  Training loss at round 400: 1.6065668
2019-02-24 14:43:09,411-INFO-#  Training loss at round 600: 0.71802795
2019-02-24 15:21:34,448-INFO-#  Training loss at round 800: 0.5258865
2019-02-24 15:59:58,933-INFO-#  Training loss at round 1000: 0.41189837
2019-02-24 16:38:19,716-INFO-#  Training loss at round 1200: 0.35548258
2019-02-24 17:16:44,757-INFO-#  Training loss at round 1400: 0.32019523
2019-02-24 17:55:19,847-INFO-#  Training loss at round 1600: 0.2957119
2019-02-24 18:33:54,379-INFO-#  Training loss at round 1800: 0.33148
2019-02-24 19:12:16,724-INFO-#  Training loss at round 1999: 0.28186548
2019-02-24 19:12:26,155-INFO-Accuracy: 0.9396596331857644
2019-02-24 19:12:54,300-INFO-#  Training loss at round 0: 7.250109
2019-02-24 19:51:24,677-INFO-#  Training loss at round 200: 3.9269714
2019-02-24 20:29:59,116-INFO-#  Training loss at round 400: 1.6970222
2019-02-24 21:08:23,530-INFO-#  Training loss at round 600: 1.2715769
2019-02-24 21:46:48,384-INFO-#  Training loss at round 800: 0.69861823
2019-02-24 22:25:12,448-INFO-#  Training loss at round 1000: 0.5774838
2019-02-24 23:03:28,104-INFO-#  Training loss at round 1200: 0.48872352
2019-02-24 23:41:49,096-INFO-#  Training loss at round 1400: 0.44037372
2019-02-25 00:20:10,629-INFO-#  Training loss at round 1600: 0.39609826
2019-02-25 00:58:34,010-INFO-#  Training loss at round 1800: 0.35991758
2019-02-25 01:36:44,401-INFO-#  Training loss at round 1999: 0.43229735
2019-02-25 01:36:53,954-INFO-Accuracy: 0.906542862070948
2019-02-25 01:37:21,846-INFO-#  Training loss at round 0: 7.253796
2019-02-25 02:15:42,625-INFO-#  Training loss at round 200: 2.9382055
2019-02-25 02:54:12,637-INFO-#  Training loss at round 400: 1.4273019
2019-02-25 03:32:52,804-INFO-#  Training loss at round 600: 0.7604866
2019-02-25 04:11:30,560-INFO-#  Training loss at round 800: 0.55012864
2019-02-25 04:50:11,753-INFO-#  Training loss at round 1000: 0.43274015
2019-02-25 05:28:55,274-INFO-#  Training loss at round 1200: 0.37561393
2019-02-25 06:07:31,693-INFO-#  Training loss at round 1400: 0.3300363
2019-02-25 06:46:07,137-INFO-#  Training loss at round 1600: 0.29817617
2019-02-25 07:24:38,980-INFO-#  Training loss at round 1800: 0.39673284
2019-02-25 08:02:58,637-INFO-#  Training loss at round 1999: 0.32783595
2019-02-25 08:03:08,229-INFO-Accuracy: 0.9311504628298741
2019-02-25 08:03:36,096-INFO-#  Training loss at round 0: 7.2570972
2019-02-25 08:42:04,543-INFO-#  Training loss at round 200: 4.5740514
2019-02-25 09:20:32,016-INFO-#  Training loss at round 400: 2.0557275
2019-02-25 09:58:57,939-INFO-#  Training loss at round 600: 1.0027069
2019-02-25 10:37:22,613-INFO-#  Training loss at round 800: 0.6769056
2019-02-25 11:15:49,927-INFO-#  Training loss at round 1000: 0.5758042
2019-02-25 11:54:13,407-INFO-#  Training loss at round 1200: 0.42308268
2019-02-25 12:32:39,103-INFO-#  Training loss at round 1400: 0.37036294
2019-02-25 13:11:02,154-INFO-#  Training loss at round 1600: 0.3583414
2019-02-25 13:49:25,129-INFO-#  Training loss at round 1800: 0.3202787
2019-02-25 14:27:41,737-INFO-#  Training loss at round 1999: 0.29350474
2019-02-25 14:27:51,129-INFO-Accuracy: 0.937735360910685
2019-02-25 14:28:18,505-INFO-#  Training loss at round 0: 7.2397885
2019-02-25 15:06:36,994-INFO-#  Training loss at round 200: 4.88211
2019-02-25 15:43:59,852-INFO-#  Training loss at round 400: 3.7021806
2019-02-25 16:21:05,361-INFO-#  Training loss at round 600: 1.435319
2019-02-25 16:58:41,974-INFO-#  Training loss at round 800: 0.81162846
2019-02-25 17:36:02,257-INFO-#  Training loss at round 1000: 0.64117604
2019-02-25 18:13:57,207-INFO-#  Training loss at round 1200: 0.5539076
2019-02-25 18:51:43,274-INFO-#  Training loss at round 1400: 0.48706093
2019-02-25 19:28:57,949-INFO-#  Training loss at round 1600: 0.43998736
2019-02-25 20:05:50,554-INFO-#  Training loss at round 1800: 0.4182452
2019-02-25 20:42:38,457-INFO-#  Training loss at round 1999: 0.3813906
2019-02-25 20:42:47,084-INFO-Accuracy: 0.9213786759421623
Score / NNSARSClassifier<PRDALL.3-16.tanh.0.005>           0.9272933989878867        0.17234218220094927       -0.7549512167869374
 ========  NNClassifier
2019-02-25 20:43:32,646-INFO-#  Training loss at round 0: 7.2505383
2019-02-25 21:18:33,257-INFO-#  Training loss at round 200: 4.825868
2019-02-25 21:51:04,654-INFO-#  Training loss at round 400: 4.829282
2019-02-25 22:23:30,534-INFO-#  Training loss at round 600: 4.8314834
2019-02-25 22:56:15,297-INFO-#  Training loss at round 800: 4.832795
2019-02-25 23:28:01,465-INFO-#  Training loss at round 1000: 4.8329673
2019-02-25 23:59:51,602-INFO-#  Training loss at round 1200: 4.834702
2019-02-26 00:31:50,687-INFO-#  Training loss at round 1400: 4.8373837
2019-02-26 01:04:16,629-INFO-#  Training loss at round 1600: 4.839617
2019-02-26 01:36:12,870-INFO-#  Training loss at round 1800: 4.8408737
2019-02-26 02:08:02,189-INFO-#  Training loss at round 1999: 4.8427625
2019-02-26 02:08:09,788-INFO-Accuracy: 0.054705916173173115
2019-02-26 02:08:34,689-INFO-#  Training loss at round 0: 7.2533402
2019-02-26 02:40:35,583-INFO-#  Training loss at round 200: 4.9244423
2019-02-26 03:13:18,185-INFO-#  Training loss at round 400: 4.9297223
2019-02-26 03:46:56,373-INFO-#  Training loss at round 600: 4.929221
2019-02-26 04:20:45,970-INFO-#  Training loss at round 800: 4.9332194
2019-02-26 04:55:59,342-INFO-#  Training loss at round 1000: 4.9356394
2019-02-26 05:31:21,360-INFO-#  Training loss at round 1200: 4.9411635
2019-02-26 06:06:57,111-INFO-#  Training loss at round 1400: 4.9425087
2019-02-26 06:42:30,363-INFO-#  Training loss at round 1600: 4.9440184
2019-02-26 07:17:54,790-INFO-#  Training loss at round 1800: 4.945107
2019-02-26 07:53:10,136-INFO-#  Training loss at round 1999: 4.9458647
2019-02-26 07:53:18,771-INFO-Accuracy: 0.04964640947507618
2019-02-26 07:53:44,989-INFO-#  Training loss at round 0: 7.256277
2019-02-26 08:29:09,560-INFO-#  Training loss at round 200: 4.9493403
2019-02-26 09:04:34,731-INFO-#  Training loss at round 400: 4.923302
2019-02-26 09:40:01,218-INFO-#  Training loss at round 600: 4.9253964
2019-02-26 10:15:31,173-INFO-#  Training loss at round 800: 4.926669
2019-02-26 10:51:03,780-INFO-#  Training loss at round 1000: 4.9281726
2019-02-26 11:26:25,945-INFO-#  Training loss at round 1200: 4.929648
2019-02-26 12:01:34,424-INFO-#  Training loss at round 1400: 4.934469
2019-02-26 12:36:31,835-INFO-#  Training loss at round 1600: 4.9360576
2019-02-26 13:11:27,852-INFO-#  Training loss at round 1800: 4.937477
2019-02-26 13:46:13,957-INFO-#  Training loss at round 1999: 4.9410267
2019-02-26 13:46:22,303-INFO-Accuracy: 0.04731788650606566
2019-02-26 13:46:48,497-INFO-#  Training loss at round 0: 7.247806
2019-02-26 14:21:48,011-INFO-#  Training loss at round 200: 4.898934
2019-02-26 14:57:15,636-INFO-#  Training loss at round 400: 4.9047675
2019-02-26 15:32:47,984-INFO-#  Training loss at round 600: 4.911038
2019-02-26 16:08:56,809-INFO-#  Training loss at round 800: 4.9119697
2019-02-26 16:45:25,359-INFO-#  Training loss at round 1000: 4.91356
2019-02-26 17:21:28,637-INFO-#  Training loss at round 1200: 4.9148426
2019-02-26 17:58:11,587-INFO-#  Training loss at round 1400: 4.915753
2019-02-26 18:36:24,245-INFO-#  Training loss at round 1600: 4.916448
2019-02-26 19:14:18,873-INFO-#  Training loss at round 1800: 4.9175706
2019-02-26 19:51:42,096-INFO-#  Training loss at round 1999: 4.9185605
2019-02-26 19:51:51,471-INFO-Accuracy: 0.049616235950211286
2019-02-26 19:52:26,172-INFO-#  Training loss at round 0: 7.2453527
2019-02-26 20:30:07,937-INFO-#  Training loss at round 200: 4.8815246
2019-02-26 21:07:50,896-INFO-#  Training loss at round 400: 4.882464
2019-02-26 21:45:33,740-INFO-#  Training loss at round 600: 4.8900185
2019-02-26 22:23:16,085-INFO-#  Training loss at round 800: 4.890564
2019-02-26 23:01:08,389-INFO-#  Training loss at round 1000: 4.891236
2019-02-26 23:38:10,676-INFO-#  Training loss at round 1200: 4.8928995
2019-02-27 00:15:15,613-INFO-#  Training loss at round 1400: 4.894695
2019-02-27 00:52:14,627-INFO-#  Training loss at round 1600: 4.8955755
2019-02-27 01:29:07,409-INFO-#  Training loss at round 1800: 4.8962955
2019-02-27 02:05:54,703-INFO-#  Training loss at round 1999: 4.898656
2019-02-27 02:06:03,995-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.5-16.tanh.0.5>             0.05589531532016072       0.03714145685063709       -0.018753858469523627
 ========  NNClassifier
2019-02-27 02:06:38,312-INFO-#  Training loss at round 0: 7.251558
2019-02-27 02:43:04,892-INFO-#  Training loss at round 200: 4.791636
2019-02-27 03:19:30,121-INFO-#  Training loss at round 400: 4.7935944
2019-02-27 03:56:07,472-INFO-#  Training loss at round 600: 4.79375
2019-02-27 04:32:58,882-INFO-#  Training loss at round 800: 4.79375
2019-02-27 05:10:27,535-INFO-#  Training loss at round 1000: 4.793752
2019-02-27 05:47:58,973-INFO-#  Training loss at round 1200: 4.7937417
2019-02-27 06:26:39,904-INFO-#  Training loss at round 1400: 4.7937317
2019-02-27 07:05:35,294-INFO-#  Training loss at round 1600: 4.79373
2019-02-27 07:44:30,792-INFO-#  Training loss at round 1800: 4.793749
2019-02-27 08:23:09,765-INFO-#  Training loss at round 1999: 4.793818
2019-02-27 08:23:19,465-INFO-Accuracy: 0.07684125797734721
2019-02-27 08:23:59,276-INFO-#  Training loss at round 0: 7.257652
2019-02-27 09:02:58,270-INFO-#  Training loss at round 200: 4.924099
2019-02-27 09:41:59,574-INFO-#  Training loss at round 400: 4.9240856
2019-02-27 10:21:01,999-INFO-#  Training loss at round 600: 4.9240885
2019-02-27 11:00:13,058-INFO-#  Training loss at round 800: 4.9240956
2019-02-27 11:36:45,945-INFO-#  Training loss at round 1000: 4.9240894
2019-02-27 12:12:26,810-INFO-#  Training loss at round 1200: 4.924083
2019-02-27 12:48:07,813-INFO-#  Training loss at round 1400: 4.9240623
2019-02-27 13:23:53,929-INFO-#  Training loss at round 1600: 4.924096
2019-02-27 13:59:39,140-INFO-#  Training loss at round 1800: 4.9241424
2019-02-27 14:35:14,573-INFO-#  Training loss at round 1999: 4.924188
2019-02-27 14:35:23,343-INFO-Accuracy: 0.07586385327430575
2019-02-27 14:36:01,288-INFO-#  Training loss at round 0: 7.2521586
2019-02-27 15:12:21,629-INFO-#  Training loss at round 200: 4.9239817
2019-02-27 15:48:17,100-INFO-#  Training loss at round 400: 4.9239836
2019-02-27 16:24:17,865-INFO-#  Training loss at round 600: 4.9239726
2019-02-27 17:00:45,473-INFO-#  Training loss at round 800: 4.9239917
2019-02-27 17:36:43,380-INFO-#  Training loss at round 1000: 4.9239984
2019-02-27 18:12:45,864-INFO-#  Training loss at round 1200: 4.923983
2019-02-27 18:48:47,363-INFO-#  Training loss at round 1400: 4.923994
2019-02-27 19:24:47,212-INFO-#  Training loss at round 1600: 4.924116
2019-02-27 20:01:01,575-INFO-#  Training loss at round 1800: 4.924073
2019-02-27 20:37:36,464-INFO-#  Training loss at round 1999: 4.924143
2019-02-27 20:37:45,465-INFO-Accuracy: 0.07132179612487782
2019-02-27 20:38:13,551-INFO-#  Training loss at round 0: 7.2499733
2019-02-27 21:15:48,256-INFO-#  Training loss at round 200: 4.897993
2019-02-27 21:54:37,261-INFO-#  Training loss at round 400: 4.8978233
2019-02-27 22:33:43,246-INFO-#  Training loss at round 600: 4.897611
2019-02-27 23:13:18,974-INFO-#  Training loss at round 800: 4.8975663
2019-02-27 23:52:36,944-INFO-#  Training loss at round 1000: 4.8972673
2019-02-28 00:32:11,136-INFO-#  Training loss at round 1200: 4.8972387
2019-02-28 01:11:59,323-INFO-#  Training loss at round 1400: 4.897232
2019-02-28 01:51:50,940-INFO-#  Training loss at round 1600: 4.8972297
2019-02-28 02:31:45,663-INFO-#  Training loss at round 1800: 4.8972287
2019-02-28 03:11:57,469-INFO-#  Training loss at round 1999: 4.8972178
2019-02-28 03:12:07,474-INFO-Accuracy: 0.06887630436657372
2019-02-28 03:12:36,689-INFO-#  Training loss at round 0: 7.245942
2019-02-28 03:52:36,988-INFO-#  Training loss at round 200: 4.8817983
2019-02-28 04:32:38,047-INFO-#  Training loss at round 400: 4.881644
2019-02-28 05:12:35,920-INFO-#  Training loss at round 600: 4.881415
2019-02-28 05:52:34,015-INFO-#  Training loss at round 800: 4.8812423
2019-02-28 06:32:34,084-INFO-#  Training loss at round 1000: 4.881596
2019-02-28 07:12:58,005-INFO-#  Training loss at round 1200: 4.881313
2019-02-28 07:52:57,598-INFO-#  Training loss at round 1400: 4.8811836
2019-02-28 08:32:57,186-INFO-#  Training loss at round 1600: 4.881175
2019-02-28 09:12:58,656-INFO-#  Training loss at round 1800: 4.8812065
2019-02-28 09:53:01,867-INFO-#  Training loss at round 1999: 4.8812714
2019-02-28 09:53:11,819-INFO-Accuracy: 0.07821887486704804
Score / NNSARSClassifier<PRDALL.5-16.tanh.0.05>            0.07422441732203051       0.0740517143715769        -0.0001727029504536154
 ========  NNClassifier
2019-02-28 09:53:41,524-INFO-#  Training loss at round 0: 7.244718
2019-02-28 10:33:41,681-INFO-#  Training loss at round 200: 4.826917
2019-02-28 11:13:58,078-INFO-#  Training loss at round 400: 4.825238
2019-02-28 11:54:10,393-INFO-#  Training loss at round 600: 4.8233685
2019-02-28 12:34:39,692-INFO-#  Training loss at round 800: 3.1787114
2019-02-28 13:15:33,759-INFO-#  Training loss at round 1000: 2.5107822
2019-02-28 13:56:35,938-INFO-#  Training loss at round 1200: 2.2984688
2019-02-28 14:37:29,471-INFO-#  Training loss at round 1400: 1.9836448
2019-02-28 15:18:52,210-INFO-#  Training loss at round 1600: 1.7261666
2019-02-28 15:59:51,233-INFO-#  Training loss at round 1800: 1.6316682
2019-02-28 16:40:27,505-INFO-#  Training loss at round 1999: 1.5309975
2019-02-28 16:40:38,142-INFO-Accuracy: 0.6452883343873972
2019-02-28 16:41:08,840-INFO-#  Training loss at round 0: 7.2408032
2019-02-28 17:15:42,467-INFO-#  Training loss at round 200: 4.9244604
2019-02-28 17:37:12,699-INFO-#  Training loss at round 400: 4.9227796
2019-02-28 17:57:55,067-INFO-#  Training loss at round 600: 4.418503
2019-02-28 18:18:42,253-INFO-#  Training loss at round 800: 2.7803936
2019-02-28 18:39:22,707-INFO-#  Training loss at round 1000: 2.3134177
2019-02-28 19:00:40,640-INFO-#  Training loss at round 1200: 2.1649404
2019-02-28 19:21:03,361-INFO-#  Training loss at round 1400: 2.0799615
2019-02-28 19:41:33,489-INFO-#  Training loss at round 1600: 2.0005906
2019-02-28 20:01:45,578-INFO-#  Training loss at round 1800: 1.9247324
2019-02-28 20:21:42,166-INFO-#  Training loss at round 1999: 1.8628669
2019-02-28 20:21:46,999-INFO-Accuracy: 0.4840165583855574
2019-02-28 20:23:09,835-INFO-#  Training loss at round 0: 7.2537794
2019-02-28 21:05:03,156-INFO-#  Training loss at round 200: 4.924076
2019-02-28 21:44:04,763-INFO-#  Training loss at round 400: 3.9790955
2019-02-28 22:20:56,598-INFO-#  Training loss at round 600: 2.6715086
2019-02-28 22:55:21,084-INFO-#  Training loss at round 800: 2.3724298
2019-02-28 23:23:59,565-INFO-#  Training loss at round 1000: 2.1983738
2019-02-28 23:52:41,884-INFO-#  Training loss at round 1200: 2.0256834
2019-03-01 00:21:34,290-INFO-#  Training loss at round 1400: 1.9399233
2019-03-01 00:50:41,099-INFO-#  Training loss at round 1600: 1.8514724
2019-03-01 01:20:17,664-INFO-#  Training loss at round 1800: 1.7758846
2019-03-01 01:50:42,224-INFO-#  Training loss at round 1999: 1.734794
2019-03-01 01:50:49,311-INFO-Accuracy: 0.5593629621111942
2019-03-01 01:51:25,548-INFO-#  Training loss at round 0: 7.2325664
2019-03-01 02:21:39,374-INFO-#  Training loss at round 200: 4.8989162
2019-03-01 02:52:36,712-INFO-#  Training loss at round 400: 4.8970685
2019-03-01 03:23:43,031-INFO-#  Training loss at round 600: 4.895811
2019-03-01 03:55:14,182-INFO-#  Training loss at round 800: 4.0816483
2019-03-01 04:27:33,870-INFO-#  Training loss at round 1000: 3.3550599
2019-03-01 04:59:52,310-INFO-#  Training loss at round 1200: 3.053042
2019-03-01 05:32:07,793-INFO-#  Training loss at round 1400: 2.785367
2019-03-01 06:04:25,265-INFO-#  Training loss at round 1600: 2.5192554
2019-03-01 06:36:52,465-INFO-#  Training loss at round 1800: 2.4758582
2019-03-01 07:09:34,799-INFO-#  Training loss at round 1999: 2.3601308
2019-03-01 07:09:42,423-INFO-Accuracy: 0.35179808549170666
2019-03-01 07:10:06,777-INFO-#  Training loss at round 0: 7.251199
2019-03-01 07:42:24,169-INFO-#  Training loss at round 200: 4.8821597
2019-03-01 08:14:43,130-INFO-#  Training loss at round 400: 4.446062
2019-03-01 08:47:05,736-INFO-#  Training loss at round 600: 2.539577
2019-03-01 09:19:35,070-INFO-#  Training loss at round 800: 2.1894588
2019-03-01 09:52:08,110-INFO-#  Training loss at round 1000: 2.0216057
2019-03-01 10:24:49,304-INFO-#  Training loss at round 1200: 1.9152694
2019-03-01 10:57:45,198-INFO-#  Training loss at round 1400: 1.8052512
2019-03-01 11:30:00,098-INFO-#  Training loss at round 1600: 1.7372289
2019-03-01 12:01:36,367-INFO-#  Training loss at round 1800: 1.6986006
2019-03-01 12:33:20,055-INFO-#  Training loss at round 1999: 1.6488492
2019-03-01 12:33:27,652-INFO-Accuracy: 0.5843562250265903
Score / NNSARSClassifier<PRDALL.5-16.tanh.0.005>           0.5249644330804892        0.09035685358833778       -0.43460757949215145
 ========  NNClassifier
2019-03-01 12:33:52,191-INFO-#  Training loss at round 0: 7.2462697
2019-03-01 13:08:19,084-INFO-#  Training loss at round 200: 5.945732
2019-03-01 13:43:17,188-INFO-#  Training loss at round 400: 4.952027
2019-03-01 14:18:11,737-INFO-#  Training loss at round 600: 4.8673043
2019-03-01 14:53:58,226-INFO-#  Training loss at round 800: 4.832453
2019-03-01 15:29:16,291-INFO-#  Training loss at round 1000: 4.8275666
2019-03-01 16:04:39,150-INFO-#  Training loss at round 1200: 4.830758
2019-03-01 16:39:55,827-INFO-#  Training loss at round 1400: 4.8356447
2019-03-01 17:15:04,308-INFO-#  Training loss at round 1600: 4.842681
2019-03-01 17:49:12,551-INFO-#  Training loss at round 1800: 4.850152
2019-03-01 18:21:32,509-INFO-#  Training loss at round 1999: 4.853354
2019-03-01 18:21:39,955-INFO-Accuracy: 0.06074282757431151
2019-03-01 18:22:05,497-INFO-#  Training loss at round 0: 7.239977
2019-03-01 18:55:22,256-INFO-#  Training loss at round 200: 5.492044
2019-03-01 19:27:16,526-INFO-#  Training loss at round 400: 4.9287724
2019-03-01 19:59:42,586-INFO-#  Training loss at round 600: 4.924304
2019-03-01 20:32:18,794-INFO-#  Training loss at round 800: 4.923677
2019-03-01 21:05:00,978-INFO-#  Training loss at round 1000: 4.9233136
2019-03-01 21:39:03,246-INFO-#  Training loss at round 1200: 4.9329705
2019-03-01 22:12:04,834-INFO-#  Training loss at round 1400: 4.9379764
2019-03-01 22:45:14,145-INFO-#  Training loss at round 1600: 4.946014
2019-03-01 23:18:38,671-INFO-#  Training loss at round 1800: 4.9505444
2019-03-01 23:51:06,464-INFO-#  Training loss at round 1999: 4.952604
2019-03-01 23:51:13,878-INFO-Accuracy: 0.07592134766860231
2019-03-01 23:51:38,560-INFO-#  Training loss at round 0: 7.248448
2019-03-02 00:24:16,557-INFO-#  Training loss at round 200: 6.9468236
2019-03-02 00:57:11,906-INFO-#  Training loss at round 400: 5.0036764
2019-03-02 01:30:26,927-INFO-#  Training loss at round 600: 4.938925
2019-03-02 02:03:48,299-INFO-#  Training loss at round 800: 4.9246707
2019-03-02 02:37:08,085-INFO-#  Training loss at round 1000: 4.9239774
2019-03-02 03:10:33,232-INFO-#  Training loss at round 1200: 4.9239764
2019-03-02 03:44:03,734-INFO-#  Training loss at round 1400: 4.9239464
2019-03-02 04:17:47,411-INFO-#  Training loss at round 1600: 4.9255605
2019-03-02 04:51:43,794-INFO-#  Training loss at round 1800: 4.9354014
2019-03-02 05:25:52,924-INFO-#  Training loss at round 1999: 4.9449077
2019-03-02 05:26:00,760-INFO-Accuracy: 0.04720289771747255
2019-03-02 05:26:26,039-INFO-#  Training loss at round 0: 7.2504816
2019-03-02 06:00:48,480-INFO-#  Training loss at round 200: 5.659653
2019-03-02 06:35:14,249-INFO-#  Training loss at round 400: 4.9259562
2019-03-02 07:09:44,061-INFO-#  Training loss at round 600: 4.9026775
2019-03-02 07:44:19,609-INFO-#  Training loss at round 800: 4.898057
2019-03-02 08:19:02,402-INFO-#  Training loss at round 1000: 4.8980827
2019-03-02 08:54:48,001-INFO-#  Training loss at round 1200: 4.8980494
2019-03-02 09:30:48,876-INFO-#  Training loss at round 1400: 4.9069657
2019-03-02 10:07:02,550-INFO-#  Training loss at round 1600: 4.911704
2019-03-02 10:43:18,243-INFO-#  Training loss at round 1800: 4.920952
2019-03-02 11:19:12,092-INFO-#  Training loss at round 1999: 4.9257135
2019-03-02 11:19:20,480-INFO-Accuracy: 0.04438439646994567
2019-03-02 11:19:46,926-INFO-#  Training loss at round 0: 7.2474327
2019-03-02 11:55:58,206-INFO-#  Training loss at round 200: 5.496522
2019-03-02 12:32:10,715-INFO-#  Training loss at round 400: 4.8938313
2019-03-02 13:08:25,290-INFO-#  Training loss at round 600: 4.885448
2019-03-02 13:44:50,824-INFO-#  Training loss at round 800: 4.883768
2019-03-02 14:21:58,526-INFO-#  Training loss at round 1000: 4.882413
2019-03-02 14:59:01,731-INFO-#  Training loss at round 1200: 4.8819423
2019-03-02 15:36:15,477-INFO-#  Training loss at round 1400: 4.8836803
2019-03-02 16:13:44,875-INFO-#  Training loss at round 1600: 195.07399
2019-03-02 16:50:22,653-INFO-#  Training loss at round 1800: 52.75875
2019-03-02 17:26:56,262-INFO-#  Training loss at round 1999: 9.340124
2019-03-02 17:27:04,906-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.3-32.tanh.0.5>             0.06128831958532188       0.04072887170749804       -0.020559447877823843
 ========  NNClassifier
2019-03-02 17:27:31,715-INFO-#  Training loss at round 0: 7.246643
2019-03-02 18:05:08,729-INFO-#  Training loss at round 200: 4.3596187
2019-03-02 18:42:50,670-INFO-#  Training loss at round 400: 3.9128497
2019-03-02 19:20:13,672-INFO-#  Training loss at round 600: 3.6458447
2019-03-02 19:58:08,414-INFO-#  Training loss at round 800: 3.3892634
2019-03-02 20:35:12,874-INFO-#  Training loss at round 1000: 3.3291197
2019-03-02 21:20:59,547-INFO-#  Training loss at round 1200: 3.0287452
2019-03-02 22:00:48,703-INFO-#  Training loss at round 1400: 3.001244
2019-03-02 22:24:53,148-INFO-#  Training loss at round 1600: 2.9209955
2019-03-02 22:46:23,936-INFO-#  Training loss at round 1800: 2.8656082
2019-03-02 23:07:45,073-INFO-#  Training loss at round 1999: 2.8199852
2019-03-02 23:07:50,215-INFO-Accuracy: 0.34125797734720864
2019-03-02 23:11:28,864-INFO-#  Training loss at round 0: 7.2489996
2019-03-02 23:46:44,566-INFO-#  Training loss at round 200: 2.6411905
2019-03-03 00:20:56,691-INFO-#  Training loss at round 400: 2.3521922
2019-03-03 00:54:49,358-INFO-#  Training loss at round 600: 2.2299752
2019-03-03 01:22:09,814-INFO-#  Training loss at round 800: 2.1859639
2019-03-03 01:43:34,588-INFO-#  Training loss at round 1000: 2.1341658
2019-03-03 02:05:04,892-INFO-#  Training loss at round 1200: 2.134093
2019-03-03 02:26:35,834-INFO-#  Training loss at round 1400: 2.1175911
2019-03-03 02:48:05,831-INFO-#  Training loss at round 1600: 2.0959127
2019-03-03 03:09:30,090-INFO-#  Training loss at round 1800: 2.094074
2019-03-03 03:30:51,945-INFO-#  Training loss at round 1999: 2.12816
2019-03-03 03:30:57,136-INFO-Accuracy: 0.48470649111711606
2019-03-03 03:31:57,803-INFO-#  Training loss at round 0: 7.246538
2019-03-03 04:06:24,744-INFO-#  Training loss at round 200: 3.8326144
2019-03-03 04:40:38,955-INFO-#  Training loss at round 400: 3.5618734
2019-03-03 05:14:55,823-INFO-#  Training loss at round 600: 3.3534915
2019-03-03 05:49:11,295-INFO-#  Training loss at round 800: 3.6394627
2019-03-03 06:23:30,378-INFO-#  Training loss at round 1000: 3.2626178
2019-03-03 06:57:59,322-INFO-#  Training loss at round 1200: 3.4518023
2019-03-03 07:32:33,640-INFO-#  Training loss at round 1400: 3.081192
2019-03-03 08:08:19,626-INFO-#  Training loss at round 1600: 3.0796618
2019-03-03 08:45:08,864-INFO-#  Training loss at round 1800: 3.0462182
2019-03-03 09:23:24,621-INFO-#  Training loss at round 1999: 3.595502
2019-03-03 09:23:33,989-INFO-Accuracy: 0.22739032944287932
2019-03-03 09:24:02,153-INFO-#  Training loss at round 0: 7.2397437
2019-03-03 10:02:37,234-INFO-#  Training loss at round 200: 3.4774184
2019-03-03 10:41:22,520-INFO-#  Training loss at round 400: 3.1144452
2019-03-03 11:20:11,344-INFO-#  Training loss at round 600: 2.9655094
2019-03-03 12:02:15,861-INFO-#  Training loss at round 800: 3.01282
2019-03-03 12:44:23,416-INFO-#  Training loss at round 1000: 3.0746012
2019-03-03 13:28:36,525-INFO-#  Training loss at round 1200: 2.7975194
2019-03-03 14:06:18,246-INFO-#  Training loss at round 1400: 2.8063688
2019-03-03 14:43:39,172-INFO-#  Training loss at round 1600: 2.6922863
2019-03-03 15:18:39,214-INFO-#  Training loss at round 1800: 2.6947105
2019-03-03 15:54:06,273-INFO-#  Training loss at round 1999: 2.6749346
2019-03-03 15:54:15,323-INFO-Accuracy: 0.3780722683761175
2019-03-03 15:58:14,970-INFO-#  Training loss at round 0: 7.2565002
2019-03-03 16:19:58,854-INFO-#  Training loss at round 200: 3.3379252
2019-03-03 16:41:25,903-INFO-#  Training loss at round 400: 3.5488298
2019-03-03 17:02:55,396-INFO-#  Training loss at round 600: 3.0370746
2019-03-03 17:24:23,665-INFO-#  Training loss at round 800: 2.889338
2019-03-03 17:45:51,707-INFO-#  Training loss at round 1000: 3.2397892
2019-03-03 18:07:15,429-INFO-#  Training loss at round 1200: 2.7000082
2019-03-03 18:28:42,867-INFO-#  Training loss at round 1400: 2.579075
2019-03-03 18:50:10,017-INFO-#  Training loss at round 1600: 2.5303419
2019-03-03 19:11:33,061-INFO-#  Training loss at round 1800: 2.5081935
2019-03-03 19:32:55,350-INFO-#  Training loss at round 1999: 2.4844687
2019-03-03 19:33:00,445-INFO-Accuracy: 0.41173426854859574
Score / NNSARSClassifier<PRDALL.3-32.tanh.0.05>            0.3686322669663834        0.16063785067316408       -0.20799441629321933
 ========  NNClassifier
2019-03-03 19:34:30,847-INFO-#  Training loss at round 0: 7.241107
2019-03-03 20:05:13,460-INFO-#  Training loss at round 200: 4.5648365
2019-03-03 20:35:46,953-INFO-#  Training loss at round 400: 0.6275408
2019-03-03 21:06:53,922-INFO-#  Training loss at round 600: 0.19212984
2019-03-03 21:39:05,145-INFO-#  Training loss at round 800: 0.104236424
2019-03-03 22:11:27,098-INFO-#  Training loss at round 1000: 0.06882978
2019-03-03 22:44:11,585-INFO-#  Training loss at round 1200: 0.11073864
2019-03-03 23:17:02,599-INFO-#  Training loss at round 1400: 0.07699824
2019-03-03 23:49:52,944-INFO-#  Training loss at round 1600: 0.060224537
2019-03-04 00:22:46,421-INFO-#  Training loss at round 1800: 0.04948057
2019-03-04 00:55:51,872-INFO-#  Training loss at round 1999: 0.041755527
2019-03-04 00:55:59,270-INFO-Accuracy: 0.9923819927557063
2019-03-04 00:56:23,850-INFO-#  Training loss at round 0: 7.2488885
2019-03-04 01:27:50,271-INFO-#  Training loss at round 200: 3.925038
2019-03-04 01:59:36,356-INFO-#  Training loss at round 400: 0.4658863
2019-03-04 02:31:34,189-INFO-#  Training loss at round 600: 0.15605073
2019-03-04 03:04:20,242-INFO-#  Training loss at round 800: 0.08115934
2019-03-04 03:36:35,986-INFO-#  Training loss at round 1000: 0.054746684
2019-03-04 04:09:10,460-INFO-#  Training loss at round 1200: 0.17148902
2019-03-04 04:42:00,726-INFO-#  Training loss at round 1400: 0.10062273
2019-03-04 05:14:56,972-INFO-#  Training loss at round 1600: 0.07460465
2019-03-04 05:48:00,182-INFO-#  Training loss at round 1800: 0.05950926
2019-03-04 06:20:59,136-INFO-#  Training loss at round 1999: 0.048990067
2019-03-04 06:21:06,810-INFO-Accuracy: 0.9908871385039959
2019-03-04 06:21:31,722-INFO-#  Training loss at round 0: 7.252016
2019-03-04 06:55:34,780-INFO-#  Training loss at round 200: 3.6835008
2019-03-04 07:28:51,010-INFO-#  Training loss at round 400: 0.3908825
2019-03-04 08:02:15,900-INFO-#  Training loss at round 600: 0.17470415
2019-03-04 08:35:50,998-INFO-#  Training loss at round 800: 4.281039
2019-03-04 09:09:27,672-INFO-#  Training loss at round 1000: 0.14218071
2019-03-04 09:43:40,438-INFO-#  Training loss at round 1200: 0.096880525
2019-03-04 10:18:02,359-INFO-#  Training loss at round 1400: 0.07411518
2019-03-04 10:52:32,322-INFO-#  Training loss at round 1600: 0.059577785
2019-03-04 11:27:14,103-INFO-#  Training loss at round 1800: 0.050356574
2019-03-04 12:01:44,248-INFO-#  Training loss at round 1999: 0.04120625
2019-03-04 12:01:52,442-INFO-Accuracy: 0.9924682343471511
2019-03-04 12:02:18,137-INFO-#  Training loss at round 0: 7.2515187
2019-03-04 12:37:06,755-INFO-#  Training loss at round 200: 1.0805823
2019-03-04 13:12:03,440-INFO-#  Training loss at round 400: 0.23372944
2019-03-04 13:46:11,556-INFO-#  Training loss at round 600: 0.11422682
2019-03-04 14:20:31,842-INFO-#  Training loss at round 800: 0.19693495
2019-03-04 14:52:14,958-INFO-#  Training loss at round 1000: 0.09279768
2019-03-04 15:21:44,236-INFO-#  Training loss at round 1200: 0.06456295
2019-03-04 15:51:21,640-INFO-#  Training loss at round 1400: 0.048509493
2019-03-04 16:21:08,521-INFO-#  Training loss at round 1600: 0.03810139
2019-03-04 16:51:33,437-INFO-#  Training loss at round 1800: 0.030669287
2019-03-04 17:20:54,202-INFO-#  Training loss at round 1999: 0.025272505
2019-03-04 17:21:00,913-INFO-Accuracy: 0.9959467617213327
2019-03-04 17:21:38,243-INFO-#  Training loss at round 0: 7.2311563
2019-03-04 17:52:16,469-INFO-#  Training loss at round 200: 1.4086802
2019-03-04 18:23:01,277-INFO-#  Training loss at round 400: 0.21608283
2019-03-04 18:54:51,813-INFO-#  Training loss at round 600: 0.09637255
2019-03-04 19:25:28,821-INFO-#  Training loss at round 800: 0.055952415
2019-03-04 19:56:16,884-INFO-#  Training loss at round 1000: 0.03735592
2019-03-04 20:27:19,954-INFO-#  Training loss at round 1200: 0.028326685
2019-03-04 20:58:39,142-INFO-#  Training loss at round 1400: 0.13434485
2019-03-04 21:30:28,351-INFO-#  Training loss at round 1600: 0.08590729
2019-03-04 22:02:40,125-INFO-#  Training loss at round 1800: 0.064856604
2019-03-04 22:35:06,735-INFO-#  Training loss at round 1999: 0.05221833
2019-03-04 22:35:14,189-INFO-Accuracy: 0.9903124730502774
Score / NNSARSClassifier<PRDALL.3-32.tanh.0.005>           0.9923993200756926        0.23333188151144763       -0.7590674385642451
 ========  NNClassifier
2019-03-04 22:35:39,187-INFO-#  Training loss at round 0: 7.249199
2019-03-04 23:09:22,057-INFO-#  Training loss at round 200: 6.741293
2019-03-04 23:42:18,764-INFO-#  Training loss at round 400: 4.9626603
2019-03-05 00:15:28,909-INFO-#  Training loss at round 600: 4.8812585
2019-03-05 00:48:45,704-INFO-#  Training loss at round 800: 4.8342457
2019-03-05 01:22:15,041-INFO-#  Training loss at round 1000: 4.826042
2019-03-05 01:55:55,557-INFO-#  Training loss at round 1200: 4.8257213
2019-03-05 02:29:39,475-INFO-#  Training loss at round 1400: 4.8373814
2019-03-05 03:04:14,377-INFO-#  Training loss at round 1600: 4.84946
2019-03-05 03:38:11,352-INFO-#  Training loss at round 1800: 4.8540034
2019-03-05 04:12:03,853-INFO-#  Training loss at round 1999: 4.8557553
2019-03-05 04:12:11,932-INFO-Accuracy: 0.0691370091416087
2019-03-05 04:12:38,920-INFO-#  Training loss at round 0: 7.245093
2019-03-05 04:46:31,364-INFO-#  Training loss at round 200: 5.9508257
2019-03-05 05:20:29,330-INFO-#  Training loss at round 400: 5.047648
2019-03-05 05:54:34,907-INFO-#  Training loss at round 600: 4.948857
2019-03-05 06:28:47,645-INFO-#  Training loss at round 800: 4.924078
2019-03-05 07:03:02,193-INFO-#  Training loss at round 1000: 4.9240465
2019-03-05 07:37:26,352-INFO-#  Training loss at round 1200: 4.9307942
2019-03-05 08:11:52,088-INFO-#  Training loss at round 1400: 4.9335837
2019-03-05 08:46:55,472-INFO-#  Training loss at round 1600: 4.9351974
2019-03-05 09:22:57,102-INFO-#  Training loss at round 1800: 4.937335
2019-03-05 09:57:40,681-INFO-#  Training loss at round 1999: 4.94581
2019-03-05 09:57:48,689-INFO-Accuracy: 0.04435692519979302
2019-03-05 09:58:14,712-INFO-#  Training loss at round 0: 7.246131
2019-03-05 10:33:24,885-INFO-#  Training loss at round 200: 5.783528
2019-03-05 11:08:33,555-INFO-#  Training loss at round 400: 4.932489
2019-03-05 11:44:11,274-INFO-#  Training loss at round 600: 4.9246435
2019-03-05 12:19:03,678-INFO-#  Training loss at round 800: 4.9239798
2019-03-05 12:53:49,610-INFO-#  Training loss at round 1000: 4.9239635
2019-03-05 13:28:39,029-INFO-#  Training loss at round 1200: 4.9239917
2019-03-05 14:03:35,148-INFO-#  Training loss at round 1400: 4.926583
2019-03-05 14:39:01,656-INFO-#  Training loss at round 1600: 4.929628
2019-03-05 15:15:29,509-INFO-#  Training loss at round 1800: 4.941172
2019-03-05 15:51:04,175-INFO-#  Training loss at round 1999: 4.9454865
2019-03-05 15:51:12,786-INFO-Accuracy: 0.04720289771747255
2019-03-05 15:51:39,179-INFO-#  Training loss at round 0: 7.2374115

2019-03-05 16:31:32,831-INFO-#  Training loss at round 200: 5.2168756
2019-03-05 17:18:39,231-INFO-#  Training loss at round 400: 4.8977566
2019-03-05 18:06:17,506-INFO-#  Training loss at round 600: 4.8975086
2019-03-05 18:54:26,885-INFO-#  Training loss at round 800: 4.8973985
2019-03-05 19:36:05,356-INFO-#  Training loss at round 1000: 4.8998547
2019-03-05 19:57:31,858-INFO-#  Training loss at round 1200: 4.9098663
2019-03-05 20:19:23,684-INFO-#  Training loss at round 1400: 4.911056
2019-03-05 20:40:57,073-INFO-#  Training loss at round 1600: 4.9182606
2019-03-05 21:02:27,978-INFO-#  Training loss at round 1800: 4.9242024
2019-03-05 21:24:28,782-INFO-#  Training loss at round 1999: 4.92817
2019-03-05 21:24:34,000-INFO-Accuracy: 0.04967372869175267
2019-03-05 21:27:25,532-INFO-#  Training loss at round 0: 7.2439413
2019-03-05 22:05:00,295-INFO-#  Training loss at round 200: 5.6412325
2019-03-05 22:38:23,468-INFO-#  Training loss at round 400: 4.8914895
2019-03-05 23:01:15,783-INFO-#  Training loss at round 600: 4.8814397
2019-03-05 23:22:49,242-INFO-#  Training loss at round 800: 4.881404
2019-03-05 23:44:20,568-INFO-#  Training loss at round 1000: 4.8818107
2019-03-06 00:05:59,524-INFO-#  Training loss at round 1200: 4.8822618
2019-03-06 00:27:37,872-INFO-#  Training loss at round 1400: 4.891272
2019-03-06 00:49:58,443-INFO-#  Training loss at round 1600: 4.897029
2019-03-06 01:12:21,663-INFO-#  Training loss at round 1800: 4.9015718
2019-03-06 01:34:06,304-INFO-#  Training loss at round 1999: 4.9056706
2019-03-06 01:34:11,368-INFO-Accuracy: 0.05013367062408371
Score / NNSARSClassifier<PRDALL.5-32.tanh.0.5>             0.052100846274942124      0.058068051889859564      0.00596720561491744
 ========  NNClassifier
2019-03-06 01:34:59,314-INFO-#  Training loss at round 0: 7.25171
2019-03-06 02:04:37,555-INFO-#  Training loss at round 200: 4.8257446
2019-03-06 02:34:17,343-INFO-#  Training loss at round 400: 4.8257675
2019-03-06 03:04:01,900-INFO-#  Training loss at round 600: 4.8258934
2019-03-06 03:33:45,892-INFO-#  Training loss at round 800: 4.826094
2019-03-06 04:03:22,824-INFO-#  Training loss at round 1000: 4.825767
2019-03-06 04:26:45,520-INFO-#  Training loss at round 1200: 4.825734
2019-03-06 04:50:31,966-INFO-#  Training loss at round 1400: 4.8256917
2019-03-06 05:14:31,443-INFO-#  Training loss at round 1600: 4.8257794
2019-03-06 05:38:36,554-INFO-#  Training loss at round 1800: 4.825838
2019-03-06 06:03:03,357-INFO-#  Training loss at round 1999: 4.8258524
2019-03-06 06:03:08,509-INFO-Accuracy: 0.0691370091416087
2019-03-06 06:03:28,575-INFO-#  Training loss at round 0: 7.248323
2019-03-06 06:28:17,408-INFO-#  Training loss at round 200: 4.924126
2019-03-06 06:53:34,598-INFO-#  Training loss at round 400: 4.9240766
2019-03-06 07:20:18,518-INFO-#  Training loss at round 600: 4.924076
2019-03-06 07:48:45,007-INFO-#  Training loss at round 800: 4.924076
2019-03-06 08:18:32,690-INFO-#  Training loss at round 1000: 4.92407
2019-03-06 08:48:37,666-INFO-#  Training loss at round 1200: 4.9240756
2019-03-06 09:19:11,766-INFO-#  Training loss at round 1400: 4.924075
2019-03-06 09:49:52,420-INFO-#  Training loss at round 1600: 4.9240885
2019-03-06 10:20:46,330-INFO-#  Training loss at round 1800: 4.9242287
2019-03-06 10:52:13,485-INFO-#  Training loss at round 1999: 4.9244084
2019-03-06 10:52:20,850-INFO-Accuracy: 0.07586385327430575
2019-03-06 10:52:45,051-INFO-#  Training loss at round 0: 7.2496367
2019-03-06 11:23:04,903-INFO-#  Training loss at round 200: 4.9240055
2019-03-06 11:53:32,491-INFO-#  Training loss at round 400: 4.9239817
2019-03-06 12:24:06,551-INFO-#  Training loss at round 600: 4.9241223
2019-03-06 12:55:00,493-INFO-#  Training loss at round 800: 4.9241195
2019-03-06 13:26:43,909-INFO-#  Training loss at round 1000: 4.9253855
2019-03-06 13:58:59,296-INFO-#  Training loss at round 1200: 4.9243855
2019-03-06 14:30:28,080-INFO-#  Training loss at round 1400: 4.9253483
2019-03-06 15:00:20,335-INFO-#  Training loss at round 1600: 4.924398
2019-03-06 15:30:14,135-INFO-#  Training loss at round 1800: 5.1521807
2019-03-06 16:00:32,327-INFO-#  Training loss at round 1999: 4.923985
2019-03-06 16:00:39,407-INFO-Accuracy: 0.07132179612487782
2019-03-06 16:01:03,921-INFO-#  Training loss at round 0: 7.2381587
2019-03-06 16:31:55,227-INFO-#  Training loss at round 200: 4.481215
2019-03-06 17:02:43,627-INFO-#  Training loss at round 400: 4.8986535
2019-03-06 17:32:16,232-INFO-#  Training loss at round 600: 4.8970976
2019-03-06 18:02:03,645-INFO-#  Training loss at round 800: 4.897112
2019-03-06 18:36:27,672-INFO-#  Training loss at round 1000: 4.896699
2019-03-06 19:16:44,667-INFO-#  Training loss at round 1200: 4.896034
2019-03-06 19:55:30,182-INFO-#  Training loss at round 1400: 4.895928
2019-03-06 20:34:46,263-INFO-#  Training loss at round 1600: 4.8958163
2019-03-06 21:14:35,304-INFO-#  Training loss at round 1800: 4.8956504
2019-03-06 21:50:14,640-INFO-#  Training loss at round 1999: 4.8957334
2019-03-06 21:50:20,454-INFO-Accuracy: 0.069221260815822
2019-03-06 21:52:13,004-INFO-#  Training loss at round 0: 7.259767
2019-03-06 22:38:30,904-INFO-#  Training loss at round 200: 4.9286623
2019-03-06 23:24:19,359-INFO-#  Training loss at round 400: 4.8814673
2019-03-07 00:09:56,422-INFO-#  Training loss at round 600: 4.881468
2019-03-07 00:55:41,696-INFO-#  Training loss at round 800: 4.881468
2019-03-07 01:40:55,161-INFO-#  Training loss at round 1000: 4.881468
2019-03-07 02:26:07,233-INFO-#  Training loss at round 1200: 4.881468
2019-03-07 03:11:46,113-INFO-#  Training loss at round 1400: 4.8814673
2019-03-07 03:54:41,415-INFO-#  Training loss at round 1600: 4.8815036
2019-03-07 04:37:05,392-INFO-#  Training loss at round 1800: 4.8815465
2019-03-07 05:20:05,555-INFO-#  Training loss at round 1999: 4.881519
2019-03-07 05:20:16,629-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.5-32.tanh.0.05>            0.07274680957057833       0.07264893180287507       -9.787776770325829e-05
 ========  NNClassifier
2019-03-07 05:21:43,407-INFO-#  Training loss at round 0: 7.2478433
2019-03-07 06:07:51,151-INFO-#  Training loss at round 200: 4.826042
2019-03-07 06:54:08,118-INFO-#  Training loss at round 400: 4.8251615
2019-03-07 07:40:39,910-INFO-#  Training loss at round 600: 4.824851
2019-03-07 08:27:24,242-INFO-#  Training loss at round 800: 4.824561
2019-03-07 09:14:12,430-INFO-#  Training loss at round 1000: 4.31815
2019-03-07 10:00:58,518-INFO-#  Training loss at round 1200: 2.1422205
2019-03-07 10:48:00,437-INFO-#  Training loss at round 1400: 1.6098444
2019-03-07 11:35:28,114-INFO-#  Training loss at round 1600: 1.5588219
2019-03-07 12:08:40,184-INFO-#  Training loss at round 1800: 1.3122901
2019-03-07 12:41:48,592-INFO-#  Training loss at round 1999: 1.25669
2019-03-07 12:41:56,684-INFO-Accuracy: 0.7171563272580923
2019-03-07 12:43:02,777-INFO-#  Training loss at round 0: 7.2474256
2019-03-07 13:20:55,365-INFO-#  Training loss at round 200: 4.786612
2019-03-07 13:52:18,035-INFO-#  Training loss at round 400: 2.7587144
2019-03-07 14:23:28,241-INFO-#  Training loss at round 600: 1.7514147
2019-03-07 14:50:03,064-INFO-#  Training loss at round 800: 1.088641
2019-03-07 15:15:16,825-INFO-#  Training loss at round 1000: 0.92768824
2019-03-07 15:40:13,708-INFO-#  Training loss at round 1200: 0.8159311
2019-03-07 16:05:54,278-INFO-#  Training loss at round 1400: 0.7290529
2019-03-07 16:32:07,992-INFO-#  Training loss at round 1600: 0.64523697
2019-03-07 16:59:32,837-INFO-#  Training loss at round 1800: 0.58093613
2019-03-07 17:26:22,153-INFO-#  Training loss at round 1999: 0.5914595
2019-03-07 17:26:28,351-INFO-Accuracy: 0.8682228482722935
2019-03-07 17:27:36,304-INFO-#  Training loss at round 0: 7.2419314
2019-03-07 18:06:48,189-INFO-#  Training loss at round 200: 4.923933
2019-03-07 18:46:00,199-INFO-#  Training loss at round 400: 4.4655776
2019-03-07 19:24:59,538-INFO-#  Training loss at round 600: 2.0637186
2019-03-07 20:04:01,611-INFO-#  Training loss at round 800: 1.3036896
2019-03-07 20:43:05,537-INFO-#  Training loss at round 1000: 1.0734817
2019-03-07 21:23:11,433-INFO-#  Training loss at round 1200: 0.8603192
2019-03-07 22:02:35,348-INFO-#  Training loss at round 1400: 0.7423442
2019-03-07 22:41:54,819-INFO-#  Training loss at round 1600: 0.72670525
2019-03-07 23:22:01,021-INFO-#  Training loss at round 1800: 0.6357015
2019-03-08 00:01:18,244-INFO-#  Training loss at round 1999: 0.62877953
2019-03-08 00:01:27,943-INFO-Accuracy: 0.8559190478928305
2019-03-08 00:01:57,402-INFO-#  Training loss at round 0: 7.247766
2019-03-08 00:41:32,463-INFO-#  Training loss at round 200: 3.8526762
2019-03-08 01:22:05,240-INFO-#  Training loss at round 400: 2.6939101
2019-03-08 02:02:10,491-INFO-#  Training loss at round 600: 2.112259
2019-03-08 02:42:11,384-INFO-#  Training loss at round 800: 1.8193741
2019-03-08 03:22:17,550-INFO-#  Training loss at round 1000: 1.6193427
2019-03-08 04:02:03,998-INFO-#  Training loss at round 1200: 1.4536396
2019-03-08 04:41:27,682-INFO-#  Training loss at round 1400: 1.2122616
2019-03-08 05:20:51,102-INFO-#  Training loss at round 1600: 1.0878688
2019-03-08 06:00:15,952-INFO-#  Training loss at round 1800: 0.9675955
2019-03-08 06:39:30,003-INFO-#  Training loss at round 1999: 0.8534948
2019-03-08 06:39:39,916-INFO-Accuracy: 0.8031448529623135
2019-03-08 06:40:08,567-INFO-#  Training loss at round 0: 7.2501984
2019-03-08 07:19:43,367-INFO-#  Training loss at round 200: 4.881056
2019-03-08 07:59:19,997-INFO-#  Training loss at round 400: 2.2333767
2019-03-08 08:38:58,665-INFO-#  Training loss at round 600: 1.3399351
2019-03-08 09:18:45,687-INFO-#  Training loss at round 800: 0.90667355
2019-03-08 09:53:46,077-INFO-#  Training loss at round 1000: 0.77694523
2019-03-08 10:24:42,127-INFO-#  Training loss at round 1200: 0.6476183
2019-03-08 10:55:45,973-INFO-#  Training loss at round 1400: 0.5815123
2019-03-08 11:26:50,184-INFO-#  Training loss at round 1600: 0.57827437
2019-03-08 11:57:57,642-INFO-#  Training loss at round 1800: 0.5282773
2019-03-08 12:28:56,476-INFO-#  Training loss at round 1999: 0.49620256
2019-03-08 12:29:03,594-INFO-Accuracy: 0.8907925374421479
Score / NNSARSClassifier<PRDALL.5-32.tanh.0.005>           0.8270471227655356        0.12945251558685497       -0.6975946071786806
 ========  NNClassifier
2019-03-08 12:29:50,503-INFO-#  Training loss at round 0: 7.248137
2019-03-08 13:14:57,658-INFO-#  Training loss at round 200: 4.8258076
2019-03-08 14:00:33,254-INFO-#  Training loss at round 400: 4.82576
2019-03-08 14:46:18,526-INFO-#  Training loss at round 600: 4.8257446
2019-03-08 15:32:13,746-INFO-#  Training loss at round 800: 4.825759
2019-03-08 16:18:18,855-INFO-#  Training loss at round 1000: 4.8257375
2019-03-08 17:04:31,482-INFO-#  Training loss at round 1200: 4.8257375
2019-03-08 17:50:48,253-INFO-#  Training loss at round 1400: 4.825729
2019-03-08 18:37:07,960-INFO-#  Training loss at round 1600: 4.825734
2019-03-08 19:23:34,497-INFO-#  Training loss at round 1800: 4.825735
2019-03-08 20:09:27,596-INFO-#  Training loss at round 1999: 4.825743
2019-03-08 20:09:37,405-INFO-Accuracy: 0.0691370091416087
2019-03-08 20:10:09,731-INFO-#  Training loss at round 0: 7.247457
2019-03-08 20:56:34,218-INFO-#  Training loss at round 200: 4.9237466
2019-03-08 21:46:13,809-INFO-#  Training loss at round 400: 4.9238143
2019-03-08 22:40:14,725-INFO-#  Training loss at round 600: 4.9237766
2019-03-08 23:35:19,902-INFO-#  Training loss at round 800: 4.923749
2019-03-09 00:30:52,719-INFO-#  Training loss at round 1000: 4.9237914
2019-03-09 01:27:03,659-INFO-#  Training loss at round 1200: 4.9237976
2019-03-09 02:23:21,516-INFO-#  Training loss at round 1400: 4.923782
2019-03-09 03:19:46,352-INFO-#  Training loss at round 1600: 4.9259567
2019-03-09 04:16:09,890-INFO-#  Training loss at round 1800: 4.9238167
2019-03-09 05:12:25,335-INFO-#  Training loss at round 1999: 4.9237504
2019-03-09 05:12:37,726-INFO-Accuracy: 0.07589260047145403
2019-03-09 05:13:15,432-INFO-#  Training loss at round 0: 7.251872
2019-03-09 06:10:41,807-INFO-#  Training loss at round 200: 4.924049
2019-03-09 07:08:28,462-INFO-#  Training loss at round 400: 4.9240446
2019-03-09 08:06:28,128-INFO-#  Training loss at round 600: 4.923995
2019-03-09 09:04:29,736-INFO-#  Training loss at round 800: 4.923999
2019-03-09 10:03:11,946-INFO-#  Training loss at round 1000: 4.9240203
2019-03-09 10:52:06,657-INFO-#  Training loss at round 1200: 4.924002
2019-03-09 11:37:13,799-INFO-#  Training loss at round 1400: 4.923993
2019-03-09 12:23:11,073-INFO-#  Training loss at round 1600: 4.9239964
2019-03-09 13:09:08,618-INFO-#  Training loss at round 1800: 4.924002
2019-03-09 13:49:43,743-INFO-#  Training loss at round 1999: 4.9239955
2019-03-09 13:49:53,581-INFO-Accuracy: 0.07132179612487782
2019-03-09 13:51:29,280-INFO-#  Training loss at round 0: 7.2562866
2019-03-09 14:50:26,114-INFO-#  Training loss at round 200: 4.898895
2019-03-09 15:46:19,471-INFO-#  Training loss at round 400: 4.8980575
2019-03-09 16:34:06,267-INFO-#  Training loss at round 600: 4.898044
2019-03-09 17:16:32,870-INFO-#  Training loss at round 800: 4.898044
2019-03-09 18:00:20,310-INFO-#  Training loss at round 1000: 4.8980494
2019-03-09 18:38:17,555-INFO-#  Training loss at round 1200: 4.8980303
2019-03-09 19:17:33,403-INFO-#  Training loss at round 1400: 4.8980203
2019-03-09 19:58:47,109-INFO-#  Training loss at round 1600: 4.8980255
2019-03-09 20:41:26,203-INFO-#  Training loss at round 1800: 4.8980384
2019-03-09 21:24:50,883-INFO-#  Training loss at round 1999: 4.8980427
2019-03-09 21:24:59,274-INFO-Accuracy: 0.06873257251272027
2019-03-09 21:27:06,112-INFO-#  Training loss at round 0: 7.26326
2019-03-09 22:16:45,028-INFO-#  Training loss at round 200: 4.881622
2019-03-09 23:06:34,382-INFO-#  Training loss at round 400: 4.8815017
2019-03-09 23:57:01,398-INFO-#  Training loss at round 600: 4.88147
2019-03-10 00:48:26,013-INFO-#  Training loss at round 800: 4.881482
2019-03-10 01:40:10,978-INFO-#  Training loss at round 1000: 4.8814764
2019-03-10 03:32:12,466-INFO-#  Training loss at round 1200: 4.881465
2019-03-10 04:23:52,098-INFO-#  Training loss at round 1400: 4.881448
2019-03-10 05:14:41,678-INFO-#  Training loss at round 1600: 4.8814507
2019-03-10 06:05:10,970-INFO-#  Training loss at round 1800: 4.881436
2019-03-10 06:55:30,944-INFO-#  Training loss at round 1999: 4.8814626
2019-03-10 06:55:44,273-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.3-8.relu.0.5>              0.07265482134938764       0.07264893180287507       -5.889546512566346e-06
 ========  NNClassifier
2019-03-10 06:56:44,351-INFO-#  Training loss at round 0: 7.257271
2019-03-10 07:47:33,847-INFO-#  Training loss at round 200: 2.3609302
2019-03-10 08:38:20,337-INFO-#  Training loss at round 400: 2.0705738
2019-03-10 09:30:04,022-INFO-#  Training loss at round 600: 1.7485263
2019-03-10 10:21:56,758-INFO-#  Training loss at round 800: 3.179135
2019-03-10 11:13:34,749-INFO-#  Training loss at round 1000: 1.6153141
2019-03-10 12:05:12,875-INFO-#  Training loss at round 1200: 1.885225
2019-03-10 12:56:48,251-INFO-#  Training loss at round 1400: 1.6788352
2019-03-10 13:48:26,391-INFO-#  Training loss at round 1600: 2.84509
2019-03-10 14:40:03,585-INFO-#  Training loss at round 1800: 1.7098234
2019-03-10 15:31:27,472-INFO-#  Training loss at round 1999: 1.9892337
2019-03-10 15:31:39,178-INFO-Accuracy: 0.5553958489047318
2019-03-10 15:32:30,837-INFO-#  Training loss at round 0: 7.267083
2019-03-10 16:24:53,910-INFO-#  Training loss at round 200: 2.0995188
2019-03-10 17:18:06,999-INFO-#  Training loss at round 400: 1.4575142
2019-03-10 18:11:51,191-INFO-#  Training loss at round 600: 2.3275604
2019-03-10 19:05:41,372-INFO-#  Training loss at round 800: 2.1184466
2019-03-10 19:59:52,471-INFO-#  Training loss at round 1000: 2.1431363
2019-03-10 20:54:15,873-INFO-#  Training loss at round 1200: 1.9820772
2019-03-10 21:48:37,725-INFO-#  Training loss at round 1400: 3.2236524
2019-03-10 22:42:58,119-INFO-#  Training loss at round 1600: 3.1658044
2019-03-10 23:37:15,982-INFO-#  Training loss at round 1800: 3.039976
2019-03-11 00:31:17,905-INFO-#  Training loss at round 1999: 3.0034206
2019-03-11 00:31:30,316-INFO-Accuracy: 0.4173518081987006
2019-03-11 00:32:21,010-INFO-#  Training loss at round 0: 7.2505083
2019-03-11 01:26:38,780-INFO-#  Training loss at round 200: 2.7615225
2019-03-11 02:21:00,529-INFO-#  Training loss at round 400: 2.400943
2019-03-11 03:15:19,869-INFO-#  Training loss at round 600: 3.3421366
2019-03-11 04:09:46,573-INFO-#  Training loss at round 800: 2.2807937
2019-03-11 05:03:46,040-INFO-#  Training loss at round 1000: 2.2144282
2019-03-11 05:57:43,950-INFO-#  Training loss at round 1200: 2.0298638
2019-03-11 06:51:44,455-INFO-#  Training loss at round 1400: 3.6699839
2019-03-11 07:45:48,974-INFO-#  Training loss at round 1600: 2.0391681
2019-03-11 08:39:58,955-INFO-#  Training loss at round 1800: 2.0230834
2019-03-11 09:33:54,274-INFO-#  Training loss at round 1999: 2.4198613
2019-03-11 09:34:06,482-INFO-Accuracy: 0.4972690162709136
2019-03-11 09:34:51,634-INFO-#  Training loss at round 0: 7.24825
2019-03-11 10:29:18,858-INFO-#  Training loss at round 200: 2.4997234
2019-03-11 11:23:57,557-INFO-#  Training loss at round 400: 2.2093418
2019-03-11 12:18:48,971-INFO-#  Training loss at round 600: 2.0957277
2019-03-11 13:13:36,658-INFO-#  Training loss at round 800: 2.088025
2019-03-11 14:08:42,409-INFO-#  Training loss at round 1000: 3.0263944
2019-03-11 15:03:55,465-INFO-#  Training loss at round 1200: 1.997444
2019-03-11 15:59:04,119-INFO-#  Training loss at round 1400: 2.3558798
2019-03-11 16:54:14,631-INFO-#  Training loss at round 1600: 1.9391363
2019-03-11 17:49:26,085-INFO-#  Training loss at round 1800: 2.4649975
2019-03-11 18:44:24,311-INFO-#  Training loss at round 1999: 2.4455428
2019-03-11 18:44:36,980-INFO-Accuracy: 0.5639175554086296
2019-03-11 18:45:16,559-INFO-#  Training loss at round 0: 7.252364
2019-03-11 19:40:34,944-INFO-#  Training loss at round 200: 1.4746078
2019-03-11 20:35:47,248-INFO-#  Training loss at round 400: 2.726823
2019-03-11 21:30:50,572-INFO-#  Training loss at round 600: 1.6538789
2019-03-11 22:25:28,197-INFO-#  Training loss at round 800: 1.2202592
2019-03-11 23:20:06,483-INFO-#  Training loss at round 1000: 3.2714167
2019-03-12 00:14:46,766-INFO-#  Training loss at round 1200: 3.1215992
2019-03-12 01:09:32,704-INFO-#  Training loss at round 1400: 3.0461988
2019-03-12 02:04:10,592-INFO-#  Training loss at round 1600: 3.0157702
2019-03-12 02:58:56,981-INFO-#  Training loss at round 1800: 2.9630134
2019-03-12 03:53:35,077-INFO-#  Training loss at round 1999: 2.9338639
2019-03-12 03:53:47,577-INFO-Accuracy: 0.42636617127087706
Score / NNSARSClassifier<PRDALL.3-8.relu.0.05>             0.4920600800107705        0.08998897144263707       -0.40207110856813344
 ========  NNClassifier
2019-03-12 03:54:24,466-INFO-#  Training loss at round 0: 7.249119
2019-03-12 04:49:11,031-INFO-#  Training loss at round 200: 1.0094283
2019-03-12 05:43:37,557-INFO-#  Training loss at round 400: 0.3308444
2019-03-12 06:38:06,699-INFO-#  Training loss at round 600: 0.25006026
2019-03-12 07:32:35,297-INFO-#  Training loss at round 800: 0.1934998
2019-03-12 08:27:06,502-INFO-#  Training loss at round 1000: 0.16325748
2019-03-12 09:21:39,223-INFO-#  Training loss at round 1200: 0.1419142
2019-03-12 10:16:14,179-INFO-#  Training loss at round 1400: 0.12777367
2019-03-12 11:10:46,333-INFO-#  Training loss at round 1600: 0.11771968
2019-03-12 12:05:24,069-INFO-#  Training loss at round 1800: 0.103610896
2019-03-12 12:59:46,359-INFO-#  Training loss at round 1999: 0.094923
2019-03-12 12:59:58,850-INFO-Accuracy: 0.9812568274593227
2019-03-12 13:00:35,378-INFO-#  Training loss at round 0: 7.2614007
2019-03-12 13:55:22,999-INFO-#  Training loss at round 200: 1.1371369
2019-03-12 14:50:14,923-INFO-#  Training loss at round 400: 0.3865271
2019-03-12 15:45:03,031-INFO-#  Training loss at round 600: 0.21808046
2019-03-12 16:39:50,450-INFO-#  Training loss at round 800: 0.15102154
2019-03-12 17:34:37,884-INFO-#  Training loss at round 1000: 0.096084565
2019-03-12 18:29:28,680-INFO-#  Training loss at round 1200: 0.14460182
2019-03-12 19:24:20,422-INFO-#  Training loss at round 1400: 0.115566485
2019-03-12 20:18:29,116-INFO-#  Training loss at round 1600: 0.0982371
2019-03-12 21:11:48,480-INFO-#  Training loss at round 1800: 0.08684229
2019-03-12 22:03:51,704-INFO-#  Training loss at round 1999: 0.076370895
2019-03-12 22:04:03,110-INFO-Accuracy: 0.9881561547749095
2019-03-12 22:04:37,787-INFO-#  Training loss at round 0: 7.2464576
2019-03-12 22:56:25,693-INFO-#  Training loss at round 200: 1.0483304
2019-03-12 23:48:00,056-INFO-#  Training loss at round 400: 1.4513873
2019-03-13 00:39:42,242-INFO-#  Training loss at round 600: 0.34058172
2019-03-13 01:31:34,435-INFO-#  Training loss at round 800: 0.27144998
2019-03-13 02:23:30,538-INFO-#  Training loss at round 1000: 0.22732592
2019-03-13 03:15:26,037-INFO-#  Training loss at round 1200: 0.19391166
2019-03-13 04:07:26,912-INFO-#  Training loss at round 1400: 0.17134239
2019-03-13 04:59:29,000-INFO-#  Training loss at round 1600: 0.15441738
2019-03-13 05:50:51,446-INFO-#  Training loss at round 1800: 0.14513795
2019-03-13 06:42:09,330-INFO-#  Training loss at round 1999: 0.1321417
2019-03-13 06:42:20,999-INFO-Accuracy: 0.9729776346806186
2019-03-13 06:43:08,421-INFO-#  Training loss at round 0: 7.246388
2019-03-13 07:34:28,088-INFO-#  Training loss at round 200: 0.9264934
2019-03-13 08:25:48,187-INFO-#  Training loss at round 400: 0.26366624
2019-03-13 09:17:36,821-INFO-#  Training loss at round 600: 0.13472304
2019-03-13 10:09:41,297-INFO-#  Training loss at round 800: 0.22612415
2019-03-13 11:01:45,547-INFO-#  Training loss at round 1000: 0.14045322
2019-03-13 11:53:57,234-INFO-#  Training loss at round 1200: 0.10760725
2019-03-13 12:46:12,893-INFO-#  Training loss at round 1400: 0.08656903
2019-03-13 13:38:27,972-INFO-#  Training loss at round 1600: 0.07302936
2019-03-13 14:30:38,307-INFO-#  Training loss at round 1800: 0.06475899
2019-03-13 15:22:34,915-INFO-#  Training loss at round 1999: 0.052579258
2019-03-13 15:22:46,646-INFO-Accuracy: 0.9889901399948257
2019-03-13 15:23:24,464-INFO-#  Training loss at round 0: 7.24804
2019-03-13 16:15:38,493-INFO-#  Training loss at round 200: 1.3179321
2019-03-13 17:07:51,213-INFO-#  Training loss at round 400: 0.5421619
2019-03-13 18:00:40,238-INFO-#  Training loss at round 600: 0.32451454
2019-03-13 18:53:24,170-INFO-#  Training loss at round 800: 0.2212115
2019-03-13 19:46:16,043-INFO-#  Training loss at round 1000: 0.1548196
2019-03-13 20:39:20,970-INFO-#  Training loss at round 1200: 0.12000824
2019-03-13 21:33:04,335-INFO-#  Training loss at round 1400: 0.08684672
2019-03-13 22:26:28,950-INFO-#  Training loss at round 1600: 0.15726991
2019-03-13 23:20:44,726-INFO-#  Training loss at round 1800: 0.0979044
2019-03-14 00:14:48,498-INFO-#  Training loss at round 1999: 0.08256941
2019-03-14 00:15:01,079-INFO-Accuracy: 0.9863167275131515
Score / NNSARSClassifier<PRDALL.3-8.relu.0.005>            0.9835394968845655        0.14923046299948622       -0.8343090338850794
 ========  NNClassifier
2019-03-14 00:15:38,485-INFO-#  Training loss at round 0: 7.2494874
2019-03-14 01:10:05,040-INFO-#  Training loss at round 200: 4.82579
2019-03-14 02:04:36,721-INFO-#  Training loss at round 400: 4.825764
2019-03-14 02:59:07,451-INFO-#  Training loss at round 600: 4.8257465
2019-03-14 03:53:53,231-INFO-#  Training loss at round 800: 4.8257565
2019-03-14 04:48:54,851-INFO-#  Training loss at round 1000: 4.8257327
2019-03-14 05:43:45,217-INFO-#  Training loss at round 1200: 4.8257484
2019-03-14 06:38:39,442-INFO-#  Training loss at round 1400: 4.8257284
2019-03-14 07:33:40,928-INFO-#  Training loss at round 1600: 4.825744
2019-03-14 08:28:41,234-INFO-#  Training loss at round 1800: 4.825739
2019-03-14 09:23:41,416-INFO-#  Training loss at round 1999: 4.825743
2019-03-14 09:23:54,133-INFO-Accuracy: 0.0691370091416087
2019-03-14 09:24:31,590-INFO-#  Training loss at round 0: 7.2475996
2019-03-14 10:19:47,119-INFO-#  Training loss at round 200: 4.9241323
2019-03-14 11:15:41,843-INFO-#  Training loss at round 400: 4.924118
2019-03-14 12:10:21,470-INFO-#  Training loss at round 600: 4.924071
2019-03-14 13:05:01,540-INFO-#  Training loss at round 800: 4.924072
2019-03-14 13:39:43,234-INFO-#  Training loss at round 1000: 4.9240556
2019-03-14 14:14:06,281-INFO-#  Training loss at round 1200: 4.924109
2019-03-14 14:49:29,062-INFO-#  Training loss at round 1400: 4.9240913
2019-03-14 15:34:07,325-INFO-#  Training loss at round 1600: 4.92406
2019-03-14 16:18:46,007-INFO-#  Training loss at round 1800: 4.9240913
2019-03-14 17:03:11,737-INFO-#  Training loss at round 1999: 4.9245553
2019-03-14 17:03:23,100-INFO-Accuracy: 0.07586385327430575
2019-03-14 17:04:53,841-INFO-#  Training loss at round 0: 7.246261
2019-03-14 17:55:53,680-INFO-#  Training loss at round 200: 4.9239926
2019-03-14 18:45:23,748-INFO-#  Training loss at round 400: 4.924004
2019-03-14 19:31:15,257-INFO-#  Training loss at round 600: 4.9239554
2019-03-14 20:17:45,241-INFO-#  Training loss at round 800: 4.9239902
2019-03-14 21:04:07,153-INFO-#  Training loss at round 1000: 4.9239964
2019-03-14 21:51:03,570-INFO-#  Training loss at round 1200: 4.924187
2019-03-14 22:38:36,463-INFO-#  Training loss at round 1400: 4.9239845
2019-03-14 23:27:19,821-INFO-#  Training loss at round 1600: 4.924015
2019-03-15 00:16:25,490-INFO-#  Training loss at round 1800: 4.9239755
2019-03-15 01:05:40,382-INFO-#  Training loss at round 1999: 4.924051
2019-03-15 01:05:52,230-INFO-Accuracy: 0.07132179612487782
2019-03-15 01:07:25,115-INFO-#  Training loss at round 0: 7.2470536
2019-03-15 02:06:29,841-INFO-#  Training loss at round 200: 4.89815
2019-03-15 03:05:37,680-INFO-#  Training loss at round 400: 4.8980584
2019-03-15 04:05:39,548-INFO-#  Training loss at round 600: 4.898046
2019-03-15 05:05:37,727-INFO-#  Training loss at round 800: 4.898037
2019-03-15 06:04:52,359-INFO-#  Training loss at round 1000: 4.898036
2019-03-15 07:05:22,277-INFO-#  Training loss at round 1200: 4.89802
2019-03-15 08:06:53,368-INFO-#  Training loss at round 1400: 4.8980236
2019-03-15 09:08:43,096-INFO-#  Training loss at round 1600: 4.898029
2019-03-15 10:10:47,140-INFO-#  Training loss at round 1800: 4.898032
2019-03-15 11:12:54,116-INFO-#  Training loss at round 1999: 4.8980227
2019-03-15 11:13:09,334-INFO-Accuracy: 0.06873257251272027
2019-03-15 11:14:26,905-INFO-#  Training loss at round 0: 7.2466264
2019-03-15 12:13:42,735-INFO-#  Training loss at round 200: 4.88159
2019-03-15 13:11:52,465-INFO-#  Training loss at round 400: 4.8814883
2019-03-15 14:10:10,782-INFO-#  Training loss at round 600: 4.8814907
2019-03-15 15:08:24,141-INFO-#  Training loss at round 800: 4.8814783
2019-03-15 16:06:17,598-INFO-#  Training loss at round 1000: 4.881481
2019-03-15 17:04:08,538-INFO-#  Training loss at round 1200: 4.8814893
2019-03-15 18:02:51,345-INFO-#  Training loss at round 1400: 4.881463
2019-03-15 19:01:39,720-INFO-#  Training loss at round 1600: 4.8814387
2019-03-15 20:00:27,100-INFO-#  Training loss at round 1800: 4.881475
2019-03-15 20:58:54,384-INFO-#  Training loss at round 1999: 4.881465
2019-03-15 20:59:08,023-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.5-8.relu.0.5>              0.07264907190995798       0.07264893180287507       -1.4010708290768115e-07
 ========  NNClassifier
2019-03-15 21:00:19,896-INFO-#  Training loss at round 0: 7.2508626
2019-03-15 21:59:49,948-INFO-#  Training loss at round 200: 3.567304
2019-03-15 22:59:24,661-INFO-#  Training loss at round 400: 3.381294
2019-03-15 23:59:10,984-INFO-#  Training loss at round 600: 3.6090865
2019-03-16 00:59:13,090-INFO-#  Training loss at round 800: 3.5126736
2019-03-16 01:59:49,289-INFO-#  Training loss at round 1000: 3.5803955
2019-03-16 03:01:03,521-INFO-#  Training loss at round 1200: 3.5532053
2019-03-16 04:02:57,587-INFO-#  Training loss at round 1400: 3.4181132
2019-03-16 05:05:56,797-INFO-#  Training loss at round 1600: 3.2658834
2019-03-16 06:08:53,743-INFO-#  Training loss at round 1800: 3.2863057
2019-03-16 07:11:49,437-INFO-#  Training loss at round 1999: 3.5453422
2019-03-16 07:12:04,624-INFO-Accuracy: 0.26404300580693385
2019-03-16 07:13:15,835-INFO-#  Training loss at round 0: 7.2492633
2019-03-16 08:14:29,436-INFO-#  Training loss at round 200: 3.271362
2019-03-16 09:15:54,479-INFO-#  Training loss at round 400: 3.0717418
2019-03-16 10:17:20,123-INFO-#  Training loss at round 600: 3.482244
2019-03-16 11:18:52,485-INFO-#  Training loss at round 800: 3.2305644
2019-03-16 12:20:25,371-INFO-#  Training loss at round 1000: 2.9661705
2019-03-16 13:22:08,907-INFO-#  Training loss at round 1200: 3.0219276
2019-03-16 14:24:00,007-INFO-#  Training loss at round 1400: 2.894224
2019-03-16 15:25:56,730-INFO-#  Training loss at round 1600: 2.8636255
2019-03-16 16:27:52,527-INFO-#  Training loss at round 1800: 2.7948592
2019-03-16 17:29:33,352-INFO-#  Training loss at round 1999: 3.0346897
2019-03-16 17:29:48,416-INFO-Accuracy: 0.29790720404760535
2019-03-16 17:30:50,687-INFO-#  Training loss at round 0: 7.2470703
2019-03-16 18:32:58,297-INFO-#  Training loss at round 200: 3.5021486
2019-03-16 19:35:14,755-INFO-#  Training loss at round 400: 3.6689765
2019-03-16 20:37:54,134-INFO-#  Training loss at round 600: 3.09763
2019-03-16 21:41:20,372-INFO-#  Training loss at round 800: 3.0200381
2019-03-16 22:44:49,839-INFO-#  Training loss at round 1000: 2.777711
2019-03-16 23:48:25,168-INFO-#  Training loss at round 1200: 2.6953647
2019-03-17 00:52:03,752-INFO-#  Training loss at round 1400: 2.6575024
2019-03-17 01:55:46,349-INFO-#  Training loss at round 1600: 2.7113032
2019-03-17 02:59:24,140-INFO-#  Training loss at round 1800: 2.5947914
2019-03-17 04:02:38,123-INFO-#  Training loss at round 1999: 2.6189861
2019-03-17 04:02:53,627-INFO-Accuracy: 0.41338469499223823
2019-03-17 04:03:54,917-INFO-#  Training loss at round 0: 7.2455764
2019-03-17 05:05:44,051-INFO-#  Training loss at round 200: 3.3935168
2019-03-17 06:07:34,482-INFO-#  Training loss at round 400: 3.0566063
2019-03-17 07:09:28,624-INFO-#  Training loss at round 600: 2.6890836
2019-03-17 08:11:23,794-INFO-#  Training loss at round 800: 2.5655177
2019-03-17 09:13:17,984-INFO-#  Training loss at round 1000: 2.6042345
2019-03-17 10:14:43,798-INFO-#  Training loss at round 1200: 2.4251437
2019-03-17 11:14:19,530-INFO-#  Training loss at round 1400: 2.4503171
2019-03-17 12:13:36,296-INFO-#  Training loss at round 1600: 2.45411
2019-03-17 13:13:11,868-INFO-#  Training loss at round 1800: 2.3101594
2019-03-17 14:12:41,778-INFO-#  Training loss at round 1999: 2.3205605
2019-03-17 14:12:56,059-INFO-Accuracy: 0.4191220858366631
2019-03-17 14:13:55,393-INFO-#  Training loss at round 0: 7.2521677
2019-03-17 15:13:20,108-INFO-#  Training loss at round 200: 2.8322673
2019-03-17 16:12:39,316-INFO-#  Training loss at round 400: 2.5147035
2019-03-17 17:11:42,369-INFO-#  Training loss at round 600: 2.7412653
2019-03-17 18:11:01,835-INFO-#  Training loss at round 800: 3.1088524
2019-03-17 19:07:20,480-INFO-#  Training loss at round 1000: 2.5461028
2019-03-17 20:02:12,255-INFO-#  Training loss at round 1200: 2.765005
2019-03-17 20:56:59,768-INFO-#  Training loss at round 1400: 2.9919674
2019-03-17 21:51:49,758-INFO-#  Training loss at round 1600: 2.8744993
2019-03-17 22:46:36,033-INFO-#  Training loss at round 1800: 2.5049572
2019-03-17 23:41:06,473-INFO-#  Training loss at round 1999: 2.2898266
2019-03-17 23:41:19,072-INFO-Accuracy: 0.5038088941271165
Score / NNSARSClassifier<PRDALL.5-8.relu.0.05>             0.3796531769621114        0.08184775196091157       -0.2978054250011999
 ========  NNClassifier
2019-03-17 23:42:10,233-INFO-#  Training loss at round 0: 7.2509727
2019-03-18 00:37:24,742-INFO-#  Training loss at round 200: 2.3607926
2019-03-18 01:32:52,804-INFO-#  Training loss at round 400: 1.2283521
2019-03-18 02:28:38,645-INFO-#  Training loss at round 600: 0.8926014
2019-03-18 03:24:50,376-INFO-#  Training loss at round 800: 0.75816774
2019-03-18 04:21:07,958-INFO-#  Training loss at round 1000: 1.0351034
2019-03-18 05:17:57,470-INFO-#  Training loss at round 1200: 0.57976234
2019-03-18 06:14:47,481-INFO-#  Training loss at round 1400: 0.52096146
2019-03-18 07:11:43,630-INFO-#  Training loss at round 1600: 0.4618688
2019-03-18 08:08:59,336-INFO-#  Training loss at round 1800: 0.5306427
2019-03-18 09:06:39,941-INFO-#  Training loss at round 1999: 0.48028448
2019-03-18 09:06:52,960-INFO-Accuracy: 0.9066865980566895
2019-03-18 09:07:42,672-INFO-#  Training loss at round 0: 7.255281
2019-03-18 10:05:38,227-INFO-#  Training loss at round 200: 1.753236
2019-03-18 11:03:18,665-INFO-#  Training loss at round 400: 0.65536606
2019-03-18 12:00:56,236-INFO-#  Training loss at round 600: 0.32503724
2019-03-18 12:58:31,104-INFO-#  Training loss at round 800: 3.7032845
2019-03-18 13:56:37,516-INFO-#  Training loss at round 1000: 2.784662
2019-03-18 14:55:08,961-INFO-#  Training loss at round 1200: 2.377902
2019-03-18 15:52:59,951-INFO-#  Training loss at round 1400: 2.1201222
2019-03-18 16:50:50,490-INFO-#  Training loss at round 1600: 1.9221647
2019-03-18 17:48:42,301-INFO-#  Training loss at round 1800: 1.7669277
2019-03-18 18:46:20,558-INFO-#  Training loss at round 1999: 1.6489527
2019-03-18 18:46:34,131-INFO-Accuracy: 0.6886391076870005
2019-03-18 18:47:34,531-INFO-#  Training loss at round 0: 7.24988
2019-03-18 19:45:17,333-INFO-#  Training loss at round 200: 2.2271237
2019-03-18 20:43:11,984-INFO-#  Training loss at round 400: 1.3595692
2019-03-18 21:41:10,757-INFO-#  Training loss at round 600: 1.0074872
2019-03-18 22:39:16,293-INFO-#  Training loss at round 800: 0.68510824
2019-03-18 23:37:46,153-INFO-#  Training loss at round 1000: 0.5106799
2019-03-19 00:36:03,949-INFO-#  Training loss at round 1200: 0.58123994
2019-03-19 01:34:20,176-INFO-#  Training loss at round 1400: 0.42964688
2019-03-19 02:32:37,573-INFO-#  Training loss at round 1600: 0.3542138
2019-03-19 03:31:07,756-INFO-#  Training loss at round 1800: 0.30071783
2019-03-19 04:29:17,137-INFO-#  Training loss at round 1999: 0.26756626
2019-03-19 04:29:30,845-INFO-Accuracy: 0.9491462082446961
2019-03-19 04:30:23,688-INFO-#  Training loss at round 0: 7.254319
2019-03-19 05:29:23,506-INFO-#  Training loss at round 200: 2.3641636
2019-03-19 06:28:11,248-INFO-#  Training loss at round 400: 1.2417264
2019-03-19 07:26:58,534-INFO-#  Training loss at round 600: 0.903759
2019-03-19 08:25:45,003-INFO-#  Training loss at round 800: 0.7525459
2019-03-19 09:24:58,028-INFO-#  Training loss at round 1000: 0.57135427
2019-03-19 10:24:05,138-INFO-#  Training loss at round 1200: 0.52420634
2019-03-19 11:23:30,968-INFO-#  Training loss at round 1400: 0.44130504
2019-03-19 12:22:54,230-INFO-#  Training loss at round 1600: 0.38003334
2019-03-19 13:22:10,372-INFO-#  Training loss at round 1800: 0.4714584
2019-03-19 14:21:40,037-INFO-#  Training loss at round 1999: 0.3875191
2019-03-19 14:21:53,762-INFO-Accuracy: 0.9245407767269382
2019-03-19 14:22:58,465-INFO-#  Training loss at round 0: 7.251756
2019-03-19 15:22:48,398-INFO-#  Training loss at round 200: 1.3406801
2019-03-19 16:22:04,258-INFO-#  Training loss at round 400: 0.55185
2019-03-19 17:21:40,321-INFO-#  Training loss at round 600: 0.5482232
2019-03-19 18:20:36,833-INFO-#  Training loss at round 800: 0.38630438
2019-03-19 19:19:51,516-INFO-#  Training loss at round 1000: 0.29776138
2019-03-19 20:18:55,471-INFO-#  Training loss at round 1200: 2.4973063
2019-03-19 21:18:06,759-INFO-#  Training loss at round 1400: 1.1208576
2019-03-19 22:17:18,571-INFO-#  Training loss at round 1600: 0.8964888
2019-03-19 23:16:42,343-INFO-#  Training loss at round 1800: 0.7522463
2019-03-20 00:15:41,462-INFO-#  Training loss at round 1999: 0.65021694
2019-03-20 00:15:55,451-INFO-Accuracy: 0.8546583493833904
Score / NNSARSClassifier<PRDALL.5-8.relu.0.005>            0.8647342080197429        0.10627141833954101       -0.7584627896802019
 ========  NNClassifier
2019-03-20 00:16:56,960-INFO-#  Training loss at round 0: 7.260494
2019-03-20 01:01:20,436-INFO-#  Training loss at round 200: 4.82604
2019-03-20 01:45:29,538-INFO-#  Training loss at round 400: 4.832066
2019-03-20 02:29:58,057-INFO-#  Training loss at round 600: 4.8257613
2019-03-20 03:14:37,221-INFO-#  Training loss at round 800: 4.8257265
2019-03-20 03:59:15,745-INFO-#  Training loss at round 1000: 4.8257465
2019-03-20 04:44:07,428-INFO-#  Training loss at round 1200: 4.825744
2019-03-20 05:29:14,003-INFO-#  Training loss at round 1400: 4.825681
2019-03-20 06:14:32,944-INFO-#  Training loss at round 1600: 4.825715
2019-03-20 06:59:45,561-INFO-#  Training loss at round 1800: 4.8257284
2019-03-20 07:44:42,594-INFO-#  Training loss at round 1999: 4.825721
2019-03-20 07:44:54,338-INFO-Accuracy: 0.0691370091416087
2019-03-20 07:45:54,280-INFO-#  Training loss at round 0: 7.2655606
2019-03-20 08:28:40,160-INFO-#  Training loss at round 200: 4.9241767
2019-03-20 09:11:23,209-INFO-#  Training loss at round 400: 4.924074
2019-03-20 09:54:07,043-INFO-#  Training loss at round 600: 4.924159
2019-03-20 10:37:10,363-INFO-#  Training loss at round 800: 4.9240623
2019-03-20 11:20:12,491-INFO-#  Training loss at round 1000: 4.9244094
2019-03-20 12:02:54,638-INFO-#  Training loss at round 1200: 4.924078
2019-03-20 12:45:28,773-INFO-#  Training loss at round 1400: 4.924065
2019-03-20 13:28:03,571-INFO-#  Training loss at round 1600: 4.9240956
2019-03-20 14:10:49,327-INFO-#  Training loss at round 1800: 4.924049
2019-03-20 14:53:37,466-INFO-#  Training loss at round 1999: 4.9241076
2019-03-20 14:53:48,221-INFO-Accuracy: 0.07586385327430575
2019-03-20 14:54:44,633-INFO-#  Training loss at round 0: 7.2511888
2019-03-20 15:38:01,897-INFO-#  Training loss at round 200: 4.9240336
2019-03-20 16:17:49,239-INFO-#  Training loss at round 400: 4.9240055
2019-03-20 16:56:30,132-INFO-#  Training loss at round 600: 4.9240313
2019-03-20 17:35:08,482-INFO-#  Training loss at round 800: 4.9239964
2019-03-20 18:14:24,732-INFO-#  Training loss at round 1000: 4.9240317
2019-03-20 18:53:57,484-INFO-#  Training loss at round 1200: 4.9239807
2019-03-20 19:14:10,843-INFO-#  Training loss at round 1400: 4.9239936
2019-03-20 19:35:39,677-INFO-#  Training loss at round 1600: 4.9239736
2019-03-20 19:55:45,272-INFO-#  Training loss at round 1800: 4.923982
2019-03-20 20:15:45,500-INFO-#  Training loss at round 1999: 4.924003
2019-03-20 20:15:50,677-INFO-Accuracy: 0.07132179612487782
2019-03-20 20:17:34,465-INFO-#  Training loss at round 0: 7.2740374
2019-03-20 20:50:45,663-INFO-#  Training loss at round 200: 4.899667
2019-03-20 21:24:38,301-INFO-#  Training loss at round 400: 4.90026
2019-03-20 22:00:17,766-INFO-#  Training loss at round 600: 4.896935
2019-03-20 22:36:13,780-INFO-#  Training loss at round 800: 4.896962
2019-03-20 23:12:36,383-INFO-#  Training loss at round 1000: 4.8968606
2019-03-20 23:48:56,824-INFO-#  Training loss at round 1200: 4.8968725
2019-03-21 00:25:21,617-INFO-#  Training loss at round 1400: 4.8968325
2019-03-21 01:02:00,770-INFO-#  Training loss at round 1600: 4.8967147
2019-03-21 01:38:58,340-INFO-#  Training loss at round 1800: 4.896918
2019-03-21 02:15:53,682-INFO-#  Training loss at round 1999: 4.8966713
2019-03-21 02:16:02,649-INFO-Accuracy: 0.06873257251272027
2019-03-21 02:16:31,075-INFO-#  Training loss at round 0: 7.250846
2019-03-21 02:53:45,107-INFO-#  Training loss at round 200: 4.881546
2019-03-21 03:31:06,188-INFO-#  Training loss at round 400: 4.881552
2019-03-21 04:08:39,485-INFO-#  Training loss at round 600: 4.881457
2019-03-21 04:45:56,812-INFO-#  Training loss at round 800: 4.8815284
2019-03-21 05:22:53,174-INFO-#  Training loss at round 1000: 4.8814764
2019-03-21 05:59:53,232-INFO-#  Training loss at round 1200: 4.881471
2019-03-21 06:37:01,528-INFO-#  Training loss at round 1400: 4.8815355
2019-03-21 07:14:16,083-INFO-#  Training loss at round 1600: 4.881434
2019-03-21 07:51:40,929-INFO-#  Training loss at round 1800: 4.8814754
2019-03-21 08:25:54,497-INFO-#  Training loss at round 1999: 4.8814826
2019-03-21 08:26:01,370-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.3-16.relu.0.5>             0.07264907190995798       0.07264893180287507       -1.4010708290768115e-07
 ========  NNClassifier
2019-03-21 08:26:40,518-INFO-#  Training loss at round 0: 7.2648497
2019-03-21 08:57:04,267-INFO-#  Training loss at round 200: 1.1014247
2019-03-21 09:28:58,642-INFO-#  Training loss at round 400: 2.462531
2019-03-21 10:02:15,516-INFO-#  Training loss at round 600: 2.2622876
2019-03-21 10:36:56,275-INFO-#  Training loss at round 800: 2.0768435
2019-03-21 11:11:24,027-INFO-#  Training loss at round 1000: 2.104307
2019-03-21 11:45:54,937-INFO-#  Training loss at round 1200: 2.1680293
2019-03-21 12:15:39,444-INFO-#  Training loss at round 1400: 2.5810316
2019-03-21 12:44:56,513-INFO-#  Training loss at round 1600: 2.1208568
2019-03-21 13:14:24,524-INFO-#  Training loss at round 1800: 2.1625779
2019-03-21 13:43:54,734-INFO-#  Training loss at round 1999: 2.0946705
2019-03-21 13:44:01,818-INFO-Accuracy: 0.6230092565974817
2019-03-21 13:44:26,807-INFO-#  Training loss at round 0: 7.281537
2019-03-21 14:14:04,553-INFO-#  Training loss at round 200: 1.9656258
2019-03-21 14:43:47,668-INFO-#  Training loss at round 400: 1.8219345
2019-03-21 15:14:26,596-INFO-#  Training loss at round 600: 1.618962
2019-03-21 15:44:18,549-INFO-#  Training loss at round 800: 2.862146
2019-03-21 16:14:32,921-INFO-#  Training loss at round 1000: 2.5855544
2019-03-21 16:44:54,510-INFO-#  Training loss at round 1200: 2.907693
2019-03-21 17:15:25,044-INFO-#  Training loss at round 1400: 2.6259167
2019-03-21 17:46:06,451-INFO-#  Training loss at round 1600: 2.5495539
2019-03-21 18:16:56,556-INFO-#  Training loss at round 1800: 2.5925205
2019-03-21 18:47:56,385-INFO-#  Training loss at round 1999: 2.5425844
2019-03-21 18:48:03,667-INFO-Accuracy: 0.48746622204335077
2019-03-21 18:48:27,333-INFO-#  Training loss at round 0: 7.2474384
2019-03-21 19:19:52,586-INFO-#  Training loss at round 200: 1.3742636
2019-03-21 19:51:06,351-INFO-#  Training loss at round 400: 0.938676
2019-03-21 20:22:38,362-INFO-#  Training loss at round 600: 0.8657683
2019-03-21 20:54:45,416-INFO-#  Training loss at round 800: 10.196676
2019-03-21 21:27:32,549-INFO-#  Training loss at round 1000: 1.6212766
2019-03-21 22:00:19,078-INFO-#  Training loss at round 1200: 1.446563
2019-03-21 22:33:24,778-INFO-#  Training loss at round 1400: 1.9862283
2019-03-21 23:07:56,698-INFO-#  Training loss at round 1600: 1.4299395
2019-03-21 23:42:25,960-INFO-#  Training loss at round 1800: 1.6404488
2019-03-22 00:17:03,436-INFO-#  Training loss at round 1999: 1.571668
2019-03-22 00:17:11,724-INFO-Accuracy: 0.6967745644799632
2019-03-22 00:17:37,680-INFO-#  Training loss at round 0: 7.25049
2019-03-22 00:53:04,533-INFO-#  Training loss at round 200: 1.6307267
2019-03-22 01:28:07,584-INFO-#  Training loss at round 400: 1.1022899
2019-03-22 02:03:15,296-INFO-#  Training loss at round 600: 3.9115317
2019-03-22 02:38:31,825-INFO-#  Training loss at round 800: 3.7694488
2019-03-22 03:14:28,801-INFO-#  Training loss at round 1000: 3.7113283
2019-03-22 03:49:58,783-INFO-#  Training loss at round 1200: 3.6703835
2019-03-22 04:26:00,766-INFO-#  Training loss at round 1400: 3.5671604
2019-03-22 05:01:59,155-INFO-#  Training loss at round 1600: 3.5091405
2019-03-22 05:38:00,770-INFO-#  Training loss at round 1800: 3.5435169
2019-03-22 06:13:57,005-INFO-#  Training loss at round 1999: 3.4929771
2019-03-22 06:14:06,004-INFO-Accuracy: 0.2525656135912841
2019-03-22 06:14:32,672-INFO-#  Training loss at round 0: 7.2469597
2019-03-22 06:50:46,080-INFO-#  Training loss at round 200: 1.4379776
2019-03-22 07:27:02,280-INFO-#  Training loss at round 400: 6.358556
2019-03-22 08:03:18,167-INFO-#  Training loss at round 600: 4.645953
2019-03-22 08:39:32,492-INFO-#  Training loss at round 800: 4.6224155
2019-03-22 09:15:55,981-INFO-#  Training loss at round 1000: 4.6209426
2019-03-22 09:52:31,899-INFO-#  Training loss at round 1200: 4.6091776
2019-03-22 10:29:12,972-INFO-#  Training loss at round 1400: 4.6021886
2019-03-22 11:06:00,673-INFO-#  Training loss at round 1600: 4.60141
2019-03-22 11:42:53,973-INFO-#  Training loss at round 1800: 4.687214
2019-03-22 12:19:48,159-INFO-#  Training loss at round 1999: 4.6387863
2019-03-22 12:19:57,218-INFO-Accuracy: 0.12510420559404375
Score / NNSARSClassifier<PRDALL.3-16.relu.0.05>            0.4369839724612247        0.10433824741985598       -0.33264572504136874
 ========  NNClassifier
2019-03-22 12:20:24,732-INFO-#  Training loss at round 0: 7.2456346
2019-03-22 12:57:37,529-INFO-#  Training loss at round 200: 0.10769576
2019-03-22 13:34:38,312-INFO-#  Training loss at round 400: 0.011821912
2019-03-22 14:11:37,677-INFO-#  Training loss at round 600: 0.00415843
2019-03-22 14:48:40,609-INFO-#  Training loss at round 800: 0.0021714661
2019-03-22 15:25:42,740-INFO-#  Training loss at round 1000: 0.0013722946
2019-03-22 16:02:45,399-INFO-#  Training loss at round 1200: 0.0010217221
2019-03-22 16:39:53,098-INFO-#  Training loss at round 1400: 0.0008509526
2019-03-22 17:17:42,291-INFO-#  Training loss at round 1600: 0.0007501069
2019-03-22 17:54:57,642-INFO-#  Training loss at round 1800: 0.00068552786
2019-03-22 18:32:09,159-INFO-#  Training loss at round 1999: 1.0915221
2019-03-22 18:32:18,382-INFO-Accuracy: 0.7709998275168171
2019-03-22 18:32:45,824-INFO-#  Training loss at round 0: 7.24807
2019-03-22 19:10:42,401-INFO-#  Training loss at round 200: 0.07366273
2019-03-22 19:48:15,466-INFO-#  Training loss at round 400: 0.007045792
2019-03-22 20:25:55,773-INFO-#  Training loss at round 600: 0.01948241
2019-03-22 21:03:41,175-INFO-#  Training loss at round 800: 0.0088983085
2019-03-22 21:41:41,060-INFO-#  Training loss at round 1000: 0.0056794416
2019-03-22 22:19:30,685-INFO-#  Training loss at round 1200: 0.0040984363
2019-03-22 22:57:49,417-INFO-#  Training loss at round 1400: 0.0031453955
2019-03-22 23:35:50,035-INFO-#  Training loss at round 1600: 0.0024359934
2019-03-23 00:13:52,984-INFO-#  Training loss at round 1800: 0.0019740784
2019-03-23 00:52:05,451-INFO-#  Training loss at round 1999: 0.0016870737
2019-03-23 00:52:15,153-INFO-Accuracy: 0.9998275168171104
2019-03-23 00:52:43,325-INFO-#  Training loss at round 0: 7.254879
2019-03-23 01:31:32,982-INFO-#  Training loss at round 200: 0.029053915
2019-03-23 02:10:16,672-INFO-#  Training loss at round 400: 0.0055188634
2019-03-23 02:49:05,010-INFO-#  Training loss at round 600: 0.0031235705
2019-03-23 03:27:48,646-INFO-#  Training loss at round 800: 0.0022139647
2019-03-23 04:06:33,772-INFO-#  Training loss at round 1000: 0.0017802216
2019-03-23 04:45:15,899-INFO-#  Training loss at round 1200: 0.0014544982
2019-03-23 05:23:53,697-INFO-#  Training loss at round 1400: 0.0013279334
2019-03-23 06:02:28,058-INFO-#  Training loss at round 1600: 0.0012489505
2019-03-23 06:40:29,755-INFO-#  Training loss at round 1800: 0.0011951757
2019-03-23 07:18:25,600-INFO-#  Training loss at round 1999: 0.0011554743
2019-03-23 07:18:47,847-INFO-Accuracy: 0.9995687920427758
2019-03-23 07:19:15,717-INFO-#  Training loss at round 0: 7.2740197
2019-03-23 07:57:18,269-INFO-#  Training loss at round 200: 0.04716606
2019-03-23 08:35:22,240-INFO-#  Training loss at round 400: 0.008439779
2019-03-23 09:13:30,098-INFO-#  Training loss at round 600: 0.07466514
2019-03-23 09:51:46,333-INFO-#  Training loss at round 800: 0.021076953
2019-03-23 10:30:02,005-INFO-#  Training loss at round 1000: 0.013725204
2019-03-23 11:08:25,749-INFO-#  Training loss at round 1200: 0.010830118
2019-03-23 11:46:53,320-INFO-#  Training loss at round 1400: 0.008932937
2019-03-23 12:25:29,775-INFO-#  Training loss at round 1600: 0.007244208
2019-03-23 13:04:05,069-INFO-#  Training loss at round 1800: 0.006403984
2019-03-23 13:42:35,357-INFO-#  Training loss at round 1999: 0.0057744486
2019-03-23 13:42:44,855-INFO-Accuracy: 0.998706413315319
2019-03-23 13:43:13,198-INFO-#  Training loss at round 0: 7.310776
2019-03-23 14:22:08,957-INFO-#  Training loss at round 200: 0.1743378
2019-03-23 15:01:12,429-INFO-#  Training loss at round 400: 0.056901947
2019-03-23 15:40:12,578-INFO-#  Training loss at round 600: 0.024612285
2019-03-23 16:19:13,127-INFO-#  Training loss at round 800: 0.013747814
2019-03-23 16:58:11,922-INFO-#  Training loss at round 1000: 0.008761924
2019-03-23 17:37:08,415-INFO-#  Training loss at round 1200: 0.015221168
2019-03-23 18:16:06,787-INFO-#  Training loss at round 1400: 0.00488356
2019-03-23 18:55:05,228-INFO-#  Training loss at round 1600: 0.0046260436
2019-03-23 19:34:04,125-INFO-#  Training loss at round 1800: 0.0033953493
2019-03-23 20:12:50,420-INFO-#  Training loss at round 1999: 0.0028184229
2019-03-23 20:13:00,267-INFO-Accuracy: 0.9993675798430448
Score / NNSARSClassifier<PRDALL.3-16.relu.0.005>           0.9536940259070134        0.22749025798425349       -0.7262037679227599
 ========  NNClassifier
2019-03-23 20:13:29,106-INFO-#  Training loss at round 0: 7.2498255
2019-03-23 20:52:34,596-INFO-#  Training loss at round 200: 4.825801
2019-03-23 21:31:47,645-INFO-#  Training loss at round 400: 4.8257546
2019-03-23 22:10:54,608-INFO-#  Training loss at round 600: 4.8257284
2019-03-23 22:50:01,309-INFO-#  Training loss at round 800: 4.8257337
2019-03-23 23:29:06,120-INFO-#  Training loss at round 1000: 4.8257294
2019-03-24 00:08:14,685-INFO-#  Training loss at round 1200: 4.8257327
2019-03-24 00:47:24,178-INFO-#  Training loss at round 1400: 4.825728
2019-03-24 01:26:31,928-INFO-#  Training loss at round 1600: 4.8257475
2019-03-24 02:05:38,003-INFO-#  Training loss at round 1800: 4.825724
2019-03-24 02:44:36,183-INFO-#  Training loss at round 1999: 4.825734
2019-03-24 02:44:46,352-INFO-Accuracy: 0.0691370091416087
2019-03-24 02:45:15,494-INFO-#  Training loss at round 0: 7.259849
2019-03-24 03:24:33,946-INFO-#  Training loss at round 200: 4.9241285
2019-03-24 04:03:47,042-INFO-#  Training loss at round 400: 4.924092
2019-03-24 04:42:59,074-INFO-#  Training loss at round 600: 4.9240623
2019-03-24 05:22:08,775-INFO-#  Training loss at round 800: 4.9240694
2019-03-24 06:01:13,677-INFO-#  Training loss at round 1000: 4.92407
2019-03-24 06:40:15,062-INFO-#  Training loss at round 1200: 4.924056
2019-03-24 07:19:18,858-INFO-#  Training loss at round 1400: 4.9240675
2019-03-24 07:58:17,342-INFO-#  Training loss at round 1600: 4.924052
2019-03-24 08:37:17,433-INFO-#  Training loss at round 1800: 4.924064
2019-03-24 09:16:04,618-INFO-#  Training loss at round 1999: 4.9240766
2019-03-24 09:16:14,336-INFO-Accuracy: 0.07586385327430575
2019-03-24 09:16:42,651-INFO-#  Training loss at round 0: 7.2489552
2019-03-24 09:55:41,200-INFO-#  Training loss at round 200: 4.924046
2019-03-24 10:34:44,972-INFO-#  Training loss at round 400: 4.924012
2019-03-24 11:13:53,448-INFO-#  Training loss at round 600: 4.9240003
2019-03-24 11:53:02,535-INFO-#  Training loss at round 800: 4.923985
2019-03-24 12:32:12,775-INFO-#  Training loss at round 1000: 4.9239817
2019-03-24 13:11:10,610-INFO-#  Training loss at round 1200: 4.9239907
2019-03-24 13:50:14,139-INFO-#  Training loss at round 1400: 4.924006
2019-03-24 14:29:19,868-INFO-#  Training loss at round 1600: 4.924012
2019-03-24 15:08:29,533-INFO-#  Training loss at round 1800: 4.9239864
2019-03-24 15:47:27,976-INFO-#  Training loss at round 1999: 4.923985
2019-03-24 15:47:37,802-INFO-Accuracy: 0.07132179612487782
2019-03-24 15:48:06,271-INFO-#  Training loss at round 0: 7.246912
2019-03-24 16:27:23,986-INFO-#  Training loss at round 200: 4.8981204
2019-03-24 17:06:36,737-INFO-#  Training loss at round 400: 4.8980675
2019-03-24 17:46:01,888-INFO-#  Training loss at round 600: 4.8980455
2019-03-24 18:25:28,109-INFO-#  Training loss at round 800: 4.8980317
2019-03-24 19:05:19,028-INFO-#  Training loss at round 1000: 4.898044
2019-03-24 19:44:47,600-INFO-#  Training loss at round 1200: 4.8980227
2019-03-24 20:24:15,821-INFO-#  Training loss at round 1400: 4.8980246
2019-03-24 21:03:43,063-INFO-#  Training loss at round 1600: 4.8980427
2019-03-24 21:43:13,093-INFO-#  Training loss at round 1800: 4.898029
2019-03-24 22:22:31,296-INFO-#  Training loss at round 1999: 4.898028
2019-03-24 22:22:40,836-INFO-Accuracy: 0.06873257251272027
2019-03-24 22:23:09,540-INFO-#  Training loss at round 0: 7.2508435
2019-03-24 23:02:39,933-INFO-#  Training loss at round 200: 4.882416
2019-03-24 23:42:08,924-INFO-#  Training loss at round 400: 4.8816304
2019-03-25 00:21:42,567-INFO-#  Training loss at round 600: 4.881533
2019-03-25 01:01:13,889-INFO-#  Training loss at round 800: 4.881554
2019-03-25 01:40:46,961-INFO-#  Training loss at round 1000: 4.881513
2019-03-25 02:20:19,212-INFO-#  Training loss at round 1200: 4.881441
2019-03-25 02:59:57,059-INFO-#  Training loss at round 1400: 4.881633
2019-03-25 03:39:36,232-INFO-#  Training loss at round 1600: 4.8819685
2019-03-25 04:19:20,714-INFO-#  Training loss at round 1800: 4.8829317
2019-03-25 04:58:42,273-INFO-#  Training loss at round 1999: 4.8815002
2019-03-25 04:58:51,955-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.5-16.relu.0.5>             0.07264907190995798       0.07264893180287507       -1.4010708290768115e-07
 ========  NNClassifier
2019-03-25 04:59:21,084-INFO-#  Training loss at round 0: 7.2675543
2019-03-25 05:39:04,212-INFO-#  Training loss at round 200: 2.291701
2019-03-25 06:18:52,592-INFO-#  Training loss at round 400: 2.0480838
2019-03-25 06:58:39,316-INFO-#  Training loss at round 600: 1.708032
2019-03-25 07:38:29,159-INFO-#  Training loss at round 800: 1.6364751
2019-03-25 08:18:18,450-INFO-#  Training loss at round 1000: 2.0229757
2019-03-25 08:57:57,698-INFO-#  Training loss at round 1200: 2.055186
2019-03-25 09:37:37,538-INFO-#  Training loss at round 1400: 2.5780387
2019-03-25 10:17:16,737-INFO-#  Training loss at round 1600: 1.8379867
2019-03-25 10:56:57,015-INFO-#  Training loss at round 1800: 2.0531352
2019-03-25 11:36:26,686-INFO-#  Training loss at round 1999: 1.8832737
2019-03-25 11:36:36,607-INFO-Accuracy: 0.5933996435347554
2019-03-25 11:37:05,675-INFO-#  Training loss at round 0: 7.250733
2019-03-25 12:16:43,943-INFO-#  Training loss at round 200: 1.6386211
2019-03-25 12:56:33,185-INFO-#  Training loss at round 400: 1.3542206
2019-03-25 13:33:51,904-INFO-#  Training loss at round 600: 1.4187305
2019-03-25 14:09:52,062-INFO-#  Training loss at round 800: 1.11056
2019-03-25 14:36:04,795-INFO-#  Training loss at round 1000: 1.1398926
2019-03-25 14:59:21,062-INFO-#  Training loss at round 1200: 1.1570543
2019-03-25 15:22:16,875-INFO-#  Training loss at round 1400: 4.9011436
2019-03-25 15:45:10,724-INFO-#  Training loss at round 1600: 4.8980694
2019-03-25 16:08:13,084-INFO-#  Training loss at round 1800: 4.896296
2019-03-25 16:32:03,697-INFO-#  Training loss at round 1999: 4.8945637
2019-03-25 16:32:09,144-INFO-Accuracy: 0.07928476973495084
2019-03-25 16:32:29,653-INFO-#  Training loss at round 0: 7.24623
2019-03-25 16:58:18,778-INFO-#  Training loss at round 200: 3.4754887
2019-03-25 17:24:03,033-INFO-#  Training loss at round 400: 2.327522
2019-03-25 17:50:27,658-INFO-#  Training loss at round 600: 1.8576212
2019-03-25 18:17:26,193-INFO-#  Training loss at round 800: 1.9088905
2019-03-25 18:44:32,203-INFO-#  Training loss at round 1000: 2.8834207
2019-03-25 19:11:19,725-INFO-#  Training loss at round 1200: 1.8236221
2019-03-25 19:41:43,313-INFO-#  Training loss at round 1400: 2.2371018
2019-03-25 20:12:52,211-INFO-#  Training loss at round 1600: 2.2373216
2019-03-25 20:43:13,371-INFO-#  Training loss at round 1800: 2.048792
2019-03-25 21:13:29,195-INFO-#  Training loss at round 1999: 2.0545306
2019-03-25 21:13:37,409-INFO-Accuracy: 0.5767837635830506
2019-03-25 21:15:04,116-INFO-#  Training loss at round 0: 7.2518663
2019-03-25 21:57:40,266-INFO-#  Training loss at round 200: 2.4641488
2019-03-25 22:40:12,899-INFO-#  Training loss at round 400: 2.354168
2019-03-25 23:15:43,824-INFO-#  Training loss at round 600: 1.8938291
2019-03-25 23:50:15,002-INFO-#  Training loss at round 800: 1.701866
2019-03-26 00:25:00,024-INFO-#  Training loss at round 1000: 1.9155034
2019-03-26 00:59:50,356-INFO-#  Training loss at round 1200: 2.4052505
2019-03-26 01:34:50,780-INFO-#  Training loss at round 1400: 1.7498002
2019-03-26 02:09:54,937-INFO-#  Training loss at round 1600: 2.032829
2019-03-26 02:45:15,886-INFO-#  Training loss at round 1800: 1.4385389
2019-03-26 03:20:25,980-INFO-#  Training loss at round 1999: 1.6325748
2019-03-26 03:20:34,819-INFO-Accuracy: 0.6340587000891138
2019-03-26 03:21:21,285-INFO-#  Training loss at round 0: 7.2474594
2019-03-26 03:57:37,454-INFO-#  Training loss at round 200: 1.5297053
2019-03-26 04:34:03,588-INFO-#  Training loss at round 400: 1.0798984
2019-03-26 05:10:56,819-INFO-#  Training loss at round 600: 1.7368582
2019-03-26 05:48:31,103-INFO-#  Training loss at round 800: 1.793999
2019-03-26 06:26:26,251-INFO-#  Training loss at round 1000: 1.5852821
2019-03-26 07:04:34,652-INFO-#  Training loss at round 1200: 1.4373763
2019-03-26 07:43:25,900-INFO-#  Training loss at round 1400: 1.6654794
2019-03-26 08:22:23,274-INFO-#  Training loss at round 1600: 1.8061371
2019-03-26 09:01:14,971-INFO-#  Training loss at round 1800: 1.5014678
2019-03-26 09:40:01,095-INFO-#  Training loss at round 1999: 1.5404643
2019-03-26 09:40:10,865-INFO-Accuracy: 0.6734987207865007
Score / NNSARSClassifier<PRDALL.5-16.relu.0.05>            0.5114051195456742        0.08663191496946308       -0.42477320457621115
 ========  NNClassifier
2019-03-26 09:40:40,183-INFO-#  Training loss at round 0: 7.263695
2019-03-26 10:20:05,577-INFO-#  Training loss at round 200: 1.1486163
2019-03-26 10:59:44,231-INFO-#  Training loss at round 400: 0.15394011
2019-03-26 11:39:38,987-INFO-#  Training loss at round 600: 0.9749577
2019-03-26 12:19:02,292-INFO-#  Training loss at round 800: 0.08544103
2019-03-26 12:59:02,001-INFO-#  Training loss at round 1000: 0.04348652
2019-03-26 13:37:57,632-INFO-#  Training loss at round 1200: 0.026636092
2019-03-26 14:16:46,151-INFO-#  Training loss at round 1400: 0.018285165
2019-03-26 14:56:04,334-INFO-#  Training loss at round 1600: 0.013527251
2019-03-26 15:34:47,819-INFO-#  Training loss at round 1800: 0.010514867
2019-03-26 16:13:23,296-INFO-#  Training loss at round 1999: 0.008592367
2019-03-26 16:13:32,944-INFO-Accuracy: 0.9987351233254758
2019-03-26 16:14:13,249-INFO-#  Training loss at round 0: 7.248523
2019-03-26 16:53:17,744-INFO-#  Training loss at round 200: 0.30418736
2019-03-26 17:32:06,565-INFO-#  Training loss at round 400: 1.7727933
2019-03-26 18:11:01,752-INFO-#  Training loss at round 600: 0.2686052
2019-03-26 18:50:18,685-INFO-#  Training loss at round 800: 0.1430089
2019-03-26 19:29:17,029-INFO-#  Training loss at round 1000: 0.09658228
2019-03-26 20:08:44,526-INFO-#  Training loss at round 1200: 0.070174724
2019-03-26 20:48:21,325-INFO-#  Training loss at round 1400: 0.05287023
2019-03-26 21:29:52,668-INFO-#  Training loss at round 1600: 3.640983
2019-03-26 22:15:33,676-INFO-#  Training loss at round 1800: 2.7040005
2019-03-26 22:58:37,893-INFO-#  Training loss at round 1999: 2.294385
2019-03-26 22:58:47,997-INFO-Accuracy: 0.49982751681711035
2019-03-26 22:59:21,771-INFO-#  Training loss at round 0: 7.2491484
2019-03-26 23:39:09,059-INFO-#  Training loss at round 200: 0.2787947
2019-03-27 00:18:38,080-INFO-#  Training loss at round 400: 0.03680127
2019-03-27 00:58:21,677-INFO-#  Training loss at round 600: 1.2737675
2019-03-27 01:37:34,175-INFO-#  Training loss at round 800: 0.5356723
2019-03-27 02:16:52,475-INFO-#  Training loss at round 1000: 0.3291672
2019-03-27 02:56:38,952-INFO-#  Training loss at round 1200: 0.23379663
2019-03-27 03:35:52,390-INFO-#  Training loss at round 1400: 0.18167359
2019-03-27 04:15:17,480-INFO-#  Training loss at round 1600: 0.14696309
2019-03-27 04:54:43,258-INFO-#  Training loss at round 1800: 3.3523624
2019-03-27 05:33:57,520-INFO-#  Training loss at round 1999: 2.427084
2019-03-27 05:34:07,293-INFO-Accuracy: 0.48827114356350254
2019-03-27 05:34:36,405-INFO-#  Training loss at round 0: 7.2609067
2019-03-27 06:14:08,556-INFO-#  Training loss at round 200: 0.9214171
2019-03-27 06:54:18,018-INFO-#  Training loss at round 400: 0.29303738
2019-03-27 07:33:56,512-INFO-#  Training loss at round 600: 0.10792304
2019-03-27 08:13:39,578-INFO-#  Training loss at round 800: 0.0459152
2019-03-27 08:53:24,268-INFO-#  Training loss at round 1000: 0.02465166
2019-03-27 09:33:10,544-INFO-#  Training loss at round 1200: 3.1044354
2019-03-27 10:13:19,318-INFO-#  Training loss at round 1400: 1.9259808
2019-03-27 10:53:47,662-INFO-#  Training loss at round 1600: 1.2797874
2019-03-27 11:34:44,595-INFO-#  Training loss at round 1800: 0.928977
2019-03-27 12:15:21,067-INFO-#  Training loss at round 1999: 0.70782775
2019-03-27 12:15:31,583-INFO-Accuracy: 0.8477592203984247
2019-03-27 12:16:01,272-INFO-#  Training loss at round 0: 7.2492237
2019-03-27 12:57:24,097-INFO-#  Training loss at round 200: 0.9404407
2019-03-27 13:39:13,493-INFO-#  Training loss at round 400: 0.055405032
2019-03-27 14:21:08,403-INFO-#  Training loss at round 600: 0.017635522
2019-03-27 15:03:00,158-INFO-#  Training loss at round 800: 0.01183391
2019-03-27 15:44:12,692-INFO-#  Training loss at round 1000: 0.0054502343
2019-03-27 16:25:22,007-INFO-#  Training loss at round 1200: 0.003972146
2019-03-27 17:06:41,334-INFO-#  Training loss at round 1400: 0.003106907
2019-03-27 17:49:13,887-INFO-#  Training loss at round 1600: 0.0025479111
2019-03-27 18:35:29,773-INFO-#  Training loss at round 1800: 0.0022022282
2019-03-27 19:21:47,239-INFO-#  Training loss at round 1999: 0.0019088141
2019-03-27 19:21:59,394-INFO-Accuracy: 0.9994538189553569
Score / NNSARSClassifier<PRDALL.5-16.relu.0.005>           0.766809364611974         0.14398718049208353       -0.6228221841198904
 ========  NNClassifier
2019-03-27 19:23:47,541-INFO-#  Training loss at round 0: 7.2682323
2019-03-27 20:13:08,254-INFO-#  Training loss at round 200: 4.8258204
2019-03-27 21:00:16,332-INFO-#  Training loss at round 400: 4.8257513
2019-03-27 21:46:39,852-INFO-#  Training loss at round 600: 4.8257422
2019-03-27 22:35:20,534-INFO-#  Training loss at round 800: 4.825733
2019-03-27 23:24:59,554-INFO-#  Training loss at round 1000: 4.8257313
2019-03-28 00:14:45,588-INFO-#  Training loss at round 1200: 4.8257294
2019-03-28 01:05:10,803-INFO-#  Training loss at round 1400: 4.8257413
2019-03-28 01:55:39,567-INFO-#  Training loss at round 1600: 4.825733
2019-03-28 02:46:29,663-INFO-#  Training loss at round 1800: 4.825727
2019-03-28 03:38:27,957-INFO-#  Training loss at round 1999: 4.825736
2019-03-28 03:38:41,484-INFO-Accuracy: 0.0691370091416087
2019-03-28 03:40:17,189-INFO-#  Training loss at round 0: 7.2943263
2019-03-28 04:34:42,372-INFO-#  Training loss at round 200: 4.9239297
2019-03-28 05:28:36,245-INFO-#  Training loss at round 400: 4.923873
2019-03-28 06:22:21,663-INFO-#  Training loss at round 600: 4.92386
2019-03-28 07:16:41,702-INFO-#  Training loss at round 800: 4.9238076
2019-03-28 08:11:48,534-INFO-#  Training loss at round 1000: 4.9237986
2019-03-28 09:07:36,913-INFO-#  Training loss at round 1200: 4.923778
2019-03-28 10:03:33,011-INFO-#  Training loss at round 1400: 4.9238377
2019-03-28 10:59:23,619-INFO-#  Training loss at round 1600: 4.9237795
2019-03-28 11:48:02,501-INFO-#  Training loss at round 1800: 4.924152
2019-03-28 12:36:33,828-INFO-#  Training loss at round 1999: 4.923746
2019-03-28 12:36:46,752-INFO-Accuracy: 0.07592134766860231
2019-03-28 12:38:03,904-INFO-#  Training loss at round 0: 7.2542086
2019-03-28 13:20:43,285-INFO-#  Training loss at round 200: 4.923985
2019-03-28 14:02:12,686-INFO-#  Training loss at round 400: 4.923999
2019-03-28 14:43:35,482-INFO-#  Training loss at round 600: 4.923989
2019-03-28 15:25:04,767-INFO-#  Training loss at round 800: 4.9239926
2019-03-28 16:06:01,104-INFO-#  Training loss at round 1000: 4.923982
2019-03-28 16:47:19,509-INFO-#  Training loss at round 1200: 4.9239745
2019-03-28 17:27:55,562-INFO-#  Training loss at round 1400: 4.9240212
2019-03-28 18:07:13,869-INFO-#  Training loss at round 1600: 4.9239807
2019-03-28 18:47:08,741-INFO-#  Training loss at round 1800: 4.923923
2019-03-28 19:27:15,155-INFO-#  Training loss at round 1999: 4.9239907
2019-03-28 19:27:25,214-INFO-Accuracy: 0.07132179612487782
2019-03-28 19:28:24,988-INFO-#  Training loss at round 0: 7.2602363
2019-03-28 20:11:22,617-INFO-#  Training loss at round 200: 4.898179
2019-03-28 20:54:24,738-INFO-#  Training loss at round 400: 4.8980756
2019-03-28 21:37:37,311-INFO-#  Training loss at round 600: 4.8980474
2019-03-28 22:20:37,610-INFO-#  Training loss at round 800: 4.8980427
2019-03-28 22:50:37,474-INFO-#  Training loss at round 1000: 4.898031
2019-03-28 23:13:09,540-INFO-#  Training loss at round 1200: 4.8980265
2019-03-28 23:35:15,744-INFO-#  Training loss at round 1400: 4.8980236
2019-03-28 23:57:21,027-INFO-#  Training loss at round 1600: 4.89803
2019-03-29 00:19:27,042-INFO-#  Training loss at round 1800: 4.8980336
2019-03-29 00:41:22,379-INFO-#  Training loss at round 1999: 4.8980374
2019-03-29 00:41:28,040-INFO-Accuracy: 0.06873257251272027
2019-03-29 00:43:53,533-INFO-#  Training loss at round 0: 7.2674837
2019-03-29 01:13:53,491-INFO-#  Training loss at round 200: 4.8815985
2019-03-29 01:40:45,316-INFO-#  Training loss at round 400: 4.881496
2019-03-29 02:08:59,042-INFO-#  Training loss at round 600: 4.881481
2019-03-29 02:35:10,197-INFO-#  Training loss at round 800: 4.8814783
2019-03-29 03:00:55,702-INFO-#  Training loss at round 1000: 4.8814774
2019-03-29 03:27:35,830-INFO-#  Training loss at round 1200: 4.881478
2019-03-29 03:55:08,930-INFO-#  Training loss at round 1400: 4.881454
2019-03-29 04:21:26,835-INFO-#  Training loss at round 1600: 4.881446
2019-03-29 04:46:26,424-INFO-#  Training loss at round 1800: 4.8814673
2019-03-29 05:11:41,424-INFO-#  Training loss at round 1999: 4.881491
2019-03-29 05:11:46,790-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.3-32.relu.0.5>             0.0726605707888173        0.07264893180287507       -1.163898594222501e-05
 ========  NNClassifier
2019-03-29 05:12:36,780-INFO-#  Training loss at round 0: 7.2565036
2019-03-29 05:43:14,007-INFO-#  Training loss at round 200: 1.7005771
2019-03-29 06:09:14,766-INFO-#  Training loss at round 400: 3.8424914
2019-03-29 06:32:23,993-INFO-#  Training loss at round 600: 1.2781965
2019-03-29 06:55:59,670-INFO-#  Training loss at round 800: 1.3012716
2019-03-29 07:18:07,800-INFO-#  Training loss at round 1000: 6.8824883
2019-03-29 07:40:09,375-INFO-#  Training loss at round 1200: 1.6997523
2019-03-29 08:02:17,472-INFO-#  Training loss at round 1400: 1.6826713
2019-03-29 08:24:25,754-INFO-#  Training loss at round 1600: 1.5885628
2019-03-29 08:46:36,377-INFO-#  Training loss at round 1800: 1.7836338
2019-03-29 09:08:34,044-INFO-#  Training loss at round 1999: 1.6982387
2019-03-29 09:08:39,227-INFO-Accuracy: 0.6563560052894842
2019-03-29 09:09:37,036-INFO-#  Training loss at round 0: 7.2834396
2019-03-29 09:32:14,054-INFO-#  Training loss at round 200: 3.420307
2019-03-29 09:57:44,804-INFO-#  Training loss at round 400: 2.365802
2019-03-29 10:32:12,696-INFO-#  Training loss at round 600: 1.9909202
2019-03-29 11:11:44,375-INFO-#  Training loss at round 800: 2.000206
2019-03-29 11:42:59,373-INFO-#  Training loss at round 1000: 1.9133946
2019-03-29 12:08:45,884-INFO-#  Training loss at round 1200: 2.1196928
2019-03-29 12:34:11,028-INFO-#  Training loss at round 1400: 1.849338
2019-03-29 13:00:55,482-INFO-#  Training loss at round 1600: 2.190335
2019-03-29 13:29:04,219-INFO-#  Training loss at round 1800: 2.127393
2019-03-29 13:57:18,193-INFO-#  Training loss at round 1999: 2.0504787
2019-03-29 13:57:24,461-INFO-Accuracy: 0.5635888000919911
2019-03-29 13:58:12,529-INFO-#  Training loss at round 0: 7.262323
2019-03-29 14:32:19,302-INFO-#  Training loss at round 200: 1.7107985
2019-03-29 15:04:36,976-INFO-#  Training loss at round 400: 1.2900038
2019-03-29 15:26:38,406-INFO-#  Training loss at round 600: 7.0704603
2019-03-29 15:49:18,564-INFO-#  Training loss at round 800: 2.8161008
2019-03-29 16:13:42,358-INFO-#  Training loss at round 1000: 2.5670376
2019-03-29 16:35:48,365-INFO-#  Training loss at round 1200: 2.539704
2019-03-29 16:59:13,253-INFO-#  Training loss at round 1400: 2.418881
2019-03-29 17:21:00,086-INFO-#  Training loss at round 1600: 2.4381456
2019-03-29 17:42:36,322-INFO-#  Training loss at round 1800: 2.3745904
2019-03-29 18:04:07,341-INFO-#  Training loss at round 1999: 2.3928287
2019-03-29 18:04:12,533-INFO-Accuracy: 0.47438624734088425
2019-03-29 18:05:38,713-INFO-#  Training loss at round 0: 7.28888
2019-03-29 18:35:14,062-INFO-#  Training loss at round 200: 1.3850119
2019-03-29 19:02:46,725-INFO-#  Training loss at round 400: 2.0731416
2019-03-29 19:29:20,869-INFO-#  Training loss at round 600: 1.342291
2019-03-29 19:55:08,045-INFO-#  Training loss at round 800: 1.2829591
2019-03-29 20:16:44,230-INFO-#  Training loss at round 1000: 3.8433592
2019-03-29 20:38:15,003-INFO-#  Training loss at round 1200: 3.6797833
2019-03-29 20:59:51,747-INFO-#  Training loss at round 1400: 3.5613527
2019-03-29 21:23:06,029-INFO-#  Training loss at round 1600: 3.5868075
2019-03-29 21:47:25,608-INFO-#  Training loss at round 1800: 3.4463294
2019-03-29 22:11:35,772-INFO-#  Training loss at round 1999: 3.3979888
2019-03-29 22:11:41,221-INFO-Accuracy: 0.3119556156035301
2019-03-29 22:12:21,956-INFO-#  Training loss at round 0: 7.260797
2019-03-29 22:42:43,447-INFO-#  Training loss at round 200: 1.73918
2019-03-29 23:11:44,276-INFO-#  Training loss at round 400: 1.226582
2019-03-29 23:38:10,855-INFO-#  Training loss at round 600: 1.1324605
2019-03-30 00:04:23,647-INFO-#  Training loss at round 800: 1.4439943
2019-03-30 00:29:58,349-INFO-#  Training loss at round 1000: 1.4753153
2019-03-30 00:55:34,652-INFO-#  Training loss at round 1200: 1.1671069
2019-03-30 01:22:05,449-INFO-#  Training loss at round 1400: 1.7914487
2019-03-30 01:48:44,764-INFO-#  Training loss at round 1600: 1.3844458
2019-03-30 02:15:27,320-INFO-#  Training loss at round 1800: 1.3462443
2019-03-30 02:39:39,131-INFO-#  Training loss at round 1999: 1.6033393
2019-03-30 02:39:44,594-INFO-Accuracy: 0.6753384885158249
Score / NNSARSClassifier<PRDALL.3-32.relu.0.05>            0.5363250313683429        0.10880037306269046       -0.4275246583056524
 ========  NNClassifier
2019-03-30 02:40:20,420-INFO-#  Training loss at round 0: 7.281166
2019-03-30 03:09:36,369-INFO-#  Training loss at round 200: 0.0125642205
2019-03-30 03:39:00,613-INFO-#  Training loss at round 400: 0.0037112487
2019-03-30 04:08:47,029-INFO-#  Training loss at round 600: 0.002021647
2019-03-30 04:34:33,185-INFO-#  Training loss at round 800: 0.001397166
2019-03-30 04:58:57,453-INFO-#  Training loss at round 1000: 0.0010959487
2019-03-30 05:23:19,572-INFO-#  Training loss at round 1200: 0.0009284396
2019-03-30 05:47:39,734-INFO-#  Training loss at round 1400: 0.00082422514
2019-03-30 06:11:55,392-INFO-#  Training loss at round 1600: 0.00075231434
2019-03-30 06:36:22,969-INFO-#  Training loss at round 1800: 0.0006943829
2019-03-30 07:00:43,755-INFO-#  Training loss at round 1999: 0.00064383395
2019-03-30 07:00:49,145-INFO-Accuracy: 0.9997987696199621
2019-03-30 07:01:40,201-INFO-#  Training loss at round 0: 7.266053
2019-03-30 07:29:37,949-INFO-#  Training loss at round 200: 0.0063762893
2019-03-30 08:01:03,724-INFO-#  Training loss at round 400: 0.0009761993
2019-03-30 08:29:39,139-INFO-#  Training loss at round 600: 0.00050940365
2019-03-30 08:55:36,959-INFO-#  Training loss at round 800: 0.00035482473
2019-03-30 09:20:02,314-INFO-#  Training loss at round 1000: 0.00028426453
2019-03-30 09:44:28,776-INFO-#  Training loss at round 1200: 0.00024589547
2019-03-30 10:08:49,995-INFO-#  Training loss at round 1400: 0.00022385083
2019-03-30 10:33:18,163-INFO-#  Training loss at round 1600: 0.00020771724
2019-03-30 10:57:40,975-INFO-#  Training loss at round 1800: 0.00019722547
2019-03-30 11:21:55,101-INFO-#  Training loss at round 1999: 0.00018956534
2019-03-30 11:22:00,452-INFO-Accuracy: 0.9998850112114069
2019-03-30 11:22:47,920-INFO-#  Training loss at round 0: 7.2681675
2019-03-30 11:48:35,921-INFO-#  Training loss at round 200: 0.004992354
2019-03-30 12:24:53,148-INFO-#  Training loss at round 400: 0.0016994977
2019-03-30 12:55:35,763-INFO-#  Training loss at round 600: 0.0012944584
2019-03-30 13:22:02,288-INFO-#  Training loss at round 800: 0.5481918
2019-03-30 13:46:23,942-INFO-#  Training loss at round 1000: 0.05995924
2019-03-30 14:10:39,269-INFO-#  Training loss at round 1200: 0.028244084
2019-03-30 14:35:01,902-INFO-#  Training loss at round 1400: 0.018483162
2019-03-30 14:59:22,419-INFO-#  Training loss at round 1600: 0.011999838
2019-03-30 15:23:44,587-INFO-#  Training loss at round 1800: 0.0055916915
2019-03-30 15:47:57,805-INFO-#  Training loss at round 1999: 0.0028376062
2019-03-30 15:48:03,339-INFO-Accuracy: 0.9994825504513309
2019-03-30 15:48:44,386-INFO-#  Training loss at round 0: 7.2633133
2019-03-30 16:13:02,058-INFO-#  Training loss at round 200: 0.007042369
2019-03-30 16:44:07,782-INFO-#  Training loss at round 400: 0.0022179903
2019-03-30 17:15:31,512-INFO-#  Training loss at round 600: 0.0015264057
2019-03-30 17:47:12,702-INFO-#  Training loss at round 800: 0.0012612129
2019-03-30 18:18:21,103-INFO-#  Training loss at round 1000: 0.0011377693
2019-03-30 18:46:53,027-INFO-#  Training loss at round 1200: 0.0010725731
2019-03-30 19:16:27,028-INFO-#  Training loss at round 1400: 0.001035302
2019-03-30 19:40:50,989-INFO-#  Training loss at round 1600: 0.0010117089
2019-03-30 20:05:16,332-INFO-#  Training loss at round 1800: 0.0009974339
2019-03-30 20:29:36,264-INFO-#  Training loss at round 1999: 0.0009858421
2019-03-30 20:29:41,751-INFO-Accuracy: 0.9995688044384397
2019-03-30 20:30:39,838-INFO-#  Training loss at round 0: 7.265011
2019-03-30 20:55:01,312-INFO-#  Training loss at round 200: 0.004783671
2019-03-30 21:19:22,314-INFO-#  Training loss at round 400: 0.0015896303
2019-03-30 21:43:40,182-INFO-#  Training loss at round 600: 0.001183013
2019-03-30 22:08:03,330-INFO-#  Training loss at round 800: 0.6729486
2019-03-30 22:32:24,867-INFO-#  Training loss at round 1000: 0.061616983
2019-03-30 22:56:46,849-INFO-#  Training loss at round 1200: 0.022963949
2019-03-30 23:21:06,193-INFO-#  Training loss at round 1400: 0.01436671
2019-03-30 23:45:28,357-INFO-#  Training loss at round 1600: 0.010829737
2019-03-31 00:09:46,165-INFO-#  Training loss at round 1800: 0.00883242
2019-03-31 00:34:00,868-INFO-#  Training loss at round 1999: 0.0074552135
2019-03-31 00:34:06,097-INFO-Accuracy: 0.9981602322706759
Score / NNSARSClassifier<PRDALL.3-32.relu.0.005>           0.999379073598363         0.28183346116770486       -0.7175456124306581
 ========  NNClassifier
2019-03-31 00:34:26,747-INFO-#  Training loss at round 0: 7.2591896
2019-03-31 00:58:55,464-INFO-#  Training loss at round 200: 4.8262773
2019-03-31 01:23:21,611-INFO-#  Training loss at round 400: 4.8292193
2019-03-31 01:47:47,707-INFO-#  Training loss at round 600: 4.8308687
2019-03-31 02:12:10,315-INFO-#  Training loss at round 800: 4.825727
2019-03-31 02:36:36,163-INFO-#  Training loss at round 1000: 4.8257146
2019-03-31 03:01:03,589-INFO-#  Training loss at round 1200: 4.825738
2019-03-31 03:25:30,947-INFO-#  Training loss at round 1400: 4.8257117
2019-03-31 03:49:56,628-INFO-#  Training loss at round 1600: 4.826941
2019-03-31 04:14:27,012-INFO-#  Training loss at round 1800: 4.82574
2019-03-31 04:39:47,342-INFO-#  Training loss at round 1999: 4.825754
2019-03-31 04:39:52,583-INFO-Accuracy: 0.0691370091416087
2019-03-31 04:40:14,472-INFO-#  Training loss at round 0: 7.2521505
2019-03-31 05:06:17,295-INFO-#  Training loss at round 200: 4.9246197
2019-03-31 05:33:26,910-INFO-#  Training loss at round 400: 4.924235
2019-03-31 06:00:48,576-INFO-#  Training loss at round 600: 4.9241533
2019-03-31 06:28:03,527-INFO-#  Training loss at round 800: 4.924125
2019-03-31 06:54:26,579-INFO-#  Training loss at round 1000: 4.9241138
2019-03-31 07:18:53,512-INFO-#  Training loss at round 1200: 4.9241047
2019-03-31 07:43:17,090-INFO-#  Training loss at round 1400: 4.9240985
2019-03-31 08:07:47,234-INFO-#  Training loss at round 1600: 4.92409
2019-03-31 08:32:15,291-INFO-#  Training loss at round 1800: 4.924081
2019-03-31 08:56:37,643-INFO-#  Training loss at round 1999: 4.9240837
2019-03-31 08:56:43,060-INFO-Accuracy: 0.07586385327430575
2019-03-31 08:57:15,316-INFO-#  Training loss at round 0: 7.2512774
2019-03-31 09:22:16,498-INFO-#  Training loss at round 200: 4.9240575
2019-03-31 09:48:40,564-INFO-#  Training loss at round 400: 4.9240417
2019-03-31 10:14:57,257-INFO-#  Training loss at round 600: 4.924027
2019-03-31 10:41:12,345-INFO-#  Training loss at round 800: 4.9239907
2019-03-31 11:16:34,268-INFO-#  Training loss at round 1000: 4.924027
2019-03-31 11:40:57,601-INFO-#  Training loss at round 1200: 4.9239836
2019-03-31 12:05:25,056-INFO-#  Training loss at round 1400: 4.923991
2019-03-31 12:29:50,481-INFO-#  Training loss at round 1600: 4.9240274
2019-03-31 12:54:18,645-INFO-#  Training loss at round 1800: 4.923985
2019-03-31 13:18:36,377-INFO-#  Training loss at round 1999: 4.924008
2019-03-31 13:18:41,716-INFO-Accuracy: 0.07132179612487782
2019-03-31 13:19:19,001-INFO-#  Training loss at round 0: 7.2551336
2019-03-31 13:43:44,883-INFO-#  Training loss at round 200: 4.8981285
2019-03-31 14:12:20,928-INFO-#  Training loss at round 400: 4.8980565
2019-03-31 14:39:09,532-INFO-#  Training loss at round 600: 4.8980494
2019-03-31 15:04:12,535-INFO-#  Training loss at round 800: 4.898036
2019-03-31 15:28:38,219-INFO-#  Training loss at round 1000: 4.898039
2019-03-31 15:53:06,418-INFO-#  Training loss at round 1200: 4.8980293
2019-03-31 16:17:32,273-INFO-#  Training loss at round 1400: 4.8980336
2019-03-31 16:41:54,268-INFO-#  Training loss at round 1600: 4.8980417
2019-03-31 17:06:21,332-INFO-#  Training loss at round 1800: 4.89804
2019-03-31 17:30:39,583-INFO-#  Training loss at round 1999: 4.8980174
2019-03-31 17:30:44,984-INFO-Accuracy: 0.06873257251272027
2019-03-31 17:31:17,250-INFO-#  Training loss at round 0: 7.254506
2019-03-31 17:56:19,544-INFO-#  Training loss at round 200: 4.881471
2019-03-31 18:21:55,259-INFO-#  Training loss at round 400: 4.8814387
2019-03-31 18:47:11,828-INFO-#  Training loss at round 600: 4.887016
2019-03-31 19:13:46,319-INFO-#  Training loss at round 800: 4.8814964
2019-03-31 19:39:03,121-INFO-#  Training loss at round 1000: 4.8814507
2019-03-31 20:05:35,454-INFO-#  Training loss at round 1200: 4.881513
2019-03-31 20:31:02,075-INFO-#  Training loss at round 1400: 4.8863707
2019-03-31 20:55:31,336-INFO-#  Training loss at round 1600: 4.881537
2019-03-31 21:19:58,178-INFO-#  Training loss at round 1800: 4.881593
2019-03-31 21:44:12,725-INFO-#  Training loss at round 1999: 4.8814874
2019-03-31 21:44:18,317-INFO-Accuracy: 0.07819012849627735
Score / NNSARSClassifier<PRDALL.5-32.relu.0.5>             0.07264907190995798       0.07264893180287507       -1.4010708290768115e-07
 ========  NNClassifier
2019-03-31 21:44:51,639-INFO-#  Training loss at round 0: 7.259586
2019-03-31 22:09:17,074-INFO-#  Training loss at round 200: 2.1153085
2019-03-31 22:34:28,841-INFO-#  Training loss at round 400: 1.5480107
2019-03-31 22:59:53,652-INFO-#  Training loss at round 600: 2.1929605
2019-03-31 23:25:05,705-INFO-#  Training loss at round 800: 2.0201066
2019-03-31 23:49:37,300-INFO-#  Training loss at round 1000: 2.2708468
2019-04-01 00:14:04,049-INFO-#  Training loss at round 1200: 1.7082906
2019-04-01 00:38:34,182-INFO-#  Training loss at round 1400: 1.9427805
2019-04-01 01:04:00,686-INFO-#  Training loss at round 1600: 1.5305871
2019-04-01 01:32:33,960-INFO-#  Training loss at round 1800: 1.5242448
2019-04-01 01:59:40,064-INFO-#  Training loss at round 1999: 2.041508
2019-04-01 01:59:45,617-INFO-Accuracy: 0.4914908296441097
2019-04-01 02:00:26,192-INFO-#  Training loss at round 0: 7.251981
2019-04-01 02:25:05,309-INFO-#  Training loss at round 200: 1.4695524
2019-04-01 02:50:04,897-INFO-#  Training loss at round 400: 4.1708736
2019-04-01 03:15:07,578-INFO-#  Training loss at round 600: 3.83915
2019-04-01 03:39:34,804-INFO-#  Training loss at round 800: 3.4732583
2019-04-01 04:04:10,233-INFO-#  Training loss at round 1000: 3.210977
2019-04-01 04:28:50,613-INFO-#  Training loss at round 1200: 3.6550634
2019-04-01 04:54:15,128-INFO-#  Training loss at round 1400: 4.1546736
2019-04-01 05:25:14,964-INFO-#  Training loss at round 1600: 4.041045
2019-04-01 05:49:47,168-INFO-#  Training loss at round 1800: 4.036652
2019-04-01 06:14:07,293-INFO-#  Training loss at round 1999: 4.012569
2019-04-01 06:14:12,777-INFO-Accuracy: 0.20065543609498074
2019-04-01 06:14:46,044-INFO-#  Training loss at round 0: 7.2483335
2019-04-01 06:39:20,916-INFO-#  Training loss at round 200: 2.5142138
2019-04-01 07:03:57,043-INFO-#  Training loss at round 400: 1.6186383
2019-04-01 07:29:02,078-INFO-#  Training loss at round 600: 1.639901
2019-04-01 07:54:29,460-INFO-#  Training loss at round 800: 1.5453056
2019-04-01 08:19:08,214-INFO-#  Training loss at round 1000: 1.7427142
2019-04-01 08:44:17,959-INFO-#  Training loss at round 1200: 1.5930387
2019-04-01 09:08:56,361-INFO-#  Training loss at round 1400: 2.2667122
2019-04-01 09:33:23,619-INFO-#  Training loss at round 1600: 1.9534899
2019-04-01 09:55:55,572-INFO-#  Training loss at round 1800: 1.9023669
2019-04-01 10:18:22,439-INFO-#  Training loss at round 1999: 1.8709643
2019-04-01 10:18:27,500-INFO-Accuracy: 0.6333007531765653
2019-04-01 10:19:06,568-INFO-#  Training loss at round 0: 7.2532835
2019-04-01 10:41:23,720-INFO-#  Training loss at round 200: 2.6085658
2019-04-01 11:04:50,900-INFO-#  Training loss at round 400: 2.595109
2019-04-01 11:27:39,393-INFO-#  Training loss at round 600: 2.3368375
2019-04-01 11:51:57,144-INFO-#  Training loss at round 800: 2.4303393
2019-04-01 12:21:09,768-INFO-#  Training loss at round 1000: 2.0317395
2019-04-01 12:43:04,267-INFO-#  Training loss at round 1200: 1.7179177
2019-04-01 13:05:24,043-INFO-#  Training loss at round 1400: 1.5933518
2019-04-01 13:28:02,842-INFO-#  Training loss at round 1600: 1.4252229
2019-04-01 13:50:20,208-INFO-#  Training loss at round 1800: 1.5132391
2019-04-01 14:12:27,085-INFO-#  Training loss at round 1999: 1.437904
2019-04-01 14:12:32,051-INFO-Accuracy: 0.6716302066864058
2019-04-01 14:13:13,383-INFO-#  Training loss at round 0: 7.252114
2019-04-01 14:36:25,746-INFO-#  Training loss at round 200: 2.0644636
2019-04-01 15:01:16,953-INFO-#  Training loss at round 400: 1.3866432
2019-04-01 15:24:39,594-INFO-#  Training loss at round 600: 1.2360239
2019-04-01 15:50:41,936-INFO-#  Training loss at round 800: 1.3090405
2019-04-01 16:30:44,741-INFO-#  Training loss at round 1000: 1.1987501
2019-04-01 17:07:43,190-INFO-#  Training loss at round 1200: 1.1617193
2019-04-01 17:36:07,150-INFO-#  Training loss at round 1400: 1.0209678
2019-04-01 18:03:13,050-INFO-#  Training loss at round 1600: 1.5171505
2019-04-01 18:29:09,580-INFO-#  Training loss at round 1800: 1.202601
2019-04-01 18:56:07,352-INFO-#  Training loss at round 1999: 1.3117349
2019-04-01 18:56:12,558-INFO-Accuracy: 0.7266220139707362
Score / NNSARSClassifier<PRDALL.5-32.relu.0.05>            0.5447398479145595        0.09587710615986075       -0.44886274175469876
 ========  NNClassifier
2019-04-01 18:58:35,988-INFO-#  Training loss at round 0: 7.2487307
2019-04-01 19:30:00,996-INFO-#  Training loss at round 200: 0.0059932536
2019-04-01 19:54:55,711-INFO-#  Training loss at round 400: 0.00113625
2019-04-01 20:20:04,721-INFO-#  Training loss at round 600: 0.0007717853
2019-04-01 20:45:30,881-INFO-#  Training loss at round 800: 0.0006380671
2019-04-01 21:14:53,096-INFO-#  Training loss at round 1000: 0.0005587686
2019-04-01 21:47:02,778-INFO-#  Training loss at round 1200: 0.00053131505
2019-04-01 22:15:06,333-INFO-#  Training loss at round 1400: 0.00051218533
2019-04-01 22:36:43,964-INFO-#  Training loss at round 1600: 0.0005018903
2019-04-01 23:00:05,036-INFO-#  Training loss at round 1800: 0.0004977776
2019-04-01 23:21:46,277-INFO-#  Training loss at round 1999: 0.0004932368
2019-04-01 23:21:51,842-INFO-Accuracy: 0.9997987696199621
2019-04-01 23:23:43,917-INFO-#  Training loss at round 0: 7.2533298
2019-04-01 23:55:14,126-INFO-#  Training loss at round 200: 0.19030084
2019-04-02 00:24:38,992-INFO-#  Training loss at round 400: 0.010898441
2019-04-02 01:04:02,706-INFO-#  Training loss at round 600: 0.0040585464
2019-04-02 01:39:12,198-INFO-#  Training loss at round 800: 0.0019953272
2019-04-02 02:03:10,671-INFO-#  Training loss at round 1000: 0.0011573224
2019-04-02 02:24:56,408-INFO-#  Training loss at round 1200: 0.0007884387
2019-04-02 02:46:41,941-INFO-#  Training loss at round 1400: 0.00054017385
2019-04-02 03:08:20,553-INFO-#  Training loss at round 1600: 0.00043095197
2019-04-02 03:30:04,787-INFO-#  Training loss at round 1800: 0.00036232985
2019-04-02 03:51:41,941-INFO-#  Training loss at round 1999: 0.00031621772
2019-04-02 03:51:47,154-INFO-Accuracy: 0.9998850112114069
2019-04-02 03:52:47,515-INFO-#  Training loss at round 0: 7.258154
2019-04-02 04:15:07,051-INFO-#  Training loss at round 200: 0.052045505
2019-04-02 04:47:40,137-INFO-#  Training loss at round 400: 0.004924275
2019-04-02 05:12:28,682-INFO-#  Training loss at round 600: 0.002233953
2019-04-02 05:42:46,180-INFO-#  Training loss at round 800: 4.0416794
2019-04-02 06:09:54,685-INFO-#  Training loss at round 1000: 0.82089126
2019-04-02 06:36:45,404-INFO-#  Training loss at round 1200: 0.33876324
2019-04-02 07:03:52,575-INFO-#  Training loss at round 1400: 4.449373
2019-04-02 07:31:16,057-INFO-#  Training loss at round 1600: 3.328803
2019-04-02 07:58:47,496-INFO-#  Training loss at round 1800: 2.714715
2019-04-02 08:26:40,741-INFO-#  Training loss at round 1999: 2.3182597
2019-04-02 08:26:47,282-INFO-Accuracy: 0.4772322198585638
2019-04-02 08:27:28,386-INFO-#  Training loss at round 0: 7.259559
2019-04-02 08:59:58,951-INFO-#  Training loss at round 200: 0.015393097
2019-04-02 09:32:22,624-INFO-#  Training loss at round 400: 0.0018527103
2019-04-02 10:03:19,022-INFO-#  Training loss at round 600: 0.0012607872
2019-04-02 10:32:34,912-INFO-#  Training loss at round 800: 0.0011015765
2019-04-02 11:00:11,875-INFO-#  Training loss at round 1000: 0.0010399115
2019-04-02 11:25:14,704-INFO-#  Training loss at round 1200: 0.0010170956
2019-04-02 11:47:05,401-INFO-#  Training loss at round 1400: 0.0009944105
2019-04-02 12:08:58,960-INFO-#  Training loss at round 1600: 0.0009874549
2019-04-02 12:31:20,264-INFO-#  Training loss at round 1800: 0.0010014023
2019-04-02 13:01:24,764-INFO-#  Training loss at round 1999: 2.289771
2019-04-02 13:01:34,224-INFO-Accuracy: 0.4485871158766206
2019-04-02 13:02:45,804-INFO-#  Training loss at round 0: 7.2671127
2019-04-02 13:37:55,627-INFO-#  Training loss at round 200: 0.04718064
2019-04-02 14:07:02,019-INFO-#  Training loss at round 400: 0.0059565683
2019-04-02 14:31:57,035-INFO-#  Training loss at round 600: 0.002600109
2019-04-02 14:57:13,858-INFO-#  Training loss at round 800: 0.0016960618
2019-04-02 15:21:09,728-INFO-#  Training loss at round 1000: 0.0013324155
2019-04-02 15:43:58,664-INFO-#  Training loss at round 1200: 0.0011706994
2019-04-02 16:07:00,706-INFO-#  Training loss at round 1400: 0.0010844872
2019-04-02 16:37:16,037-INFO-#  Training loss at round 1600: 4.936105
2019-04-02 17:09:52,664-INFO-#  Training loss at round 1800: 0.3558173
2019-04-02 17:40:15,833-INFO-#  Training loss at round 1999: 3.582217
2019-04-02 17:40:23,323-INFO-Accuracy: 0.21266565096156612
Score / NNSARSClassifier<PRDALL.5-32.relu.0.005>           0.6276337535056239        0.16875356674821199       -0.45888018675741193
(epsilon) [yaberraf@yaberraf-ld2 epsilon]$
(epsilon) [yaberraf@yaberraf-ld2 epsilon]$
